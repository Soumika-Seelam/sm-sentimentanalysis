Title,Content,Score,Comments,Cleaned_Title,Sentiment
[D] The machine learning community has a toxicity problem,"It is omnipresent!

**First** of all, the peer-review process is *broken*. Every fourth NeurIPS submission is put on arXiv. There are DeepMind researchers publicly going after reviewers who are criticizing their ICLR submission. On top of that, papers by well-known institutes that were put on arXiv are accepted at top conferences, despite the reviewers agreeing on rejection. In contrast, vice versa, some papers with a majority of accepts are overruled by the AC. (I don't want to call any names, just have a look the openreview page of this year's ICRL).

**Secondly,** there is a *reproducibility crisis*. Tuning hyperparameters on the test set seem to be the standard practice nowadays. Papers that do not beat the current state-of-the-art method have a zero chance of getting accepted at a good conference. As a result, hyperparameters get tuned and subtle tricks implemented to observe a gain in performance where there isn't any.

**Thirdly,** there is a *worshiping* problem. Every paper with a Stanford or DeepMind affiliation gets praised like a breakthrough. For instance, BERT has seven times more citations than ULMfit. The Google affiliation gives so much credibility and visibility to a paper. At every ICML conference, there is a crowd of people in front of every DeepMind poster, regardless of the content of the work. The same story happened with the Zoom meetings at the virtual ICLR 2020. Moreover, NeurIPS 2020 had twice as many submissions as ICML, even though both are top-tier ML conferences. Why? Why is the name ""neural"" praised so much? Next, Bengio, Hinton, and LeCun are truly deep learning pioneers but calling them the ""godfathers"" of AI is insane. It has reached the level of a cult.

**Fourthly**, the way Yann LeCun talked about biases and fairness topics was insensitive. However, the *toxicity* and backlash that he received are beyond any reasonable quantity. Getting rid of LeCun and silencing people won't solve any issue.

**Fifthly**, machine learning, and computer science in general, have a huge *diversity problem*. At our CS faculty, only 30% of undergrads and 15% of the professors are women. Going on parental leave during a PhD or post-doc usually means the end of an academic career. However, this lack of diversity is often abused as an excuse to shield certain people from any form of criticism.  Reducing every negative comment in a scientific discussion to race and gender creates a toxic environment. People are becoming afraid to engage in fear of being called a racist or sexist, which in turn reinforces the diversity problem.

**Sixthly**, moral and ethics are set *arbitrarily*. The U.S. domestic politics dominate every discussion. At this very moment, thousands of Uyghurs are put into concentration camps based on computer vision algorithms invented by this community, and nobody seems even remotely to care. Adding a ""broader impact"" section at the end of every people will not make this stop. There are huge shitstorms because a researcher wasn't mentioned in an article. Meanwhile, the 1-billion+ people continent of Africa is virtually excluded from any meaningful ML discussion (besides a few Indaba workshops).

**Seventhly**, there is a cut-throat publish-or-perish *mentality*. If you don't publish 5+ NeurIPS/ICML papers per year, you are a looser. Research groups have become so large that the PI does not even know the name of every PhD student anymore. Certain people submit 50+ papers per year to NeurIPS. The sole purpose of writing a paper has become to having one more NeurIPS paper in your CV. Quality is secondary; passing the peer-preview stage has become the primary objective.

**Finally**, discussions have become *disrespectful*. Schmidhuber calls Hinton a thief, Gebru calls LeCun a white supremacist, Anandkumar calls Marcus a sexist, everybody is under attack, but nothing is improved.

Albert Einstein was opposing the theory of [quantum mechanics](https://en.wikipedia.org/wiki/Albert_Einstein#Einstein's_objections_to_quantum_mechanics). Can we please stop demonizing those who do not share our exact views. We are allowed to disagree without going for the jugular. 

The moment we start silencing people because of their opinion is the moment scientific and societal progress dies. 

Best intentions, Yusuf",3892,568,d the machine learning community has a toxicity problem,-0.4019
[D] Our community must get serious about opposing OpenAI,"OpenAI was founded for the explicit purpose of democratizing access to AI and acting as a counterbalance to the closed off world of big tech by developing open source tools.

They have abandoned this idea entirely.

Today, with the release of GPT4 and their direct statement that they will not release details of the model creation due to ""safety concerns"" and the competitive environment, they have created a precedent worse than those that existed before they entered the field. We're at risk now of other major players, who previously at least published their work and contributed to open source tools, close themselves off as well.

AI alignment is a serious issue that we definitely have not solved. Its a huge field with a dizzying array of ideas, beliefs and approaches. We're talking about trying to capture the interests and goals of all humanity, after all. In this space, the one approach that is horrifying (and the one that OpenAI was LITERALLY created to prevent) is a singular or oligarchy of for profit corporations making this decision for us. This is exactly what OpenAI plans to do.

I get it, GPT4 is incredible. However, we are talking about the single most transformative technology and societal change that humanity has ever made. It needs to be for everyone or else the average person is going to be left behind.

We need to unify around open source development; choose companies that contribute to science, and condemn the ones that don't.

This conversation will only ever get more important.",3035,451,d our community must get serious about opposing openai,-0.0772
[D] Siraj has a new paper: 'The Neural Qubit'. It's plagiarised,"Exposed in this Twitter thread: https://twitter.com/AndrewM_Webb/status/1183150368945049605

Text, figures, tables, captions, equations (even equation numbers) are all lifted from another paper with minimal changes.

Siraj's paper: http://vixra.org/pdf/1909.0060v1.pdf

The original paper: https://arxiv.org/pdf/1806.06871.pdf

Edit: I've chosen to expose this publicly because he has a lot of fans and currently a lot of paying customers. They really trust this guy, and I don't think he's going to change.",2572,451,d siraj has a new paper the neural qubit its plagiarised,0.0
"[P] AppleNeuralHash2ONNX: Reverse-Engineered Apple NeuralHash, in ONNX and Python","As you may already know Apple is going to implement NeuralHash algorithm for on-device [CSAM detection](https://www.apple.com/child-safety/pdf/CSAM_Detection_Technical_Summary.pdf) soon. Believe it or not, this algorithm already exists as early as iOS 14.3, hidden under obfuscated class names. After some digging and reverse engineering on the hidden APIs I managed to export its model (which is MobileNetV3) to ONNX and rebuild the whole NeuralHash algorithm in Python. You can now try NeuralHash even on Linux!

Source code: [https://github.com/AsuharietYgvar/AppleNeuralHash2ONNX](https://github.com/AsuharietYgvar/AppleNeuralHash2ONNX)

No pre-exported model file will be provided here for obvious reasons. But it's very easy to export one yourself following the guide I included with the repo above. You don't even need any Apple devices to do it.

Early tests show that it can tolerate image resizing and compression, but not cropping or rotations.

Hope this will help us understand NeuralHash algorithm better and know its potential issues before it's enabled on all iOS devices.

Happy hacking!",1737,224,p appleneuralhashonnx reverseengineered apple neuralhash in onnx and python,0.0
[D] Does anybody else despise OpenAI?," I  mean, don't get me started with the closed source models they have that were trained using the work of unassuming individuals who will never  see a penny for it. Put it up on Github they said. I'm all for  open-source, but when a company turns around and charges you for a  product they made with freely and publicly made content, while forbidding you from using the output to create competing models, that is where I  draw the line. It is simply ridiculous. 

Sam Altman couldn't be anymore predictable with his recent attempts to get the government to start regulating AI.

What  risks? The AI is just a messenger for information that is already out  there if one knows how/where to look. You don't need AI to learn how to  hack, to learn how to make weapons, etc. Fake news/propaganda? The  internet has all of that covered. LLMs are no where near the level of AI  you see in sci-fi. I mean, are people really afraid of text? Yes, I  know that text can sometimes be malicious code such as viruses, but  those can be found on github as well.  If they fall for this they might  as well shutdown the internet while they're at it.

He  is simply blowing things out of proportion and using fear to increase  the likelihood that they do what he wants, hurt the competition. I  bet he is probably teething with bitterness everytime a new huggingface  model comes out. The thought of us peasants being able to use AI  privately is too dangerous. No, instead we must be fed scraps while they  slowly take away our jobs and determine our future.

This  is not a doomer post, as I am all in favor of the advancement of AI.  However, the real danger here lies in having a company like OpenAI  dictate the future of humanity. I get it, the writing is on the wall;  the cost of human intelligence will go down, but if everyone has their  personal AI then it wouldn't seem so bad or unfair would it? Listen,  something that has the power to render a college degree that costs  thousands of dollars worthless should be available to the public. This  is to offset the damages and job layoffs that will come as a result of  such an entity. It wouldn't be as bitter of a taste as it would if you were replaced by it while still not being able to access it. Everyone should be able to use it as leverage, it is the only fair solution.

If  we don't take action now, a company like ClosedAI will, and they are  not in favor of the common folk. Sam Altman is so calculated to the  point where there were times when he seemed to be shooting OpenAI in the foot during his talk.  This move is to simply conceal his real intentions, to climb the ladder and take it with him. If he didn't include his company in his  ramblings, he would be easily read. So instead, he pretends to be scared of his own product, in an effort to legitimize his claim. Don't fall  for it.

They are slowly making a  reputation as one the most hated tech companies, right up there with  Adobe, and they don't show any sign of change. They have no moat,  othewise they wouldn't feel so threatened to the point where they would have to resort to creating barriers of entry via regulation. This only  means one thing, we are slowly catching up. We just need someone to  vouch for humanity's well-being, while acting as an opposing force to the  evil corporations who are only looking out for themselves. Question is,  who would be a good candidate?",1499,427,d does anybody else despise openai,-0.34
[D] Anyone else witnessing a panic inside NLP orgs of big tech companies?,"I'm in a big tech company working along side a science team for a product you've all probably used. We have these year long initiatives to productionalize ""state of the art NLP models"" that are now completely obsolete in the face of GPT-4. I think at first the science orgs were quiet/in denial. But now it's very obvious we are basically working on worthless technology. And by ""we"", I mean a large organization with scores of teams. 

Anyone else seeing this? What is the long term effect on science careers that get disrupted like this? Whats even more odd is the ego's of some of these science people

Clearly the model is not a catch all, but still",1368,482,d anyone else witnessing a panic inside nlp orgs of big tech companies,-0.5106
"[D] Siraj Raval - Potentially exploiting students, banning students asking for refund. Thoughts?","I'm not a personal follower of Siraj, but this issue came up in a ML FBook group that I'm part of. I'm curious to hear what you all think.

It appears that Siraj recently offered a course ""Make Money with Machine Learning"" with a registration fee but did not follow through with promises made in the initial offering of the course. On top of that, he created a refund and warranty page with information regarding the course *after* people already paid. Here is a link to a WayBackMachine captures of u/klarken's documentation of Siraj's potential misdeeds: [case for a refund](https://web.archive.org/save/https://case-for-a-refund.s3.us-east-2.amazonaws.com/feedback.html), [discussion in course Discord](https://web.archive.org/web/20190923211614/https://case-for-a-refund.s3.us-east-2.amazonaws.com/reference_messages.png), [\~1200 individuals in the course](https://web.archive.org/web/20190923211815/https://case-for-a-refund.s3.us-east-2.amazonaws.com/members.png), [Multiple Slack channel discussion, students hidden from each other](https://web.archive.org/web/20190923211940/https://case-for-a-refund.s3.us-east-2.amazonaws.com/multiple_slack_channels.png), [""Hundreds refunded""](https://web.archive.org/web/20190923212113/https://case-for-a-refund.s3.us-east-2.amazonaws.com/hundreds_refunded.png)

According to Twitter threads, he has been banning anyone in his Discord/Slack that has been asking for refunds.

On top of this there are many Twitter threads regarding his behavior. A screenshot (bottom of post) of an account that has since been deactivated/deleted (he made the account to try and get Siraj's attention). Here is a Twitter WayBackMachine archive link of a search for the user in the screenshot: [https://web.archive.org/web/20190921130513/https:/twitter.com/search?q=safayet96434935&src=typed\_query](https://web.archive.org/web/20190921130513/https:/twitter.com/search?q=safayet96434935&src=typed_query). In the search results it is apparent that there are many students who have been impacted by Siraj.

UPDATE 1: Additional searching on Twitter has yielded many more posts, check out the tweets/retweets of these people: [student1](https://web.archive.org/save/https:/twitter.com/ReneeSLiu1) [student2](https://web.archive.org/web/20190921133155/https://twitter.com/Aravind56898077)

UPDATE 2: A user mentioned that I should ask a question on r/legaladvice regarding the legality of the refusal to refund and whatnot. I have done so [here](https://www.reddit.com/r/legaladvice/comments/d7gopa/independent_online_course_false_advertising_and/). It appears that per California commerce law (where the School of AI is registered) individuals have the right to ask for a refund for 30 days.

UPDATE 3: Siraj has replied to the post below, and on [Twitter](https://web.archive.org/web/20190922213957/https://twitter.com/sirajraval/status/1175864213916372992?s=09) (Way Back Machine capture)

UPDATE 4: Another student has shared their interactions via [this Imgur post](https://imgur.com/gallery/msAdqBn). And another recorded moderators actively suppressing any mentions of refunds [on a live stream](https://web.archive.org/save/https://imgur.com/a/o1TMRY2). [Here is an example](https://imgur.com/a/KhMV6Xo) of assignment quality, note that the assignment is to generate fashion designs not pneumonia prediction.

UPDATE5: Relevant Reddit posts: [Siraj response](https://www.reddit.com/r/MachineLearning/comments/d7vv1l/d_siraj_apologizes_and_promises_refunds_within_30/), [question about opinions on course two weeks before this](https://www.reddit.com/r/learnmachinelearning/comments/cp7kht/guys_what_do_you_think_about_siraj_ravals_new/ewnv00m/?utm_source=share&utm_medium=web2x), [Siraj-Udacity relationship](https://www.reddit.com/r/MachineLearning/comments/d8nlqf/n_udacity_had_an_interventional_meeting_with/)

UPDATE6: The Register has [published a piece on the debacle](https://www.theregister.co.uk/2019/09/27/youtube_ai_star/), Coffezilla [posted a video on all of this](https://www.youtube.com/watch?v=7jmBE4yPrOs)

UPDATE7: Example of blatant ripoff: GitHub user gregwchase [diabetic retinopathy](https://github.com/gregwchase/dsi-capstone), Siraj's [ripoff](https://web.archive.org/web/20190928160728/https://github.com/llSourcell/AI_in_Medicine_Clinical_Imaging_Classification)

UPDATE8: Siraj has a [new paper and it is plagiarized](https://www.reddit.com/r/MachineLearning/comments/dh2xfs/d_siraj_has_a_new_paper_the_neural_qubit_its/)

If you were/are a student in the course and have your own documentation of your interactions, please feel free to bring them to my attention either via DM or in the comments below and I will add them to the main body here.

&#x200B;

https://preview.redd.it/i75r44bku7o31.jpg?width=347&format=pjpg&auto=webp&s=ec2f02ee1998e27ea00d529ffb2086657dc60d77",1358,468,d siraj raval  potentially exploiting students banning students asking for refund thoughts,-0.4404
"So long r/MachineLearning, it's been an interesting few years","Some of you may recognize me, most of you probably don't. I've been the most active moderator of r/MachineLearning for a few years now, but on June 30th I'll be deleting my Reddit account.

I pretty much exclusively used Apollo to moderate. It would notify me of any new post, which allowed me to moderate from anywhere, anytime. That's how I stayed on top of moderating such a large sub.

When I stepped back on my moderation efforts a few months ago, [the effects](https://old.reddit.com/r/MachineLearning/comments/110swn2/d_quality_of_posts_in_this_sub_going_down) were quite apparent to [many of you](https://old.reddit.com/r/MachineLearning/comments/115ez2r/deleted_by_user).

Of course, this is the internet, and each of you have your own subjective view on moderation. Just know that it is a very time consuming task that I did for free because I genuinely cared about the community.

If you want to join me, I'll be moving on to kbin where I'm a moderator for [m/machinelearning](https://kbin.social/m/machinelearning). Otherwise, this is my farewell.

P.S. I'm sure there will be some who are sympathetic and some who just have an axe to grind and will complain about anything. I'm not a piñata; there's no prize inside if you bash me, but if you just can't help yourself, then have at it. I'll be gone soon anyway.",1324,90,so long rmachinelearning its been an interesting few years,0.4019
[R] AlphaFold 2,"Seems like DeepMind just caused the ImageNet moment for protein folding.

Blog post isn't that deeply informative yet (paper is promised to appear soonish). Seems like the improvement over the first version of AlphaFold is mostly usage of transformer/attention mechanisms applied to residue space and combining it with the working ideas from the first version. Compute budget is surprisingly moderate given how crazy the results are. Exciting times for people working in the intersection of molecular sciences and ML :)

Tweet by Mohammed AlQuraishi (well-known domain expert)  
[https://twitter.com/MoAlQuraishi/status/1333383634649313280](https://twitter.com/MoAlQuraishi/status/1333383634649313280)

DeepMind BlogPost  
[https://deepmind.com/blog/article/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology](https://deepmind.com/blog/article/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology)  


UPDATE:   
Nature published a comment on it as well  
[https://www.nature.com/articles/d41586-020-03348-4](https://www.nature.com/articles/d41586-020-03348-4)",1320,240,r alphafold ,0.0
AMA: We are the Google Brain team. We'd love to answer your questions about machine learning.,"We’re a group of research scientists and engineers that work on the [Google Brain team](http://g.co/brain).  Our group’s mission is to make intelligent machines, and to use them to improve people’s lives.  For the last five years, we’ve conducted research and built systems to advance this mission.

We disseminate our work in multiple ways:

* By publishing papers about our research (see [publication list](https://research.google.com/pubs/BrainTeam.html))
* By building and open-sourcing software systems like TensorFlow (see [tensorflow.org](http://tensorflow.org) and [https://github.com/tensorflow/tensorflow](https://github.com/tensorflow/tensorflow))
* By working with other teams at Google and Alphabet to get our work into the hands of billions of people (some examples: [RankBrain for Google Search](https://en.wikipedia.org/wiki/RankBrain), [SmartReply for GMail](https://research.googleblog.com/2015/11/computer-respond-to-this-email.html), [Google Photos](https://research.googleblog.com/2014/09/building-deeper-understanding-of-images.html), [Google Speech Recognition](https://research.googleblog.com/2012/08/speech-recognition-and-deep-learning.html), …)
* By training new researchers through internships and the [Google Brain Residency](http://g.co/brainresidency) program

We are:

* [Jeff Dean](http://research.google.com/people/jeff) (/u/jeffatgoogle)
* [Geoffrey Hinton](https://research.google.com/pubs/GeoffreyHinton.html) (/u/geoffhinton)
* [Vijay Vasudevan](http://research.google.com/pubs/VijayVasudevan.html) (/u/Spezzer)
* [Vincent Vanhoucke](http://research.google.com/pubs/VincentVanhoucke.html) (/u/vincentvanhoucke)
* [Chris Olah](http://research.google.com/pubs/ChristopherOlah.html) (/u/colah)
* [Rajat Monga](http://research.google.com/pubs/RajatMonga.html) (/u/rajatmonga)
* [Greg Corrado](http://research.google.com/pubs/GregCorrado.html) (/u/gcorrado)
* [George Dahl](https://scholar.google.com/citations?user=ghbWy-0AAAAJ&hl=en) (/u/gdahl)
* [Doug Eck](http://research.google.com/pubs/author39086.html) (/u/douglaseck)
* [Samy Bengio](http://research.google.com/pubs/bengio.html) (/u/samybengio)
* [Quoc Le](http://research.google.com/pubs/QuocLe.html) (/u/quocle)
* [Martin Abadi](http://research.google.com/pubs/abadi.html) (/u/martinabadi)
* [Claire Cui](https://www.linkedin.com/in/claire-cui-5021035) (/u/clairecui)
* [Anna Goldie](https://www.linkedin.com/in/adgoldie) (/u/anna_goldie)
* [Zak Stone](https://www.linkedin.com/in/zstone) (/u/poiguy)
* [Dan Mané](https://www.linkedin.com/in/danmane) (/u/danmane)
* [David Patterson](https://www2.eecs.berkeley.edu/Faculty/Homepages/patterson.html) (/u/pattrsn)
* [Maithra Raghu](http://maithraraghu.com/) (/u/mraghu)
* [Anelia Angelova](http://research.google.com/pubs/AneliaAngelova.html) (/u/aangelova)
* [Fernanda Viégas](http://hint.fm/) (/u/fernanda_viegas)
* [Martin Wattenberg](http://hint.fm/) (/u/martin_wattenberg)
* [David Ha](http://blog.otoro.net/) (/u/hardmaru)
* [Sherry Moore](https://www.linkedin.com/in/sherry-moore-38b3a32) (/u/sherryqmoore/)
* … and maybe others: we’ll update if others become involved.

We’re excited to answer your questions about the Brain team and/or machine learning!  (We’re gathering questions now and will be answering them on August 11, 2016).

Edit (~10 AM Pacific time): A number of us are gathered in Mountain View, San Francisco, Toronto, and Cambridge (MA), snacks close at hand.  Thanks for all the questions, and we're excited to get this started.

Edit2: We're back from lunch.  Here's [our AMA command center](http://imgur.com/gallery/zHkoC)

Edit3: (2:45 PM Pacific time): We're mostly done here.  Thanks for the questions, everyone!  We may continue to answer questions sporadically throughout the day.",1303,791,ama we are the google brain team wed love to answer your questions about machine learning,0.6369
[P] OpenAssistant - The world's largest open-source replication of ChatGPT,"We’re excited to announce the release of OpenAssistant.

The future of AI development depends heavily on high quality datasets and models being made publicly available, and that’s exactly what this project does.

Watch the annoucement video:

[https://youtu.be/ddG2fM9i4Kk](https://youtu.be/ddG2fM9i4Kk)

&#x200B;

Our team has worked tirelessly over the past several months collecting large amounts of text-based input and feedback to create an incredibly diverse and unique dataset designed specifically for training language models or other AI applications.

With over 600k human-generated data points covering a wide range of topics and styles of writing, our dataset will be an invaluable tool for any developer looking to create state-of-the-art instruction models!

To make things even better, we are making this entire dataset free and accessible to all who wish to use it. Check it out today at our HF org: OpenAssistant

On top of that, we've trained very powerful models that you can try right now at: [open-assistant.io/chat](https://open-assistant.io/chat) !",1272,174,p openassistant  the worlds largest opensource replication of chatgpt,0.0
[D] Siraj is still plagiarizing,"Siraj's latest video on explainable computer vision is still using people's material without credit. In this week's video, the slides from 1:40 to 6:00 \[1\] are lifted verbatim from a 2018 tutorial \[2\], except that Siraj removed the footer saying it was from the Fraunhofer institute on all but one slide.

Maybe we should just ignore him at this point, but proper credit assignment really is the foundation of any discipline, and any plagiarism hurts it (even if he is being better about crediting others than before).

I mean, COME ON MAN.

\[1\] [https://www.youtube.com/watch?v=Y8mSngdQb9Q&feature=youtu.be](https://www.youtube.com/watch?v=Y8mSngdQb9Q&feature=youtu.be) 

\[2\]  [http://heatmapping.org/slides/2018\_MICCAI.pdf](http://heatmapping.org/slides/2018_MICCAI.pdf)",1184,143,d siraj is still plagiarizing,0.0
"We are Oriol Vinyals and David Silver from DeepMind’s AlphaStar team, joined by StarCraft II pro players TLO and MaNa! Ask us anything","Hi there! We are Oriol Vinyals (/u/OriolVinyals) and David Silver (/u/David_Silver), lead researchers on DeepMind’s AlphaStar team, joined by StarCraft II pro players TLO, and MaNa.

This evening at DeepMind HQ we held a livestream demonstration of AlphaStar playing against TLO and MaNa - you can read more about the matches [here](https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/) or re-watch the stream on YouTube [here](https://www.youtube.com/watch?v=cUTMhmVh1qs).

Now, we’re excited to talk with you about AlphaStar, the challenge of real-time strategy games for AI research, the matches themselves, and anything you’d like to know from TLO and MaNa about their experience playing against AlphaStar! :)

We are opening this thread now and will be here at **16:00 GMT / 11:00 ET / 08:00PT** on Friday, 25 January to answer your questions.

&#x200B;

EDIT: Thanks everyone for your great questions. It was a blast, hope you enjoyed it as well!",1166,1003,we are oriol vinyals and david silver from deepminds alphastar team joined by starcraft ii pro players tlo and mana ask us anything,0.0
[N] 2024 Nobel Prize for Physics goes to ML and DNN researchers J. Hopfield and G. Hinton,"Announcement: https://x.com/NobelPrize/status/1843589140455272810

Our boys John Hopfield and Geoffrey Hinton were rewarded for their foundational contributions to machine learning and deep learning with the Nobel prize for physics 2024!

I hear furious Schmidhuber noises in the distance!

On a more serious note, despite the very surprising choice, I am generally happy - as a physicist myself with strong interest in ML, I love this physics-ML cinematic universe crossover.

The restriction to Hopfield and Hinton will probably spark discussions about the relative importance of {Hopfield, Hinton, LeCun, Schmidhuber, Bengio, Linnainmaa, ...} for the success of modern ML/DL/AI.
A discussion especially Schmidhuber very actively engages in.

The response from the core physics community however is rather mixed, as shown in the [/r/physics thread](https://www.reddit.com/r/Physics/comments/1fyw12p/the_2024_nobel_prize_in_physics_is_awarded_to/). There, the missing link/connection to physics research is noted and the concurrent ""loss"" of the '24 prize for physics researchers.",1150,308,n  nobel prize for physics goes to ml and dnn researchers j hopfield and g hinton,0.5106
"[D] Chinese government uses machine learning not only for surveillance, but also for predictive policing and for deciding who to arrest in Xinjiang","Link to **[story](https://www.icij.org/investigations/china-cables/exposed-chinas-operating-manuals-for-mass-internment-and-arrest-by-algorithm/)**

This post is not an ML *research* related post. I am posting this because I think it is important for the community to see how research is applied by authoritarian governments to achieve their goals. It is related to a few previous popular posts on this subreddit with high upvotes, which prompted me to post this [story](https://www.icij.org/investigations/china-cables/exposed-chinas-operating-manuals-for-mass-internment-and-arrest-by-algorithm/).

Previous related stories:

- [Is machine learning's killer app totalitarian surveillance and oppression?](https://redd.it/c9n1u2)

- [Using CV for surveillance and regression for threat scoring citizens in Xinjiang](https://redd.it/7kzflw)

- [ICCV 19: The state of some ethically questionable papers](https://redd.it/dp389c)

- [Hikvision marketed ML surveillance camera that automatically identifies Uyghurs](https://redd.it/dv5axp)

- [Working on an ethically questionnable project...](https://redd.it/dw7sms)

The **[story](https://www.icij.org/investigations/china-cables/exposed-chinas-operating-manuals-for-mass-internment-and-arrest-by-algorithm/)** reports the details of a new leak of highly classified Chinese government documents reveals the operations manual for running the mass detention camps in Xinjiang and exposed the mechanics of the region’s system of mass surveillance.

**The [lead journalist](https://twitter.com/BethanyAllenEbr/status/1198663008152621057)'s summary of findings**

The China Cables represent the first leak of a classified Chinese government document revealing the inner workings of the detention camps, as well as the first leak of classified government documents unveiling the predictive policing system in Xinjiang.

The leak features classified intelligence briefings that reveal, in the government’s own words, how Xinjiang police essentially take orders from a massive “cybernetic brain” known as IJOP, which flags entire categories of people for investigation & detention.

These secret intelligence briefings reveal the scope and ambition of the government’s AI-powered policing platform, which purports to predict crimes based on computer-generated findings alone. The result? Arrest by algorithm.

**The article describe methods used for algorithmic policing**

The classified intelligence briefings reveal the scope and ambition of the government’s artificial-intelligence-powered policing platform, which purports to predict crimes based on these computer-generated findings alone. Experts say the platform, which is used in both policing and military contexts, demonstrates the power of technology to help drive industrial-scale human rights abuses.

“The Chinese [government] have bought into a model of policing where they believe that through the collection of large-scale data run through artificial intelligence and machine learning that they can, in fact, predict ahead of time where possible incidents might take place, as well as identify possible populations that have the propensity to engage in anti-state anti-regime action,” said Mulvenon, the SOS International document expert and director of intelligence integration. “And then they are preemptively going after those people using that data.”

In addition to the predictive policing aspect of the article, there are side [articles](https://qz.com/1755018/chinas-manual-for-uighur-detention-camps-revealed-in-data-leak/) about the entire ML stack, including how [mobile apps](https://www.icij.org/investigations/china-cables/how-china-targets-uighurs-one-by-one-for-using-a-mobile-app/) are used to target Uighurs, and also how the inmates are [re-educated](https://www.bbc.com/news/world-asia-china-50511063) once inside the concentration camps. The documents reveal how every aspect of a detainee's life is monitored and controlled.

*Note: My motivation for posting this story is to raise ethical concerns and awareness in the research community. I do not want to heighten levels of racism towards the Chinese research community (not that it may matter, but I am Chinese). See this [thread](https://redd.it/e10b5x) for some context about what I don't want these discussions to become.*

*I am aware of the fact that the Chinese government's policy is to integrate the state and the people as one, so accusing the party is perceived domestically as insulting the Chinese people, but I also believe that we as a research community is intelligent enough to be able to separate government, and those in power, from individual researchers. We as a community should keep in mind that there are many Chinese researchers (in mainland and abroad) who are not supportive of the actions of the CCP, but they may not be able to voice their concerns due to personal risk.*

**Edit** Suggestion from /u/DunkelBeard:

When discussing issues relating to the Chinese government, try to use the term CCP, Chinese Communist Party, Chinese government, or Beijing. Try *not* to use only the term *Chinese* or *China* when describing the government, as it may be misinterpreted as referring to the Chinese people (either citizens of China, or people of Chinese ethnicity), if that is not your intention. As mentioned earlier, conflating China and the CCP is actually a tactic of the CCP.",1126,191,d chinese government uses machine learning not only for surveillance but also for predictive policing and for deciding who to arrest in xinjiang,-0.4767
[D] Why do PhD Students in the US seem like overpowered final bosses ,"Hello,

I'm a PhD student in a European university, working on AI/ML/CV ..etc. my PhD is 4 years. The first year I literally just spent learning how to actually do research, teaching one course to learn how things work...etc. Second year, I published my first publication as a co-author in CVPR. By third year, I can manage research projects, I understand how to do grants applications, how funding works, the politics of it all ...etc. I added to my CV, 2 publications, one journal and another conference as first author. I'm very involved in industry and I also write a lot of production grade code in regard to AI, systems architecture, backend, cloud, deployment, etc for companies that have contracts with my lab.

The issue is when I see PhD students similar to me in the US, they be having 10 publications, 5 of them 1st author, all of them are either CVPR, ICML, ICLR, NeurIPS ...etc. I don't understand, do these people not sleep ? How are they able to achieve this crazy amount of work and still have 3 publications every year in A\* journals ?

I don't think these people are smarter than I, usually I get ideas and I look up if something exists, and I can see that something was just published by some PhD student in Stanford or DeepMind ..etc like 1 month ago, So I can see that my reasoning isn't late in regard to SOTA. but the concepts that you would need to grasp to just have one of those publications + the effort and the time you need to invest and the resources to get everything done, wouldn't be possible for 2\~3 months project. How is it possible for these people to do this ?

Thank you !",1083,278,d why do phd students in the us seem like overpowered final bosses ,0.3612
[P] NumPy Illustrated. The Visual Guide to NumPy,"Hi, r/MachineLearning,

I've built a (more or less) complete guide to numpy by taking ""Visual Intro to NumPy"" by Jay Alammar as a starting point and significantly expanding the coverage.

Here's the [link](https://medium.com/better-programming/numpy-illustrated-the-visual-guide-to-numpy-3b1d4976de1d?source=friends_link&sk=57b908a77aa44075a49293fa1631dd9b).",1068,53,p numpy illustrated the visual guide to numpy,0.0
[N] OpenAI may have benchmarked GPT-4’s coding ability on it’s own training data,"[GPT-4 and professional benchmarks: the wrong answer to the wrong question](https://aisnakeoil.substack.com/p/gpt-4-and-professional-benchmarks)

*OpenAI may have tested on the training data. Besides, human benchmarks are meaningless for bots.*

 **Problem 1: training data contamination**

To benchmark GPT-4’s coding ability, OpenAI evaluated it on problems from Codeforces, a website that hosts coding competitions. Surprisingly, Horace He pointed out that GPT-4 solved 10/10 pre-2021 problems and 0/10 recent problems in the easy category. The training data cutoff for GPT-4 is September 2021. This strongly suggests that the model is able to memorize solutions from its training set — or at least partly memorize them, enough that it can fill in what it can’t recall.

As further evidence for this hypothesis, we tested it on Codeforces problems from different times in 2021. We found that it could regularly solve problems in the easy category before September 5, but none of the problems after September 12.

In fact, we can definitively show that it has memorized problems in its training set: when prompted with the title of a Codeforces problem, GPT-4 includes a link to the exact contest where the problem appears (and the round number is almost correct: it is off by one). Note that GPT-4 cannot access the Internet, so memorization is the only explanation.",1006,135,n openai may have benchmarked gpts coding ability on its own training data,0.3182
"[D] Off my chest. I'm doing PhD in ML, and I'm a failure."," I'm halfway through my ML PhD.

I was quite lucky and got into a good program, especially in a good lab where students are superstars and get fancy jobs upon graduation. I'm not one of them. I have one crappy, not-so-technical publication and I'm struggling to find a new problem that is solvable within my capacity. I've tried hard. I've been doing research throughout my undergrad and masters, doing everything I could – doing projects, reading papers, taking ML and math courses, writing grants for professors...

The thing is, I just can't reach the level of generating new ideas. No matter how hard I try, it just ain't my thing. I think why. I begin to wonder if STEM wasn't my thing in the first place. I look around and there are people whose brain simply ""gets"" things easier. For me, it requires extra hard working and extra time. During undergrad, I could get away with studying harder and longer. Well, not for PhD. Especially not in this fast-paced, crowded field where I need to take in new stuff and publish quickly.

I'm an imposter, and this is not a syndrome. I'm getting busted. Everybody else is getting multiple internship offers and all that. I'm getting rejected from everywhere. It seems now they know. They know I'm useless. Would like to say this to my advisor but he's such a genius that he doesn't get the mind of the commoner. All my senior labmates are full-time employed, so practically I'm the most senior in my lab right now.",996,328,d off my chest im doing phd in ml and im a failure,-0.5106
Meta does everything OpenAI should be [D]," I'm surprised (or maybe not) to say this, but Meta (or Facebook) democratises AI/ML much more than OpenAI, which was originally founded and primarily funded for this purpose. OpenAI has largely become a commercial project for profit only. Although as far as Llama models go, they don't yet reach GPT4 capabilities for me, but I believe it's only a matter of time. What do you guys think about this?",981,256,meta does everything openai should be d,0.0
[P] Using PyTorch + NumPy? A bug that plagues thousands of open-source ML projects.,"Using NumPy’s random number generator with multi-process data loading in PyTorch causes identical augmentations unless you specifically set seeds using the worker\_init\_fn option in the DataLoader. I didn’t and this bug silently regressed my model’s accuracy.

How many others has this bug done damage to? Curious, I downloaded over a hundred thousand repositories from GitHub that import PyTorch, and analysed their source code. I kept projects that define a custom dataset, use NumPy’s random number generator with multi-process data loading, and are more-or-less straightforward to analyse using abstract syntax trees. Out of these, over 95% of the repositories are plagued by this problem. It’s inside PyTorch's official tutorial, OpenAI’s code, and NVIDIA’s projects. Even Karpathy admitted falling prey to it.

For example, the following image shows the duplicated random crop augmentations you get when you blindly follow the official PyTorch tutorial on custom datasets:

https://preview.redd.it/pccy5wskpes61.png?width=1652&format=png&auto=webp&s=f292d0282ad954cbac2c693a9656d62fa0dd9682

You can read more details [here](https://tanelp.github.io/posts/a-bug-that-plagues-thousands-of-open-source-ml-projects/).",978,159,p using pytorch  numpy a bug that plagues thousands of opensource ml projects,0.0
[D] Anyone else find themselves rolling their eyes at a lot of mainstream articles that talk about “AI”?,"I’m not talking about papers, or articles from more scientific publications, but mainstream stuff that gets published on the BBC, CNN, etc. Stuff that makes it to Reddit front pages. 

There’s so much misinformation out there, it’s honestly nauseating. AI is doom and gloom nonsense ranging from racist AIs to the extinction of human kind. 

I just wish people would understand that we are so incomprehensibly far away from a true, thinking machine. The stuff we have now that is called “ai” are just fancy classification/regression models that rely on huge amounts of data to train. The applications are awesome, no doubt, but ultimately AI in its current state is just another tool in the belt of a researcher/engineer. AI itself is neither good, or bad, in the same way that a chainsaw is neither good or bad. It’s just another tool.  

Tldr: I rant about the misinformation regarding AI in its current state.",954,231,d anyone else find themselves rolling their eyes at a lot of mainstream articles that talk about ai,0.0
"[D] If you say in a paper you provide code, it should be required to be available at time of publication","TL;DR: The only thing worse than not providing code is saying you did and not following through.

I'm frustrated, so this might be a little bit of a rant but here goes: I cannot believe that it is acceptable in highly ranked conferences to straight-up lie about the availability of code. Firstly, obviously it would be great if everyone released their code all the time because repeatability in ML is pretty dismal at times. But if you're not going to publish your code, then don't say you are. Especially when you're leaving details out of the paper and referring the reader to said ""published"" code.

Take for example [this paper](https://arxiv.org/abs/2004.04725), coming out of NVIDIA's research lab and published in CVPR2020. It is fairly detail-sparse, and nigh on impossible to reproduce in its current state as a result. It refers the reader to [this repository](https://github.com/NVlabs/wetectron) which has been a single readme since its creation. It is simply unacceptable for this when the paper directly says the code has been released.

As top conferences are starting to encourage the release of code, I think there needs to be another component: the code must actually be available. Papers that link to empty or missing repositories within some kind of reasonable timeframe of publication should be withdrawn. It should be unacceptable to direct readers to code that doesn't exist for details, and similarly for deleting repositories shortly after publication. I get that this is logistically a little tough, because it has to be done after publication, but still we can't let this be considered okay

EDIT: To repeat the TL;DR again and highlight the key point - There won't always be code, that's frustrating but tolerable. There is no excuse for claiming to have code available, but not actually making it available. Code should be required to be up at time of publication, and kept up for some duration, if a paper wishes to claim to have released their code.",955,134,d if you say in a paper you provide code it should be required to be available at time of publication,0.0
[D] Call for questions for Andrej Karpathy from Lex Fridman,"Hi, my name is Lex Fridman. I host a [podcast](https://www.youtube.com/c/lexfridman). I'm talking to Andrej Karpathy on it soon. To me, Andrej is one of the best researchers and educators in the history of the machine learning field. If you have questions/topic suggestions you'd like us to discuss, including technical and philosophical ones, please let me know.

**EDIT**: Here's [the resulting published episode](https://www.youtube.com/watch?v=cdiD-9MMpb0). Thank you for the questions!",947,346,d call for questions for andrej karpathy from lex fridman,0.0
[N] Netflix and European Space Agency no longer working with Siraj Raval,"*According to article in [The Register](https://www.theregister.co.uk/2019/10/14/ravel_ai_youtube/)*:

A Netflix spokesperson confirmed to The Register it wasn’t working with Raval, and the ESA has cancelled the whole workshop altogether.

“The situation is as it is. The workshop is cancelled, and that’s all,” Guillaume Belanger, an astrophysicist and the INTEGRAL Science Operations Coordinator at the ESA, told The Register on Monday.

Raval isn’t about to quit his work any time soon, however. He promised students who graduated from his course that they would be referred to recruiters at Nvidia, Intel, Google and Amazon for engineering positions, or matched with a startup co-founder or a consulting client.

In an unlisted YouTube video recorded live for his students discussing week eight of his course, and seen by El Reg, he read out a question posed to him: “Will your referrals hold any value now?”

“Um, yeah they’re going to hold value. I don’t see why they wouldn’t. I mean, yes, some people on Twitter were angry but that has nothing to do with… I mean… I’ve also had tons of support, you know. I’ve had tons of support from people, who, uh, you know, support me, who work at these companies.

*He continues to justify his actions:*

“Public figures called me in private to remind me that this happens. You know, people make mistakes. You just have to keep going. They’re basically just telling me to not to stop. Of course, you make mistakes but you just keep going,” he claimed.

*When The Register asked Raval for comment, he responded:*

**I've hardly taken any time off to relax since I first started my YouTube channel almost four years ago. And despite the enormous amount of work it takes to release two high quality videos a week for my audience, I progressively started to take on multiple other projects simultaneously by myself – a book, a docu-series, podcasts, YouTube videos, the course, the school of AI. Basically, these past few weeks, I've been experiencing a burnout unlike anything I've felt before. As a result, all of my output has been subpar.**

**I made the [neural qubits] video and paper in one week. I remember wishing I had three to six months to really dive into quantum machine-learning and make something awesome, but telling myself I couldn't take that long as it would hinder my other projects. I plagiarized large chunks of the paper to meet my self-imposed one-week deadline. The associated video with animations took a lot more work to make. I didn't expect the paper to be cited as serious research, I considered it an additional reading resource for people who enjoyed the associated video to learn more about quantum machine learning. If I had a second chance, I'd definitely take way more time to write the paper, and in my own words.**

**I've given refunds to every student who's asked so far, and the majority of students are still enrolled in the course. There are many happy students, they're just not as vocal on social media. We're on week 8 of 10 of my course, fully committed to student success.**

“And, no, I haven't plagiarized research for any other paper,” he added.

https://www.theregister.co.uk/2019/10/14/ravel_ai_youtube/",918,253,n netflix and european space agency no longer working with siraj raval,-0.296
30% of Google's Reddit Emotions Dataset is Mislabeled [D],"Last year, Google released their Reddit Emotions dataset: a collection of 58K Reddit comments human-labeled according to 27 emotions. 

I analyzed the dataset... and found that a 30% is mislabeled!

Some of the errors:

1. **\*aggressively tells friend I love them\*** – mislabeled as **ANGER**
2. **Yay, cold McDonald's. My favorite.** – mislabeled as **LOVE**
3. **Hard to be sad these days when I got this guy with me** – mislabeled as **SADNESS**
4. **Nobody has the money to. What a joke** – mislabeled as **JOY**

&#x200B;

I wrote a blog about it here, with more examples and my main two suggestions for how to fix Google's data annotation methodology.

Link: [https://www.surgehq.ai/blog/30-percent-of-googles-reddit-emotions-dataset-is-mislabeled](https://www.surgehq.ai/blog/30-percent-of-googles-reddit-emotions-dataset-is-mislabeled)",914,133, of googles reddit emotions dataset is mislabeled d,0.0
[D] What is happening in this subreddit?,"I was not going to post this but something wrong is happening here in this subreddit which forced my hands.


This week two posts relating to machine learning were posted here one is about [How visual search works](https://thomasdelteil.github.io/VisualSearch_MXNet/) and other about [generating ramen](https://www.reddit.com/r/MachineLearning/comments/8l5w56/p_generative_ramen/). The former post contains a small write up, source code and a demo site to explain how visual search works and the latter just have a gif of generated  ramen probably with a GAN. The irony is that the post which has more information and source code for reproducing that work got only about 25 votes and the one with gif only with no source code or explanation provided got more than 1000 votes (not so unique work any one with basic understanding of GAN can make one). Today the most upvoted post here is about [a circle generating GAN](https://www.reddit.com/r/MachineLearning/comments/8mgs8k/p_visualisation_of_a_gan_learning_to_generate_a/) which also has only a gif with brief explanation as comment and no source code. Are you seeing a pattern here?

The problem I mentioned above is not a one of case, I am a regular lurker in this subreddit and for the past few months I started seeing some disturbing patterns in posts posted here. People who posts gif/movie/photo only post tends to get more upvotes than the posts with full source code or explanation.  I agree some original research posts [such as this](https://www.youtube.com/watch?v=qc5P2bvfl44&feature=youtu.be&t=7s) or [this](https://www.youtube.com/watch?v=y__pYj9UHfc) can be only be released as videos and not the source code because of its commercial value. But most of the gif/movie/photo only posts here are not at all original research but they used a already know algorithm with a different dataset (eg: Ramen generation). 

The problem here is If we continue this type of posts people will stop sharing their original works, source code or explanation and then starts sharing this type of end result only posts which will get less scrutiny and more votes. In future, this will not only decrease the quality of this subreddit but also its a greater danger to the open nature of Machine learning field. What's the point in posting a github project link or blogpost here when we can get much more votes with a gif alone?.

*I am not a academician but I use r/MachineLearning to find blogs, articles and projects which explains/program recent discoveries in AI which then I myself can try out.*
",909,130,d what is happening in this subreddit,0.0
[D] What is the best ML paper you read in 2018 and why?,"Enjoyed this thread last year, so I am making a one for this year. ",912,149,d what is the best ml paper you read in  and why,0.6369
"[P] This is the worst AI ever. (GPT-4chan model, trained on 3.5 years worth of /pol/ posts)","[https://youtu.be/efPrtcLdcdM](https://youtu.be/efPrtcLdcdM)

GPT-4chan was trained on over 3 years of posts from 4chan's ""politically incorrect"" (/pol/) board.

Website (try the model here): [https://gpt-4chan.com](https://gpt-4chan.com)

Model: [https://huggingface.co/ykilcher/gpt-4chan](https://huggingface.co/ykilcher/gpt-4chan)

Code: [https://github.com/yk/gpt-4chan-public](https://github.com/yk/gpt-4chan-public)

Dataset: [https://zenodo.org/record/3606810#.YpjGgexByDU](https://zenodo.org/record/3606810#.YpjGgexByDU)

&#x200B;

OUTLINE:

0:00 - Intro

0:30 - Disclaimers

1:20 - Elon, Twitter, and the Seychelles

4:10 - How I trained a language model on 4chan posts

6:30 - How good is this model?

8:55 - Building a 4chan bot

11:00 - Something strange is happening

13:20 - How the bot got unmasked

15:15 - Here we go again

18:00 - Final thoughts",903,169,p this is the worst ai ever gptchan model trained on  years worth of pol posts,-0.4939
[D] Should beginner's tutorials be banned?,"This sub is full of them. They rise to the top for some bizarre reason and reaffirm that this subs focus is on helping people start off learning about a narrow set (neural networks / deep learning) of machine learning.

Allowing this content to be so prevalent drives the sub further from discussion of research and more into a place where spam links reside.

Furthermore, a lot of these beginners tutorials are written by beginners themselves. They contain mistakes, which upon being read by other beginners cloud their understanding and slow their learning.

Can we ban this type of content and push it to /r/learnmachinelearning or something?",876,131,d should beginners tutorials be banned,-0.4588
[D] LLMs are harming AI research,"This is a bold claim, but I feel like LLM hype dying down is long overdue. Not only there has been relatively little progress done to LLM performance and design improvements after GPT4: the primary way to make it better is still just to make it bigger and all alternative architectures to transformer proved to be subpar and inferior, they drive attention (and investment) away from other, potentially more impactful technologies. This is in combination with influx of people without any kind of knowledge of how even basic machine learning works, claiming to be ""AI Researcher"" because they used GPT for everyone to locally  host a model, trying to convince you that ""language models totally can reason. We just need another RAG solution!"" whose sole goal of being in this community is not to develop new tech but to use existing in their desperate attempts to throw together a profitable service. Even the papers themselves are beginning to be largely written by LLMs. I can't help but think that the entire field might plateau simply because the ever growing community is content with mediocre fixes that at best make the model score slightly better on that arbitrary ""score"" they made up, ignoring the glaring issues like hallucinations, context length, inability of basic logic and sheer price of running models this size. I commend people who despite the market hype are working on agents capable of true logical process and hope there will be more attention brought to this soon.",873,280,d llms are harming ai research,-0.5574
[D] Confessions from an ICML reviewer,"Welp, I realize that many of you are about to receive feedback in a couple weeks which will most likely be a reject from ICML. I realize that its difficult to stomach rejection, and I empathize with you as I'm submitting as well and will likely get a reject as well.

But please, please, please, please, as someone who has already spent 20-30 hours reviewing this week, and will likely be spending another 30-40 hours this week on the reviewing process. Please!

Stop submitting unfinished work to conferences.

At this point more than half of the papers I'm reviewing are clearly unfinished work. They have significant, unmistakable flaws to the point that no reasonable person can believe that this work could possibly appear in a peer reviewed, top tier conference. No reasonable person can put these submitted papers next to even the worst ICML paper from the last few years, and believe that yeah, they're of similar or higher quality.

Please take the time to get your work reviewed by your peers, or even your advisor prior to submission. If they can find \*any\* flaw in your work, I assure you, your reviewers are going to find so many flaws and give you a hurtful, and demoralizing review.

I realize that we're all in a huge hype bubble, and we all want to ride the hype train, but reviewing these unfinished works makes me feel so disrespected by the authors. They're clearly submitting for early feedback. It's not fair to the conference system and the peer review process to ask your reviewers to do \*unpaid\* research work for you and advise you on how to construct and present your work. It's not fair to treat your reviewers as free labor.

It takes me at a \*minimum\* 6-7 hours to review one paper, and more likely 10+ hours. That's 10+ hours of my life that these authors think is entitled to them to help them in their research so they can get published. It makes me feel so disrespected, and quite honestly, makes me want to give up on signing up as a reviewer if this is the quality of work I am expected to review.

Not only are these authors being selfish, but they're hurting the overall research community, conference quality, and the peer review process. More unfinished work being submitted, means reviewers have a higher workload. We don't get to spend as much time on each paper as we would like to, meaning \*good well written deserving papers\* either get overlooked, unfairly rejected, or get terrible feedback. This is simply unacceptable!

These authors, quite honestly, are acting like those people who hoard toilet paper during an epidemic. They act selfishly to the detriment of the community, putting themselves above both the research process, and other authors who submit good work.

Please, please, PLEASE don't do this. Submit finished, good work, that you think is ready for publication and peer review.

&#x200B;

Edit: Thanks for the gold award kind stranger. You make me feel a little better about my week.

Edit2: Thanks for the platinum. Thanks for the support/discussion guys.

&#x200B;",872,102,d confessions from an icml reviewer,0.0
[N] China forced the organizers of the International Conference on Computer Vision (ICCV) in South Korea to change Taiwan’s status from a “nation” to a “region” in a set of slides.,"Link: [http://www.taipeitimes.com/News/front/archives/2019/11/02/2003725093](http://www.taipeitimes.com/News/front/archives/2019/11/02/2003725093)

>The Ministry of Foreign Affairs yesterday protested after China forced the organizers of the International Conference on Computer Vision (ICCV) in South Korea to change Taiwan’s status from a “nation” to a “region” in a set of slides.  
>  
>At the opening of the conference, which took place at the COEX Convention and Exhibition Center in Seoul from Tuesday to yesterday, the organizers released a set of introductory slides containing graphics showing the numbers of publications or attendees per nation, including Taiwan.  
>  
>However, the titles on the slides were later changed to “per country/region,” because of a complaint filed by a Chinese participant.  
>  
>“Taiwan is wrongly listed as a country. I think this may be because the person making this chart is not familiar with the history of Taiwan,” the Chinese participant wrote in a letter titled “A mistake at the opening ceremony of ICCV 2019,” which was published on Chinese social media under the name Cen Feng (岑峰), who is a cofounder of leiphone.com.  
>  
>The ministry yesterday said that China’s behavior was contemptible and it would not change the fact that Taiwan does not belong to China.  
>  
>Beijing using political pressure to intervene in an academic event shows its dictatorial nature and that to China, politics outweigh everything else, ministry spokeswoman Joanne Ou (歐江安) said in a statement.  
>  
>The ministry has instructed its New York office to express its concern to the headquarters of the Institute of Electrical and Electronics Engineers, which cosponsored the conference, asking it not to cave in to Chinese pressure and improperly list Taiwan as part of China’s territory, she said.  
>  
>Beijing has to forcefully tout its “one China” principle in the global community because it is already generally accepted that Taiwan is not part of China, she added.  
>  
>As China attempts to force other nations to accept its “one China” principle and sabotage academic freedom, Taiwan hopes that nations that share its freedoms and democratic values can work together to curb Beijing’s aggression, she added.",859,205,n china forced the organizers of the international conference on computer vision iccv in south korea to change taiwans status from a nation to a region in a set of slides,-0.25
[R] GPT-4 didn't really score 90th percentile on the bar exam,"According to [this article](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4441311), OpenAI's claim that it scored 90th percentile on the UBE appears to be based on approximate conversions from estimates of February administrations of the Illinois Bar Exam, which ""are heavily skewed towards repeat test-takers who failed the July administration and score significantly lower than the general test-taking population.""

Compared to July test-takers, GPT-4's UBE score would be 68th percentile, including \~48th on essays. Compared to first-time test takers, GPT-4's UBE score is estimated to be \~63rd percentile, including \~42nd on essays. Compared to those who actually passed, its UBE score would be \~48th percentile, including \~15th percentile on essays.",852,158,r gpt didnt really score th percentile on the bar exam,0.0
[D] Advanced courses update,"EDIT Jan 2021 : I am still updating the list as of Jan, 2021 and will most probably continue to do so for foreseeable future. So, please feel free to message me any courses you find interesting that fit here.

- - -

We have a [PhD level or Advanced courses](https://www.reddit.com/r/MachineLearning/comments/51qhc8/phdlevel_courses/) thread in the sidebar but it's three year old now. There were two other 7-8 month old threads ([1](https://www.reddit.com/r/MachineLearning/comments/cae59l/d_advanced_courses_update/), [2](https://www.reddit.com/r/MachineLearning/comments/cjnund/d_what_are_your_favorite_videos_lectures_on/)) but they don't have many quality responses either. 

So, can we have a new one here?

To reiterate - CS231n, CS229, ones from Udemy etc are not advanced. 

Advanced ML/DL/RL, attempts at building theory of DL, optimization theory, advanced applications etc are some examples of what I believe should belong here, much like the original sidebar post.

You can also suggest (new) categories for the courses you share. :)

- - -

Here are some courses we've found so far. 

ML >> 

* [Learning Discrete Latent Structure - sta4273/csc2547 Spring'18](https://duvenaud.github.io/learn-discrete/)
* [Learning to Search - csc2547 Fall'19](https://duvenaud.github.io/learning-to-search/)
* [Scalable and Flexible Models of Uncertainty - csc2541](https://csc2541-f17.github.io/)
* [Fundamentals of Machine Learning Over Networks - ep3260](https://sites.google.com/view/mlons/home)
* [Machine Learning on Graphs - cs224w](http://web.stanford.edu/class/cs224w/), [videos](https://www.youtube.com/playlist?list=PL-Y8zK4dwCrQyASidb2mjj_itW2-YYx6-)
* [Mining Massive Data Sets - cs246](http://web.stanford.edu/class/cs246/index.html)
* [Interactive Learning - cse599](https://courses.cs.washington.edu/courses/cse599i/20wi/)
* [Machine Learning for Sequential Decision Making Under Uncertainty - ee290s/cs194](https://inst.eecs.berkeley.edu/%7Eee290s/fa18/resources.html)
* [Probabilistic Graphical Methods - 10-708](https://www.cs.cmu.edu/~epxing/Class/10708-20/)
* [Introduction to Causal Inference](https://www.bradyneal.com/causal-inference-course)

ML >> Theory

* [Statistical Machine Learning - 10-702/36-702 with videos](https://www.stat.cmu.edu/~ryantibs/statml/), [2016 videos](https://www.youtube.com/playlist?list=PLTB9VQq8WiaCBK2XrtYn5t9uuPdsNm7YE)
* [Statistical Learning Theory - cs229T/stats231 Stanford Autumn'18-19](http://web.stanford.edu/class/cs229t/)
* [Statistical Learning Theory - cs281b /stat241b UC Berkeley, Spring'14 ](https://www.stat.berkeley.edu/%7Ebartlett/courses/2014spring-cs281bstat241b/)
* [Statistical Learning Theory - csc2532 Uni of Toronto, Spring'20](https://erdogdu.github.io/csc2532/)

ML >> Bayesian

* [Bayesian Data Analysis](https://github.com/avehtari/BDA_course_Aalto)
* [Bayesian Methods Research Group, Moscow](https://bayesgroup.ru/), Bayesian Methods in ML - [spring2020](https://www.youtube.com/playlist?list=PLe5rNUydzV9TjW6dol0gVdWpr02hBicS0), [fall2020](https://www.youtube.com/playlist?list=PLe5rNUydzV9THZg7-QnaLhcccIbQ5eQm8)
* [Deep Learning and Bayesian Methods - summer school](http://deepbayes.ru), videos available for 2019 version

ML >> Systems and Operations

* [Stanford MLSys Seminar Series](https://mlsys.stanford.edu/)
* [Visual Computing Systems- cs348v](http://graphics.stanford.edu/courses/cs348v-18-winter/) - Another systems course that discusses hardware from a persepective of visual computing but is relevant to ML as well 
* [Advanced Machine Learning Systems - cs6787](https://www.cs.cornell.edu/courses/cs6787/2019fa/) - lecture 9 and onwards discuss hardware side of things
* [Machine Learning Systems Design - cs329S](https://stanford-cs329s.github.io/)
* [Topics in Deployable ML - 6.S979](https://people.csail.mit.edu/madry/6.S979/)
* [Machine Learning in Production / AI Engineering (17-445/17-645/17-745/11-695)](https://ckaestne.github.io/seai/)
* [AutoML - Automated Machine Learning](https://ki-campus.org/courses/automl-luh2021)

DL >>

* [Deep Unsupervised Learning - cs294](https://sites.google.com/view/berkeley-cs294-158-sp20/home)
* [Deep Multi-task and Meta learning - cs330](https://cs330.stanford.edu/)
* [Topics in Deep Learning - stat991 UPenn/Wharton](https://github.com/dobriban/Topics-in-deep-learning) *most chapters start with introductory topics and dig into advanced ones towards the end. 
* [Deep Generative Models - cs236](https://deepgenerativemodels.github.io/)
* [Deep Geometric Learning of Big Data and Applications](https://www.ipam.ucla.edu/programs/workshops/workshop-iv-deep-geometric-learning-of-big-data-and-applications/?tab=overview)
* [Deep Implicit Layers - NeurIPS 2020 tutorial](http://implicit-layers-tutorial.org/)

DL >> Theory

* [Topics course on Mathematics of Deep Learning - CSCI-GA 3033](https://joanbruna.github.io/MathsDL-spring19/)
* [Topics Course on Deep Learning - stat212b](http://joanbruna.github.io/stat212b/)
* [Analyses of Deep Learning - stats385](https://stats385.github.io/), [videos from 2017 version](https://www.researchgate.net/project/Theories-of-Deep-Learning)
* [Mathematics of Deep Learning](http://www.vision.jhu.edu/teaching/learning/deeplearning19/)
* [Geometry of Deep Learning](https://www.microsoft.com/en-us/research/event/ai-institute-2019/)

RL >>

* [Meta-Learning - ICML 2019 Tutorial](https://sites.google.com/view/icml19metalearning) , [Metalearning: Applications to Data Mining - google books link](https://books.google.com/books?id=DfZDAAAAQBAJ&printsec=copyright&redir_esc=y#v=onepage&q&f=false)
* [Deep Multi-Task and Meta Learning - cs330](http://cs330.stanford.edu/), [videos](https://www.youtube.com/playlist?list=PLoROMvodv4rMC6zfYmnD7UG3LVvwaITY5)
* [Deep Reinforcement Learning - cs285](http://rail.eecs.berkeley.edu/deeprlcourse/)
* [Advanced robotics - cs287](https://people.eecs.berkeley.edu/%7Epabbeel/cs287-fa19/)
* [Reinforcement Learning - cs234](https://web.stanford.edu/class/cs234/), [videos for 2019 run](https://www.youtube.com/playlist?list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u)
* [Reinforcement Learning Summer School 2019: Bandits, RL & Deep RL](https://rlss.inria.fr/program/)

Optimization >> 

* [Convex Optimization I - ee364a](http://stanford.edu/class/ee364a/), has quite recent [videos](https://www.youtube.com/playlist?list=PLdrixi40lpQm5ksInXlRon1eRwq_gzIcw) too. 
[Convex Optimization II - ee364b](http://web.stanford.edu/class/ee364b/), [2008 videos](https://www.youtube.com/watch?v=U3lJAObbMFI&list=PL3940DD956CDF0622&index=20)
* [Convex Optimization and Approximation - ee227c](https://ee227c.github.io/)
* [Convex Optimization - ee227bt](https://people.eecs.berkeley.edu/%7Eelghaoui/Teaching/EE227BT/index.html)
* [Variational Methods for Computer Vision](https://vision.in.tum.de/teaching/ws2013/vmcv2013)
* [Advanced Optimization and Randomized Algorithms - 10-801](http://www.cs.cmu.edu/%7Esuvrit/teach/index.html), [videos](https://www.youtube.com/playlist?list=PLjTcdlvIS6cjdA8WVXNIk56X_SjICxt0d)
* [Optimization Methods for Machine Learning and Engineering - Karlsruhe Institute of Technology](https://www.youtube.com/playlist?list=PLdkTDauaUnQpzuOCZyUUZc0lxf4-PXNR5)

Applications >> Computer Vision

* [Computational Video Manipulation - cs448v](https://magrawala.github.io/cs448v-sp19/)
* [Advanced Topics in ML: Modeling and Segmentation of Multivariate Mixed Data](http://www.vision.jhu.edu/teaching/learning/learning10/)
* [TUM AI Guest lecture series](https://www.youtube.com/playlist?list=PLQ8Y4kIIbzy8kMlz7cRqz-BjbdyWsfLXt) - many influential researchers in DL, vision, graphics talk about latest advances and their latest works.
* [Advanced Deep Learning for Computer Vision - TUM ADL4CV](https://www.youtube.com/playlist?list=PLog3nOPCjKBkngkkF552-Hiwa5t_ZeDnh)
* [Detection, Segmentation and Tracking - TUM CV3DST](https://www.youtube.com/playlist?list=PLog3nOPCjKBneGyffEktlXXMfv1OtKmCs)
* [Guest lectures at TUM Dynamic Vision and Learning group](https://www.youtube.com/playlist?list=PLog3nOPCjKBnAuymJ7uTysuG357zVn7et)
* [Vision Seminar at MIT](https://www.youtube.com/channel/UCLMiFkFyfcNnZs6iwYLPI9g/videos)
* [Autonomous Vision Group, Talk@Tübingen Seminar](https://www.youtube.com/playlist?list=PLeCNfJWZKqxu-BwwcR4tDBOFNkJEOPWb_)

Applications >> Natural Language Processing

* [Natural Language Processing with Deep Learning - cs224n](http://web.stanford.edu/class/cs224n/) (* not sure if it belongs here, people working in NLP can help me out)
* [Neural networks for NLP - cs11-747](http://www.phontron.com/class/nn4nlp2020/schedule.html)
* [Natural Language Understanding - cs224u](https://web.stanford.edu/class/cs224u/), [video](https://www.youtube.com/playlist?list=PLoROMvodv4rObpMCir6rNNUlFAn56Js20)

Applications >> 3D Graphics 

* [Non-Euclidean Methods in Machine Learning - cs468, 2020](http://graphics.stanford.edu/courses/cs468-20-fall/schedule.html)
* [Machine Learning for 3D Data - cs468, spring 2017](http://graphics.stanford.edu/courses/cs468-17-spring/schedule.html)
* [Data-Driven Shape Analysis - cs468, 2014](http://graphics.stanford.edu/courses/cs468-14-spring/)
* [Geometric Deep Learning](http://geometricdeeplearning.com/) - Not a course but the website links a few tutorials on Geometric DL
* [Deep Learning for Computer Graphics - SIGGRAPH 2019](https://geometry.cs.ucl.ac.uk/creativeai/)
* [Machine Learning for Machine Vision as Inverse Graphics - csc2547 Winter'20](http://www.cs.utoronto.ca/~bonner/courses/2020s/csc2547/) 
* [Machine Learning Meets Geometry, winter 2020](https://geoml.github.io/schedule.html); [Machine Learning for 3D Data, winter 2018](https://cse291-i.github.io/WI18/schedule.html)

---

Edit: Upon suggestion, categorized the courses. There might be some misclassifications as I'm not trained on this task ;). Added some good ones from older (linked above) discussions.",845,87,d advanced courses update,0.25
"Hey all, I'm Sebastian Raschka, author of Machine Learning with Pytorch and Scikit-Learn. Please feel free to ask me anything!","Hello everyone. I am excited about the invitation to do an AMA here. It's my first AMA on reddit, and I will be trying my best!
I recently wrote the ""Machine Learning with Pytorch and Scikit-Learn"" book and joined a startup(Grid.ai) in January. I am also an Assistant Professor of Statistics at the University of Wisconsin-Madison since 2018. Btw. I am also a very passionate Python programmer and love open source.

Please feel free to ask me anything about my [book](https://sebastianraschka.com/blog/2022/ml-pytorch-book.html), working in industry (although my experience is still limited, haha), academia, or my [research projects](https://sebastianraschka.com/publications/). But also don't hesitate to go on tangents and ask about other things -- this is an ask me **anything** after all (... topics like cross-country skiing come to mind).

EDIT:

**Thanks everyone for making my first AMA here a really fun experience! Unfortunately, I have to call it a day, but I had a good time! Thanks for all the good questions, and sorry that I couldn't get to all of them!**",842,107,hey all im sebastian raschka author of machine learning with pytorch and scikitlearn please feel free to ask me anything,0.7088
[D] Let's start 2021 by confessing to which famous papers/concepts we just cannot understand.,"* **Auto-Encoding Variational Bayes  (Variational Autoencoder)**: I understand the main concept, understand the NN implementation, but just cannot understand this paper, which contains a theory that is much more general than most of the implementations suggest.
* **Neural ODE**: I have a background in differential equations, dynamical systems and have course works done on numerical integrations. The theory of ODE is extremely deep (read tomes such as the one by Philip Hartman), but this paper seems to take a short cut to all I've learned about it. Have no idea what this paper is talking about after 2 years. Looked on Reddit, a bunch of people also don't understand and have came up with various extremely bizarre interpretations.
* **ADAM:** this is a shameful confession because I never understood anything beyond the ADAM equations. There are stuff in the paper such as  signal-to-noise ratio, regret bounds, regret proof, and even another algorithm called AdaMax hidden in the paper. Never understood any of it. Don't know the theoretical implications.

I'm pretty sure there are other papers out there. I have not read the **transformer** paper yet, from what I've heard, I might be adding that paper on this list soon.",836,268,d lets start  by confessing to which famous papersconcepts we just cannot understand,0.0
"[N] Stability AI announce their open-source language model, StableLM","Repo: https://github.com/stability-AI/stableLM/

Excerpt from the Discord announcement:

> We’re incredibly excited to announce the launch of StableLM-Alpha; a nice and sparkly newly released open-sourced language model! Developers, researchers, and curious hobbyists alike can freely inspect, use, and adapt our StableLM base models for commercial and or research purposes! *Excited yet?*
>
> Let’s talk about parameters! The Alpha version of the model is available in 3 billion and 7 billion parameters, with 15 billion to 65 billion parameter models to follow. StableLM is trained on a new experimental dataset built on “The Pile” from EleutherAI (a 825GiB diverse, open source language modeling data set that consists of 22 smaller, high quality datasets combined together!) The richness of this dataset gives StableLM surprisingly high performance in conversational and coding tasks, despite its small size of 3-7 billion parameters.",833,176,n stability ai announce their opensource language model stablelm,0.0
[D] Overwhelmed by fast advances in recent weeks,"I was watching the GTC keynote and became entirely overwhelmed by the amount of progress achieved from last year.  I'm wondering how everyone else feels.

&#x200B;

Firstly, the entire ChatGPT, GPT-3/GPT-4 chaos has been going on for a few weeks, with everyone scrambling left and right to integrate chatbots into their apps, products, websites. Twitter is flooded with new product ideas, how to speed up the process from idea to product, countless promp engineering blogs, tips, tricks, paid courses.

&#x200B;

Not only was ChatGPT disruptive, but a few days later, Microsoft and Google also released their models and integrated them into their search engines. Microsoft also integrated its LLM into its Office suite. It all happenned overnight. I understand that they've started integrating them along the way, but still, it seems like it hapenned way too fast. This tweet encompases the past few weeks perfectly [https://twitter.com/AlphaSignalAI/status/1638235815137386508](https://twitter.com/AlphaSignalAI/status/1638235815137386508) , on a random Tuesday countless products are released that seem revolutionary.

&#x200B;

In addition to the language models, there are also the generative art models that have been slowly rising in mainstream recognition. Now Midjourney AI is known by a lot of people who are not even remotely connected to the AI space.

&#x200B;

For the past few weeks, reading Twitter, I've felt completely overwhelmed, as if the entire AI space is moving beyond at lightning speed, whilst around me we're just slowly training models, adding some data, and not seeing much improvement, being stuck on coming up with ""new ideas, that set us apart"".

&#x200B;

Watching the GTC keynote from NVIDIA I was again, completely overwhelmed by how much is being developed throughout all the different domains. The ASML EUV (microchip making system) was incredible, I have no idea how it does lithography and to me it still seems like magic. The Grace CPU with 2 dies (although I think Apple was the first to do it?) and 100 GB RAM, all in a small form factor. There were a lot more different hardware servers that I just blanked out at some point. The omniverse sim engine looks incredible, almost real life (I wonder how much of a domain shift there is between real and sim considering how real the sim looks). Beyond it being cool and usable to train on synthetic data, the car manufacturers use it to optimize their pipelines. This change in perspective, of using these tools for other goals than those they were designed for I find the most interesting.

&#x200B;

The hardware part may be old news, as I don't really follow it, however the software part is just as incredible. NVIDIA AI foundations (language, image, biology models), just packaging everything together like a sandwich. Getty, Shutterstock and Adobe will use the generative models to create images. Again, already these huge juggernauts are already integrated.

&#x200B;

I can't believe the point where we're at. We can use AI to write code, create art, create audiobooks using Britney Spear's voice, create an interactive chatbot to converse with books, create 3D real-time avatars, generate new proteins (?i'm lost on this one), create an anime and countless other scenarios. Sure, they're not perfect, but the fact that we can do all that in the first place is amazing.

&#x200B;

As Huang said in his keynote, companies want to develop ""disruptive products and business models"". I feel like this is what I've seen lately. Everyone wants to be the one that does something first, just throwing anything and everything at the wall and seeing what sticks.

&#x200B;

In conclusion, I'm feeling like the world is moving so fast around me whilst I'm standing still. I want to not read anything anymore and just wait until everything dies down abit, just so I can get my bearings. However, I think this is unfeasible. I fear we'll keep going in a frenzy until we just burn ourselves at some point.

&#x200B;

How are you all fairing? How do you feel about this frenzy in the AI space? What are you the most excited about?",828,331,d overwhelmed by fast advances in recent weeks,0.0516
[N] 20 hours of new lectures on Deep Learning and Reinforcement Learning with lots of examples,"If anyone's interested in a Deep Learning and Reinforcement Learning series, I uploaded 20 hours of lectures on YouTube yesterday. Compared to other lectures, I think this gives quite a broad/compact overview of the fields with lots of minimal examples to build on. Here are the links:

**Deep Learning** ([playlist](https://www.youtube.com/playlist?list=PLMsTLcO6etti_SObSLvk9ZNvoS_0yia57))  
*The first five lectures are more theoretical, the second half is more applied.*

* Lecture 1: Introduction. ([slides](https://cwkx.github.io/data/teaching/dl-and-rl/dl-lecture1.pdf), [video](https://www.youtube.com/watch?v=s2uXPz3wyCk&list=PLMsTLcO6etti_SObSLvk9ZNvoS_0yia57&index=1))
* Lecture 2: Mathematical principles and backpropagation. ([slides](https://cwkx.github.io/data/teaching/dl-and-rl/dl-lecture2.pdf), [colab](https://colab.research.google.com/gist/cwkx/dfa207c8ceed5999bdad1ec6f637dd47/distributions.ipynb), [video](https://www.youtube.com/watch?v=dfZ0cIQSjm4&list=PLMsTLcO6etti_SObSLvk9ZNvoS_0yia57&index=2))
* Lecture 3: PyTorch programming: *coding session*. ([colab1](https://colab.research.google.com/gist/cwkx/441e508d3b904413fd3950a09a1d3bd6/classifier.ipynb), [colab2](https://colab.research.google.com/gist/cwkx/3a6eba039aa9f68d0b9d37a02216d385/convnet.ipynb), [video](https://www.youtube.com/watch?v=KiqXWOcz4Z0&list=PLMsTLcO6etti_SObSLvk9ZNvoS_0yia57&index=3)) - minor issues with audio, but it fixes itself later.
* Lecture 4: Designing models to generalise. ([slides](https://cwkx.github.io/data/teaching/dl-and-rl/dl-lecture4.pdf), [video](https://www.youtube.com/watch?v=4vKKj8bkS-E&list=PLMsTLcO6etti_SObSLvk9ZNvoS_0yia57&index=4))
* Lecture 5: Generative models. ([slides](https://cwkx.github.io/data/teaching/dl-and-rl/dl-lecture5.pdf), [desmos](https://www.desmos.com/calculator/2sboqbhler), [colab](https://colab.research.google.com/gist/cwkx/e3ef25d0adb6e2f2bf747ce664bab318/conv-autoencoder.ipynb), [video](https://www.youtube.com/watch?v=hyxlTwvLi-o&list=PLMsTLcO6etti_SObSLvk9ZNvoS_0yia57&index=5))
* Lecture 6: Adversarial models. ([slides](https://cwkx.github.io/data/teaching/dl-and-rl/dl-lecture6.pdf), [colab1](https://colab.research.google.com/gist/cwkx/74e33bc96f94f381bd15032d57e43786/simple-gan.ipynb), [colab2](https://colab.research.google.com/gist/cwkx/348cde3bf11a08c45a69b1873ebb6de3/conditional-gan.ipynb), [colab3](https://colab.research.google.com/gist/cwkx/7f5377ed8414a096180128b487846698/info-gan.ipynb), [colab4](https://colab.research.google.com/gist/cwkx/aece978bc38ba35c2267d91b793a1456/unet.ipynb), [video](https://www.youtube.com/watch?v=JLHyU7AjB4s&list=PLMsTLcO6etti_SObSLvk9ZNvoS_0yia57&index=6))
* Lecture 7: Energy-based models. ([slides](https://cwkx.github.io/data/teaching/dl-and-rl/dl-lecture7.pdf), [colab](https://colab.research.google.com/gist/cwkx/6b2d802e804e908a3ee3d58c1e0e73be/dbm.ipynb), [video](https://www.youtube.com/watch?v=kpulMklVmRU&list=PLMsTLcO6etti_SObSLvk9ZNvoS_0yia57&index=7))
* Lecture 8: Sequential models: *by* u/samb-t. ([slides](https://cwkx.github.io/data/teaching/dl-and-rl/dl-lecture8.pdf), [colab1](https://colab.research.google.com/gist/samb-t/ac6dbd433c618eedcd0442f577697ea3/generative-rnn.ipynb), [colab2](https://colab.research.google.com/gist/samb-t/27cc3217799825975b65326d6e7b377b/transformer-translation.ipynb), [video](https://www.youtube.com/watch?v=pxRnFwNFTOM&list=PLMsTLcO6etti_SObSLvk9ZNvoS_0yia57&index=8))
* Lecture 9: Flow models and implicit networks. ([slides](https://cwkx.github.io/data/teaching/dl-and-rl/dl-lecture9.pdf), [SIREN](https://vsitzmann.github.io/siren/), [GON](https://cwkx.github.io/data/GON/), [video](https://www.youtube.com/watch?v=zRdwh9C5xn4&list=PLMsTLcO6etti_SObSLvk9ZNvoS_0yia57&index=9))
* Lecture 10: Meta and manifold learning. ([slides](https://cwkx.github.io/data/teaching/dl-and-rl/dl-lecture10.pdf), [interview](https://youtu.be/PqbB07n_uQ4?t=444), [video](https://www.youtube.com/watch?v=na1-oIn8Kdo&list=PLMsTLcO6etti_SObSLvk9ZNvoS_0yia57&index=10))

**Reinforcement Learning** ([playlist](https://www.youtube.com/playlist?list=PLMsTLcO6ettgmyLVrcPvFLYi2Rs-R4JOE))  
*This is based on David Silver's course but targeting younger students within a shorter 50min format (missing the advanced derivations) + more examples and Colab code.*

* Lecture 1: Foundations. ([slides](https://cwkx.github.io/data/teaching/dl-and-rl/rl-lecture1.pdf), [video](https://www.youtube.com/watch?v=K67RJH3V7Yw&list=PLMsTLcO6ettgmyLVrcPvFLYi2Rs-R4JOE&index=1))
* Lecture 2: Markov decision processes. ([slides](https://cwkx.github.io/data/teaching/dl-and-rl/rl-lecture2.pdf), [colab](https://colab.research.google.com/gist/cwkx/ba6c44031137575d2445901ee90454da/mrp.ipynb), [video](https://www.youtube.com/watch?v=RmOdTQYQqmQ&list=PLMsTLcO6ettgmyLVrcPvFLYi2Rs-R4JOE&index=2))
* Lecture 3: OpenAI gym. ([video](https://www.youtube.com/watch?v=BNSwFURmaCA&list=PLMsTLcO6ettgmyLVrcPvFLYi2Rs-R4JOE&index=3))
* Lecture 4: Dynamic programming. ([slides](https://cwkx.github.io/data/teaching/dl-and-rl/rl-lecture4.pdf), [colab](https://colab.research.google.com/gist/cwkx/670c8d44a9a342355a4a883c498dbc9d/dynamic-programming.ipynb), [video](https://www.youtube.com/watch?v=gqC_p2XWpLU&list=PLMsTLcO6ettgmyLVrcPvFLYi2Rs-R4JOE&index=4))
* Lecture 5: Monte Carlo methods. ([slides](https://cwkx.github.io/data/teaching/dl-and-rl/rl-lecture5.pdf), [colab](https://colab.research.google.com/gist/cwkx/a5129e8888562d1b4ecb0da611c58ce8/monte-carlo-methods.ipynb), [video](https://www.youtube.com/watch?v=4xfWzLmIccs&list=PLMsTLcO6ettgmyLVrcPvFLYi2Rs-R4JOE&index=5))
* Lecture 6: Temporal-difference methods. ([slides](https://cwkx.github.io/data/teaching/dl-and-rl/rl-lecture6.pdf), [colab](https://colab.research.google.com/gist/cwkx/54e2e6d59918a083e47f19404fe275b4/temporal-difference-learning.ipynb), [video](https://www.youtube.com/watch?v=phgI_880uSw&list=PLMsTLcO6ettgmyLVrcPvFLYi2Rs-R4JOE&index=6))
* Lecture 7: Function approximation. ([slides](https://cwkx.github.io/data/teaching/dl-and-rl/rl-lecture7.pdf), [code](https://github.com/higgsfield/RL-Adventure), [video](https://www.youtube.com/watch?v=oqmCj95d3Y4&list=PLMsTLcO6ettgmyLVrcPvFLYi2Rs-R4JOE&index=7))
* Lecture 8: Policy gradient methods. ([slides](https://cwkx.github.io/data/teaching/dl-and-rl/rl-lecture8.pdf), [code](https://github.com/higgsfield/RL-Adventure-2), [theory](https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html), [video](https://www.youtube.com/watch?v=h4HixR0Co6Q&list=PLMsTLcO6ettgmyLVrcPvFLYi2Rs-R4JOE&index=8))
* Lecture 9: Model-based methods. ([slides](https://cwkx.github.io/data/teaching/dl-and-rl/rl-lecture9.pdf), [video](https://www.youtube.com/watch?v=aUjuBvqJ8UM&list=PLMsTLcO6ettgmyLVrcPvFLYi2Rs-R4JOE&index=9))
* Lecture 10: Extended methods. ([slides](https://cwkx.github.io/data/teaching/dl-and-rl/rl-lecture10.pdf), [atari](https://www.youtube.com/playlist?list=PL34t13IwtOXUNliyyJtoamekLAbqhB9Il), [video](https://www.youtube.com/watch?v=w6rGqprrxp8&list=PLMsTLcO6ettgmyLVrcPvFLYi2Rs-R4JOE&index=10))",823,43,n  hours of new lectures on deep learning and reinforcement learning with lots of examples,0.0
[R] Google has a credit assignment problem in research,"Google has some serious cultural problems with proper credit assignment. They continue to rename methods discovered earlier DESPITE admitting the existence of this work.

See this new paper they released:

[https://arxiv.org/abs/2006.14536](https://arxiv.org/abs/2006.14536)

Stop calling this method SWISH; its original name is SILU. The original Swish authors from Google even admitted to this mistake in the past ([https://www.reddit.com/r/MachineLearning/comments/773epu/r\_swish\_a\_selfgated\_activation\_function\_google/](https://www.reddit.com/r/MachineLearning/comments/773epu/r_swish_a_selfgated_activation_function_google/)). And the worst part is this new paper has the very same senior author as the previous Google paper.

And just a couple weeks ago, the same issue again with the SimCLR paper. See thread here:

[https://www.reddit.com/r/MachineLearning/comments/hbzd5o/d\_on\_the\_public\_advertising\_of\_neurips/fvcet9j/?utm\_source=share&utm\_medium=web2x](https://www.reddit.com/r/MachineLearning/comments/hbzd5o/d_on_the_public_advertising_of_neurips/fvcet9j/?utm_source=share&utm_medium=web2x)

They site only cite prior work with the same idea in the last paragraph of their supplementary and yet again rename the method to remove its association to the prior work. This is unfair. Unfair to the community and especially unfair to the lesser known researchers who do not have the advertising power of Geoff Hinton and Quoc Le on their papers.

SiLU/Swish is by Stefan Elfwing, Eiji Uchibe, Kenji Doya ([https://arxiv.org/abs/1702.03118](https://arxiv.org/abs/1702.03118)).

Original work of SimCLR is by Mang Ye, Xu Zhang, Pong C. Yuen, Shih-Fu Chang ([https://arxiv.org/abs/1904.03436](https://arxiv.org/abs/1904.03436))

Update:

Dan Hendrycks and Kevin Gimpel also proposed the SiLU non-linearity in 2016 in their work Gaussian Error Linear Units (GELUs) ([https://arxiv.org/abs/1606.08415](https://arxiv.org/abs/1606.08415))

Update 2:

""Smooth Adversarial Training"" by Cihang Xie is only an example of the renaming issue because of issues in the past by Google to properly assign credit. Cihang Xie's work is not the cause of this issue. Their paper does not claim to discover a new activation function. They are only using the SiLU activation function in some of their experiments under the name Swish. [Cihang Xie will provide an update of the activation function naming used in the paper](https://www.reddit.com/r/MachineLearning/comments/hkiyir/r\_google\_has\_a\_credit\_assignment\_problem\_in/fwtttqo?utm\_source=share&utm\_medium=web2x) to reflect the correct naming. 

The cause of the issue is Google in the past decided to continue with renaming the activation as [Swish despite being made aware of the method already having the name SiLU](https://arxiv.org/abs/1710.05941). Now it is stuck in our research community and stuck in our ML libraries (https://github.com/tensorflow/tensorflow/issues/41066).",831,126,r google has a credit assignment problem in research,-0.0258
[Project] This Word Does Not Exist,"Hello! I've been working on [this word does not exist](http://www.thisworddoesnotexist.com/). In it, I ""learned the dictionary"" and trained a GPT-2 language model over the Oxford English Dictionary. Sampling from it, you get realistic sounding words with fake definitions and example usage, e.g.:

>**pellum (noun)**  
>  
>the highest or most important point or position  
>  
>*""he never shied from the pellum or the right to preach""*

On the [website](http://www.thisworddoesnotexist.com/), I've also made it so you can prime the algorithm with a word, and force it to come up with an example, e.g.:

>[redditdemos](https://www.thisworddoesnotexist.com/w/redditdemos/eyJ3IjogInJlZGRpdGRlbW9zIiwgImQiOiAicmVqZWN0aW9ucyBvZiBhbnkgZ2l2ZW4gcG9zdCBvciBjb21tZW50LiIsICJwIjogInBsdXJhbCBub3VuIiwgImUiOiAiYSBzdWJyZWRkaXRkZW1vcyIsICJzIjogWyJyZWQiLCAiZGl0IiwgImRlIiwgIm1vcyJdfQ==.vySthHa3YR4Zg_oWbKqt5If_boekKDzBsR9AEP_5Z8k=) **(noun)**  
>  
>rejections of any given post or comment.  
>  
>*""a subredditdemos""*

Most of the project was spent throwing a number of rejection tricks to make good samples, e.g.,

* Rejecting samples that contain words that are in the a training set / blacklist to force generation completely novel words
* Rejecting samples without the use of the word in the example usage
* Running a part of speech tagger on the example usage to ensure they use the word in the correct POS

Source code link: [https://github.com/turtlesoupy/this-word-does-not-exist](https://github.com/turtlesoupy/this-word-does-not-exist)

Thanks!",825,141,project this word does not exist,0.0
[D] Siraj Raval's official apology regarding his plagiarized paper,"> I’ve seen claims that my Neural Qubit paper was partly plagiarized. This is true & I apologize. I made the vid & paper in 1 week to align w/ my “2 vids/week” schedule. I hoped to inspire others to research. Moving forward, I’ll slow down & being more thoughtful about my output

What do you guys think about this?",819,321,d siraj ravals official apology regarding his plagiarized paper,0.0516
[D] Why machine learning is more boring than you may think,"I came across [this interview with a machine learning tech lead](https://crossminds.ai/video/5fb2e4a686dab96c840acd9e/?playlist_id=5f07c51e2de531fe96279ccb). He discusses the reality of ML deployments in four major parts of his work and how to cope with the boringness. Here is a quick summary and you can also check out the [original blog](https://towardsdatascience.com/data-science-is-boring-1d43473e353e) he wrote.

[**1. Designing**](https://crossminds.ai/video/5fb2e4a686dab96c840acd9e/?timecode=114.57635909155273)

\- Expected: Apply the latest & greatest algorithms on every project

\- Reality: Implement algorithms that will get the job done within the timeframe.

[**2. Coding**](https://crossminds.ai/video/5fb2e4a686dab96c840acd9e/?timecode=175.29553207390975)

\- Expected: Spend most time coding the ML component

\- Reality: Spend most time coding everything else (system, data pipeline, etc.)

[**3. Debugging**](https://crossminds.ai/video/5fb2e4a686dab96c840acd9e/?timecode=274.7941132145767)

\- Expected: Improve model performance (intellectually challenging & rewarding)

\- Reality: Fix traditional software issues to get a good enough result and move on

[**4. Firefighting**](https://crossminds.ai/video/5fb2e4a686dab96c840acd9e/?timecode=365.2176719809265)

\- Expected: not much

\- Reality: deal with unexpected internal/external problems all the time

[**Some coping mechanisms:**](https://crossminds.ai/video/5fb2e4a686dab96c840acd9e/?timecode=483.4506288521805)

Developing side projects, gamifying the debug process, talking to people in the industry, etc.

**Bottom line:**  You would need to accept that there are a lot more than just developing smart algorithms in a machine learning career. Try to cope with the frustration and boringness, and ""enjoy the small reward along the way and the final victory"".

 (I'd agree with most of his thoughts. In fact, this is a common reality for most research deployments. Any thoughts or experience?)",818,126,d why machine learning is more boring than you may think,-0.3804
[D] How is it that the YouTube recommendation system has gotten WORSE in recent years?,"Currently, the recommendation system seems so bad it's basically broken. I get videos recommended to me that I've just seen (probably because I've re-""watched"" music). I rarely get recommendations from interesting channels I enjoy, and there is almost no diversity in the sort of recommendations I get, despite my diverse interests. I've used the same google account for the past 6 years and I can say that recommendations used to be significantly better.

What do you guys think may be the reason it's so bad now?

Edit:

I will say my personal experience of youtube hasn't been about political echo-cambers but that's probably because I rarely watch political videos and when I do, it's usually a mix of right-wing and left-wing. But I have a feeling that if I did watch a lot of political videos, it would ultimately push me toward one side, which would be a bad experience for me because both sides can have idiotic ideas and low quality content.

Also anecdotally, I have spent LESS time on youtube than I did in the past. I no longer find interesting rabbit holes. ",815,231,d how is it that the youtube recommendation system has gotten worse in recent years,-0.5904
"[R] Meta, INRIA researchers discover that explicit registers eliminate ViT attention spikes","When visualizing the inner workings of vision transformers (ViTs), researchers noticed weird spikes of attention on random background patches. This didn't make sense since the models should focus on foreground objects.

By analyzing the output embeddings, they found a small number of tokens (2%) had super high vector norms, causing the spikes.

The high-norm ""outlier"" tokens occurred in redundant areas and held less local info but more global info about the image.

Their hypothesis is that ViTs learn to identify unimportant patches and recycle them as temporary storage instead of discarding. This enables efficient processing but causes issues.

Their fix is simple - just add dedicated ""register"" tokens that provide storage space, avoiding the recycling side effects.

Models trained with registers have:

* Smoother and more meaningful attention maps
* Small boosts in downstream performance
* Way better object discovery abilities

The registers give ViTs a place to do their temporary computations without messing stuff up. Just a tiny architecture tweak improves interpretability and performance. Sweet!

I think it's cool how they reverse-engineered this model artifact and fixed it with such a small change. More work like this will keep incrementally improving ViTs.

TLDR: Vision transformers recycle useless patches to store data, causing problems. Adding dedicated register tokens for storage fixes it nicely.

[**Full summary**](https://notes.aimodels.fyi/demystifying-the-artifacts-in-vision-transformer-models/)**.** Paper is [here](https://arxiv.org/pdf/2309.16588.pdf).",812,48,r meta inria researchers discover that explicit registers eliminate vit attention spikes,0.0
[D] Hey Reddit! We're a bunch of research scientists and software engineers and we just open sourced a new state-of-the-art AI model that can translate between 200 different languages. We're excited to hear your thoughts so we're hosting an AMA on 07/21/2022 @ 9:00AM PT. Ask Us Anything!,"PROOF: [https://i.redd.it/2z42nlnbssc91.jpg](https://i.redd.it/2z42nlnbssc91.jpg)

We’re part of the team behind Meta AI’s latest AI breakthrough in machine translation with our No Language Left Behind (NLLB) project. It’s a translation system that can support over 200 languages, even if there isn't a lot of text available to learn from.   The reality is that a handful of languages dominate the web meaning only a fraction of the world can access content and contribute to the web in their own language. We want to change this by creating more inclusive machine translations systems – ones that unlock access to the web for the more than 4B people around the world that are currently excluded because they do not speak one of the few languages content is available in.   Here are a few things about NLLB we’re excited for:

* Latest breakthrough: we created a single model that translates over 200 different languages with state-of-the-art results.
* Billions of translations: We’re applying the techniques from the research advancements from NLLB to support more than 25 billion translations served every day on Facebook News Feed, Instagram, and our other platforms.
* Meta’s AI Research SuperCluster (RSC): This large-scale conditional language model is one of the first AI models trained on Meta’s AI Research SuperCluster (RSC) supercomputer.
* Open sourcing: By open sourcing our model and publishing a slew of research tools, we hope that AI researchers whose languages are not supported well or at all on commercial translations services could use our model to create support for that language. Furthermore, we’ve open sourced datasets, such as NLLB-Seed and FLORES-200 evaluation benchmark, which doubles the existing language coverage over our previous benchmark.
* Wikimedia Foundation collaboration: We collaborated with the Wikimedia Foundation to help improve translation systems on their Content Translations tool. Editors can now more efficiently translate and edit articles in 20  low-resource languages, including 10 that previously were not supported by any machine translation tools on the platform. 
* Books translation: we’re partnering with local publishers around the world to translate children’s stories.

You can check out some of our materials and open sourced artifacts here: 

* Our latest blog post: [https://ai.facebook.com/blog/nllb-200-high-quality-machine-translation](https://ai.facebook.com/blog/nllb-200-high-quality-machine-translation)
* Project Overview: [https://ai.facebook.com/research/no-language-left-behind/ ](https://ai.facebook.com/research/no-language-left-behind/ )
* Product demo: [https://nllb.metademolab.com/](https://nllb.metademolab.com/)
* Research paper: [https://research.facebook.com/publications/no-language-left-behind](https://research.facebook.com/publications/no-language-left-behind)
* NLLB-200: [https://github.com/facebookresearch/fairseq/tree/nllb](https://github.com/facebookresearch/fairseq/tree/nllb)
* FLORES-200: [https://github.com/facebookresearch/flores](https://github.com/facebookresearch/flores)
* LASER3: [https://github.com/facebookresearch/LASER](https://github.com/facebookresearch/LASER)  

Joining us today for the AMA are:

* Angela Fan (AF), Research Scientist 
* Jean Maillard (JM), Research Scientist
* Maha Elbayad (ME), Research Scientist
* Philipp Koehn (PK), Research Scientist
* Shruti Bhosale (SB), Software Engineer  

We’ll be here from 07/21/2022 @09:00AM PT - 10:00AM PT 

Thanks and we’re looking forward to answering your questions!

**EDIT 10:30am PT:** Thanks for all the questions, we’re signing off! We had a great time and we’re glad to answer so many thoughtful questions!",802,117,d hey reddit were a bunch of research scientists and software engineers and we just open sourced a new stateoftheart ai model that can translate between  different languages were excited to hear your thoughts so were hosting an ama on   am pt ask us anything,0.4559
[P] I turned Stable Diffusion into a lossy image compression codec and it performs great!,"After playing around with the Stable Diffusion source code a bit, I got the idea to use it for lossy image compression and it works even better than expected.
Details and colab source code here:

https://matthias-buehlmann.medium.com/stable-diffusion-based-image-compresssion-6f1f0a399202?source=friends_link&sk=a7fb68522b16d9c48143626c84172366",805,102,p i turned stable diffusion into a lossy image compression codec and it performs great,0.6588
[R] 🐶 Bark - Text2Speech...But with Custom Voice Cloning using your own audio/text samples 🎙️📝,"We've got some cool news for you. You know Bark, the new Text2Speech model, right? It was released with some voice cloning restrictions and ""allowed prompts"" for safety reasons. 🐶🔊

&#x200B;

But we believe in the power of creativity and wanted to explore its potential! 💡 So, we've reverse engineered the voice samples, removed those ""allowed prompts"" restrictions, and created a set of user-friendly Jupyter notebooks! 🚀📓

&#x200B;

Now you can clone audio using just 5-10 second samples of audio/text pairs! 🎙️📝 Just remember, with great power comes great responsibility, so please use this wisely. 😉

&#x200B;

[Check out our website](https://serp.ly/@serpai/bark) for a post on this release. 🐶

Check out our [GitHub repo](https://github.com/serp-ai/bark-with-voice-clone) and give it a whirl 🌐🔗

&#x200B;

We'd love to hear your thoughts, experiences, and creative projects using this alternative approach to Bark! 🎨 So, go ahead and share them in the comments below. 🗨️👇

&#x200B;

Happy experimenting, and have fun! 😄🎉

If you want to check out more of our projects, [check out our github!](https://github.com/serp-ai)

[Check out our discord](https://devin.to/discord) to chat about AI with some friendly people or need some support 😄",800,78,r  bark  textspeechbut with custom voice cloning using your own audiotext samples ,0.0
OpenAI is now complaining about regulation of AI [D],"I held off for a while but hypocrisy just drives me nuts after hearing this.

SMH this company like white knights who think they are above everybody. They want regulation but they want to be untouchable by this regulation. Only wanting to hurt other people but not “almighty” Sam and friends.

Lies straight through his teeth to Congress about suggesting similar things done in the EU, but then starts complain about them now. This dude should not be taken seriously in any political sphere whatsoever.

My opinion is this company is anti-progressive for AI by locking things up which is contrary to their brand name. If they can’t even stay true to something easy like that, how should we expect them to stay true with AI safety which is much harder?

I am glad they switch sides for now, but pretty ticked how they think they are entitled to corruption to benefit only themselves. SMH!!!!!!!!

What are your thoughts?",793,345,openai is now complaining about regulation of ai d,-0.2023
[D] Why is tensorflow so hated on and pytorch is the cool kids framework?,"I have seen so many posts on social media about how great pytorch is and, in one latest tweet, 'boomers' use tensorflow ... It doesn't make sense to me and I see it as being incredibly powerful and widely used in research and industry. Should I be jumping ship? What is the actual difference and why is one favoured over the other? I have only used tensorflow and although I have been using it for a number of years now, still am learning. Should I be switching? Learning both? I'm not sure this post will answer my question but I would like to hear your honest opinion why you use one over the other or when you choose to use one instead of the other.

EDIT: thank you all for your responses. I honestly did not expect to get this much information and I will definitely be taking a harder look at Pytorch and maybe trying it in my next project. For those of you in industry, do you see tensorflow used more or Pytorch in a production type implementation? My work uses tensorflow and I have heard it is used more outside of academia - mixed maybe at this point?

EDIT2: I read through all the comments and here are my summaries and useful information to anyone new seeing this post or having the same question: 

TL;DR: People were so frustrated with TF 1.x that they switched to PT and never came back.

* Python is 30 years old FYI 
* Apparently JAX is actually where the cool kids are … this is feeling like highschool again, always the wrong crowd. 
* Could use pytorch to develop then convert with ONNX to tensorflow for deployment 
* When we say TF we should really say tf.keras. I would not wish TF 1.x on my worst enemy. 
* Can use PT in Colab. PT is also definitely popular on Kaggle
* There seems to be some indie kid rage where big brother google is not loved so TF is not loved. 
* TF 2.x with tf.keras and PT seem to now do similar things. However see below for some details. Neither seems perfect but I am now definitely looking at PT. Just looking at the installation and docs is a winner. As a still TF advocate (for the time being) I encourage you to check out TF 2.x - a lot of comments are related to TF 1.x Sessions etc.

Reasons for: 

* PT can feel laborious. With tf.keras it seems to be simpler and quicker, however also then lack of control. 
* Seems to still win the production argument 
* TF is now TF.Keras. Eager execution etc. has made it more align with PT 
* TF now has numpy implementation right in there. As well as gradient tape in for loop fashion making it actually really easy to manipulate tensors.
* PT requires a custom training loop from the get go. Maybe TF 2.x easier then for beginners now and can be faster to get a quick and dirty implementation / transfer learning. 
* PT requires to specify the hardware too (?) You need to tell it which gpu to use? This was not mentioned but that is one feeling I had. 
* Tf.keras maybe more involved in industry because of short implementation time 
* Monitoring systems? Not really mentioned but I don't know what is out there for PT. eg TF dashboard, projector
* PT needs precise handling of input output layer sizes. You have to know math.
* How is PT on edge devices - is there tfLite equivalent? PT Mobile it seems

Reason for Pytorch or against TF:

* Pythonic
* Actually opensource
* Steep learning curve for TF 1.x. Many people seem to have switched and never looked back on TF 2.x. Makes sense since everything is the same for PT since beginning
* Easier implementation (it just works is a common comment)
* Backward compatibility and framework changes in TF. RIP your 1.x code. Although I have heard there is a tool to auto convert to TF 2.x - never tried it though. I'm sure it fails unless your code is perfect. Pytorch is stable through and through.
* Installation. 3000 series GPUs. I already have experience with this. I hate having to install TF on any new system. Looks like PT is easier and more compatible.
* Academia is on PT kick. New students learning it as the first. Industry doesn't seem to care much as long as it works and any software devs can use it.
* TF has an issue of many features / frameworks trying to be forced together, creating incompatibility issues. Too many ways to do one thing, not all of which will actually do what you need down the road. 
* Easier documentation - potentially. 
* The separation between what is in tf and tf.keras
* Possible deprecation for Jax, although with all the hype I honestly see Jax maybe just becoming TF 3.x
* Debug your model by accessing intermediate representations (Is this what MLIR in TF is now?)
* Slow TF start-up
* PyTorch has added support for ROCm 4.0 which is still in beta. You can now use AMD GPUs! WOW - that would be great, although I like the nvidia monopoly for my stocks!
* Although tf.keras is now simple and quick, it may be oversimplified. PT seems to be a nice middle for any experimentation. 

Funny / excellent comments: 

* ""I'd rather be punched in the face than having to use TensorFlow ever again."" 
* "" PyTorch == old-style Lego kits where they gave pretty generic blocks that you could combine to create whatever you want. TensorFlow == new-style Lego kits with a bunch of custom curved smooth blocks, that you can combine to create the exact picture on the box; but is awkward to build anything else. 
* On the possibility of dropping TF for Jax. ""So true, Google loves killing things: hangouts, Google plus, my job application.."" 
* ""I've been using PyTorch a few months now and I've never felt better. I have more energy. My skin is clearer. My eye sight has improved. - Andrej Karpathy (2017)"" 
* ""I feel like there is 'I gave up on TF and never looked back feel here'""
* ""I hated the clusterfuck of intertwined APIs of TF2."" 
* ""…Pytorch had the advantage of being the second framework that could learn from the mistakes of Tensorflow - hence it's huge success."" 
* ""Keras is the gateway drug of DL!"" 
* ""like anything Google related they seemed to put a lot of effort into making the docs extremely unreadable and incomplete"" 
* ""more practical imo, pytorch is - the yoda bot"" 
* ""Pytorch easy, tensorflow hard, me lazy, me dumb. Me like pytorch.""",790,265,d why is tensorflow so hated on and pytorch is the cool kids framework,-0.6207
[N] OpenAI Gym is now actively maintained again (by me)! Here's my plan,"So OpenAI made me a maintainer of Gym. This means that all the installation issues will be fixed, the now 5 year backlog of PRs will be resolved, and in general Gym will now be reasonably maintained. I posted my manifesto for future maintenance here: [https://github.com/openai/gym/issues/2259](https://github.com/openai/gym/issues/2259)  


Edit: I've been getting a bunch of messages about open source donations, so I created links:

[https://liberapay.com/jkterry](https://liberapay.com/jkterry)

[https://www.buymeacoffee.com/jkterry](https://www.buymeacoffee.com/jkterry)",787,47,n openai gym is now actively maintained again by me heres my plan,0.3802
[P] My side project: Cloud GPUs for 1/3 the cost of AWS/GCP,"Some of you may have seen me comment around, now it’s time for an official post!

I’ve just finished building a little side project of mine - [https://gpu.land/](https://gpu.land/).

**What is it?** Cheap GPU instances in the cloud.

**Why is it awesome?**

* It’s dirt-cheap. You get a Tesla V100 for $0.99/hr, which is 1/3 the cost of AWS/GCP/Azure/\[insert big cloud name\].
* It’s dead simple. It takes 2mins from registration to a launched instance. Instances come pre-installed with everything you need for Deep Learning, including a 1-click Jupyter server.
* It sports a retro, MS-DOS-like look. Because why not:)

I’m a self-taught ML engineer. I built this because when I was starting my ML journey I was totally lost and frustrated by AWS. Hope this saves some of you some nerve cells (and some pennies)!

The most common question I get is - how is this so cheap? The answer is because AWS/GCP are charging you a huge markup and I’m not. In fact I’m charging just enough to break even, and built this project really to give back to community (and to learn some of the tech in the process). 

AMA!",785,213,p my side project cloud gpus for  the cost of awsgcp,0.0
"[D] Google researchers achieve performance breakthrough, rendering Stable Diffusion images in sub-12 seconds on a mobile phone. Generative AI models running on your mobile phone is nearing reality.","**What's important to know:**

&#x200B;

*  Stable Diffusion is an \\\~1-billion parameter model that is typically resource intensive. DALL-E sits at 3.5B parameters, so there are even heavier models out there.
*  Researchers at Google layered in a series of four GPU optimizations to enable Stable Diffusion 1.4 to run on a Samsung phone and generate images in under 12 seconds. RAM usage was also reduced heavily.
* **Their breakthrough isn't device-specific; rather it's a generalized approach that can add improvements to all latent diffusion models.** Overall image generation time decreased by 52% and 33% on a Samsung S23 Ultra and an iPhone 14 Pro, respectively.
*  Running generative AI locally on a phone, without a data connection or a cloud server, opens up a host of possibilities. This is just an example of how rapidly this space is moving as Stable Diffusion only just released last fall, and in its initial versions was slow to run on a hefty RTX 3080 desktop GPU.

&#x200B;

As small form-factor devices can run their own generative AI models, what does that mean for the future of computing? Some very exciting applications could be possible.

&#x200B;

If you're curious, the paper (very technical) [can be accessed here.](https://arxiv.org/abs/2304.11267)",782,69,d google researchers achieve performance breakthrough rendering stable diffusion images in sub seconds on a mobile phone generative ai models running on your mobile phone is nearing reality,0.296
Google Brain will be doing an AMA in /r/MachineLearning on August 11,"Happy to announce the [Google Brain](https://research.google.com/teams/brain/) team will be making a visit to /r/MachineLearning to do an AMA on August 11.

A thread will be created before the official AMA time for those who won't be able to attend on that day.",773,64,google brain will be doing an ama in rmachinelearning on august ,0.0
[Discussion] When ML and Data Science are the death of a good company: A cautionary tale.,"TD;LR: At Company A, Team X does advanced analytics using on-prem ERP tools and older programming languages. Their tools work very well and are designed based on very deep business and domain expertise. Team Y is a new and ambitious Data Science team that thinks they can replace Team X's tools with a bunch of R scripts and a custom built ML platform. Their models are simplistic, but more ""fashionable"" compared to the econometric models used by Team X, and team Y benefits from the ML/DS moniker so leadership is allowing Team Y to start a large scale overhaul of the analytics platform in question. Team Y doesn't have the experience for such a larger scale transformation, and is refusing to collaborate with team X. This project is very likely going to fail, and cause serious harm to the company as a whole financially and from a people perspective. I argue that this is not just because of bad leadership, but also because of various trends and mindsets in the DS community at large. 

---------------------------------------------------------------------------------------------
Update (Jump to below the line for the original story): 

Several people in the comments are pointing out that this just a management failure, not something due to ML/DS, and that you can replace DS with any buzz tech and the story will still be relevant. 

My response: 
Of course, any failure at an organization level is ultimately a management failure one way or the other. 
Moreover, it is also the case that ML/DS when done correctly, will always improve a company's bottom line. There is no scenario where the proper ML solution, delivered at a reasonable cost and in a timely fashion, will somehow hurt the company's bottom line.

My point is that in this case management is failing because of certain trends and practices that are specific to the ML/DS community, namely: 
* The idea that DS teams should operate independently of tech and business orgs -- too much autonomy for DS teams 
* The disregard for domain knowledge that seems prevalent nowadays  thanks to the ML hype, that DS can be generalists and someone with good enough ML chops can solve any business problem.  That wasn't the case when I first left academia for the industry in 2009  (back then nobody would even bother with a phone screen if you didn't have the right domain knowledge). 
* Over reliance on resources who check all the ML hype related boxes (knows Python, R, Tensorflow, Shiny, etc..., has the right Coursera certifications, has blogged on the topic, etc...), but are lacking in depth of  experience. DS interviews nowadays all seem to be: Can you tell me what a p-value is? What is elastic net regression? Show me how to fit a model in sklearn? How do you impute NAs in an R dataframe? Any smart person can look those up on Stackoverflow or Cross-Validated,.....Instead teams should be asking stuff like: why does portfolio optimization use QP not LP? How does a forecast influence a customer service level? When should a recommendation engine be content based and when should it use collaborative filtering? etc...

---------------------------------------------------------------------------------------------

*(This is a true story, happening to the company I currently work for. Names, domains, algorithms, and roles have been shuffled around to protect my anonymity)* 

Company A has been around for several decades. It is not the biggest name in its domain, but it is a well respected one. Risk analysis and portfolio optimization have been a core of Company A's business since the 90s. They have a large team of 30 or so analysts who perform those tasks on a daily basis. These analysts use ERP solutions implemented for them by one the big ERP companies (SAP, Teradata, Oracle, JD Edwards,...) or one of the major tech consulting companies (Deloitte, Accenture, PWC, Capgemini, etc...) in collaboration with their own in house engineering team. The tools used are embarrassingly old school: Classic RDBMS running on on-prem servers or maybe even on mainframes, code written in COBOL, Fortran, weird proprietary stuff like ABAP or SPSS.....you get the picture. But the models and analytic functions were pretty sophisticated, and surprisingly cutting edge compared to the published academic literature. Most of all, they fit well with the company's enterprise ecosystem, and were honed based on years of deep domain knowledge. 

They have a tech team of several engineers (poached from the aforementioned software and consulting companies) and product managers (who came from the experienced pools of analysts and managers who use the software, or poached from business rivals) maintaining and running this software. Their technology might be old school, but collectively, they know the domain and the company's overall architecture very, very well. They've guided the company through several large scale upgrades and migrations and they have a track record of delivering on time, without too much overhead. The few times they've stumbled, they knew how to pick themselves up very quickly. In fact within their industry niche, they have a reputation for their expertise, and have very good relations with the various vendors they've had to deal with. They were the launching pad of several successful ERP consulting careers. 

Interestingly, despite dealing on a daily basis with statistical modeling and optimization algorithms, none of the analysts, engineers, or product managers involved describe themselves as data scientists or machine learning experts. It is mostly a cultural thing: Their expertise predates the Data Science/ML hype that started circa 2010, and they got most of their chops using proprietary enterprise tools instead of the open source tools popular nowadays. A few of them have formal statistical training, but most of them came from engineering or domain backgrounds and learned stats on the fly while doing their job. Call this team ""Team X"". 

Sometime around the mid 2010s, Company A started having some serious anxiety issues: Although still doing very well for a company its size, overall economic and demographic trends were shrinking its customer base, and a couple of so called disruptors came up with a new app and business model that started seriously eating into their revenue. A suitable reaction to appease shareholders and Wall Street was necessary. The company already had a decent website and a pretty snazzy app, what more could be done? Leadership decided that it was high time that AI and ML become a core part of the company's business. An ambitious Manager, with no science or engineering background, but who had very briefly toyed with a recommender system a couple of years back, was chosen to build a data science team, call it team ""Y"" (he had a bachelor's in history from the local state college and worked for several years in the company's marketing org). Team ""Y"" consists mostly of internal hires who decided they wanted to be data scientists and completed a Coursera certification or a Galvanize boot camp, before being brought on to the team, along with a few of fresh Ph.D or M.Sc holders who didn't like academia and wanted to try their hand at an industry role. All of them were very bright people, they could write great Medium blog posts and give inspiring TED talks, but collectively they had very little real world industry experience. 

As is the fashion nowadays, this group was made part of a data science org that reported directly to the CEO and Board, bypassing the CIO and any tech or business VPs, since Company A wanted to claim the monikers ""data driven"" and ""AI powered"" in their upcoming shareholder meetings. In 3 or 4 years of existence, team Y produced a few Python and R scripts. Their architectural experience  consisted almost entirely in connecting Flask to S3 buckets or Redshift tables, with a couple of the more resourceful ones learning how to plug their models into Tableau or how to spin up a Kuberneties pod.  But they needn't worry: The aforementioned manager, who was now a director (and was also doing an online Masters to make up for his qualifications gap and bolster his chances of becoming VP soon - at least he now understands what L1 regularization is), was a master at playing corporate politics and self-promotion. No matter how few actionable insights team Y produced or how little code they deployed to production, he always had their back and made sure they had ample funding. In fact he now had grandiose plans for setting up an all-purpose machine learning platform that can be used to solve all of the company's data problems. 

A couple of sharp minded members of team Y, upon googling their industry name along with the word ""data science"", realized that risk analysis was a prime candidate for being solved with Bayesian models, and there was already a nifty R package for doing just that, whose tutorial they went through on R-Bloggers.com. One of them had even submitted a Bayesian classifier Kernel for a competition on Kaggle (he was 203rd on the leaderboard), and was eager to put his new-found expertise to use on a real world problem. They pitched the idea to their director, who saw a perfect use case for his upcoming ML platform. They started work on it immediately, without bothering to check whether anybody at Company A was already doing risk analysis. Since their org was independent, they didn't really need to check with anybody else before they got funding for their initiative. Although it was basically a Naive Bayes classifier, the term ML was added to the project tile, to impress the board. 

As they progressed with their work however, tensions started to build. They had asked the data warehousing and CA analytics teams to build pipelines for them, and word eventually got out to team X about their project. Team X was initially thrilled: They offered to collaborate whole heartedly, and would have loved to add an ML based feather to their already impressive cap. The product owners and analysts were totally onboard as well: They saw a chance to get in on the whole Data Science hype that they kept hearing about. But through some weird mix of arrogance and insecurity, team Y refused to collaborate with them or share any of their long term goals with them, even as they went to other parts of the company giving brown bag presentations and tutorials on the new model they created. 

Team X got resentful: from what they saw of team Y's model, their approach was hopelessly naive and had little chances of scaling or being sustainable in production, and they knew exactly how to help with that. Deploying the model to production would have taken them a few days, given how comfortable they were with DevOps and continuous delivery (team Y had taken several months to figure out how to deploy a simple R script to production). And despite how old school their own tech was, team X were crafty enough to be able to plug it in to their existing architecture. Moreover, the output of the model was such that it didn't take into account how the business will consume it or how it was going to be fed to downstream systems, and the product owners could have gone a long way in making the model more amenable to adoption by the business stakeholders. But team Y wouldn't listen, and their leads brushed off any attempts at communication, let alone collaboration. The vibe that team Y was giving off was ""We are the cutting edge ML team, you guys are the legacy server grunts. We don't need your opinion."", and they seemed to have a complete disregard for domain knowledge, or worse, they thought that all that domain knowledge consisted of was being able to grasp the definitions of a few business metrics. 

Team X got frustrated and tried to express their concerns to leadership. But despite owning a vital link in Company A's business process, they were only \~50 people in a large 1000 strong technology and operations org, and they were several layers removed from the C-suite, so it was impossible for them to get their voices heard. 

Meanwhile, the unstoppable director was doing what he did best: Playing corporate politics. Despite how little his team had actually delivered, he had convinced the board that all analysis and optimization tasks should now be migrated to his yet to be delivered ML platform. Since most leaders now knew that there was overlap between team Y and team X's objectives, his pitch was no longer that team Y was going to create a new insight, but that they were going to replace (or modernize) the legacy statistics based on-prem tools with more accurate cloud based ML tools. Never mind that there was no support in the academic literature for the idea that Naive Bayes works better than the Econometric approaches used by team X, let alone the additional wacky idea that Bayesian Optimization would definitely outperform the QP solvers that were running in production. 

Unbeknownst to team X, the original Bayesian risk analysis project has now grown into a multimillion dollar major overhaul initiative, which included the eventual replacement of all of the tools and functions supported by team X along with the necessary migration to the cloud. The CIO and a couple of business VPs are on now board, and tech leadership is treating it as a done deal.

An outside vendor, a startup who nobody had heard of, was contracted to help build the platform, since team Y has no engineering skills. The choice was deliberate, as calling on any of the established consulting or software companies would have eventually led leadership to the conclusion that team X was better suited for a transformation on this scale than team Y. 

Team Y has no experience with any major ERP deployments, and no domain knowledge, yet they are being tasked with fundamentally changing the business process that is at the core of Company A's business. Their models actually perform worse than those deployed by team X, and their architecture is hopelessly simplistic, compared to what is necessary for running such a solution in production. 

Ironically, using Bayesian thinking and based on all the evidence, the likelihood that team Y succeeds is close to 0%. 

At best, the project is going to end up being a write off of 50 million dollars or more. Once the !@#$!@# hits the fan, a couple of executive heads are going to role, and dozens of people will get laid off.

At worst, given how vital risk analysis and portfolio optimization is to Company A's revenue stream, the failure will eventually sink the whole company. It probably won't go bankrupt, but it will lose a significant portion of its business and work force. Failed ERP implementations can and do sink large companies: Just see what happened to National Grid US, SuperValu or Target Canada. 

One might argue that this is more about corporate disfunction and bad leadership than about data science and AI. 

But I disagree. I think the core driver of this debacle is indeed the blind faith in Data Scientists, ML models and the promise of AI, and the overall culture of hype and self promotion that is very common among the ML crowd. 

We haven't seen the end of this story: I sincerely hope that this ends well for the sake of my colleagues and all involved. Company A is a good company, and both its customers and its employees deserver better. But the chances of that happening are negligible given all the information available, and this failure will hit my company hard. ",777,198,discussion when ml and data science are the death of a good company a cautionary tale,-0.25
[R] How Youtube is recommending your next video,Recently I came across a paper of Google that was describing how their recommendation algorithm works for Youtube. I wrote my own summary and key takeaways down. Check it out my paper review [here](https://medium.com/vantageai/how-youtube-is-recommending-your-next-video-7e5f1a6bd6d9).,770,45,r how youtube is recommending your next video,0.0
[D] Here are 17 ways of making PyTorch training faster – what did I miss?,"[I've been collecting methods to accelerate training in PyTorch](https://efficientdl.com/faster-deep-learning-in-pytorch-a-guide/) – here's what I've found so far. What did I miss? What did I get wrong?

The methods – roughly sorted from largest to smallest expected speed-up – are:

1. Consider using a different learning rate schedule.
2. Use multiple workers and pinned memory in DataLoader.
3. Max out the batch size.
4. Use Automatic Mixed Precision (AMP).
5. Consider using a different optimizer.
6. Turn on cudNN benchmarking.
7. Beware of frequently transferring data between CPUs and GPUs.
8. Use gradient/activation checkpointing.
9. Use gradient accumulation.
10. Use DistributedDataParallel for multi-GPU training.
11. Set gradients to None rather than 0.
12. Use .as\_tensor rather than .tensor()
13. Turn off debugging APIs if not needed.
14. Use gradient clipping.
15. Turn off bias before BatchNorm.
16. Turn off gradient computation during validation.
17. Use input and batch normalization.

## 1. Consider using another learning rate schedule

The learning rate (schedule) you choose has a large impact on the speed of convergence as well as the generalization performance of your model.

Cyclical Learning Rates and the 1Cycle learning rate schedule are both methods introduced by Leslie N. Smith ([here](https://arxiv.org/pdf/1506.01186.pdf) and [here](https://arxiv.org/abs/1708.07120)), and then popularised by fast.ai's Jeremy Howard and Sylvain Gugger ([here](https://www.fast.ai/2018/07/02/adam-weight-decay/) and [here](https://github.com/sgugger/Deep-Learning/blob/master/Cyclical%20LR%20and%20momentums.ipynb)). Essentially, the 1Cycle learning rate schedule looks something like this:

&#x200B;

https://preview.redd.it/sc37u5knmxa61.png?width=476&format=png&auto=webp&s=09b309b4dbd67eedb4ab5f86e03e0e83d7b072d1

Sylvain writes:

>\[1cycle consists of\]  two steps of equal lengths, one going from a lower learning rate to a higher one than go back to the minimum. The maximum should be the value picked with the Learning Rate Finder, and the lower one can be ten times lower. Then, the length of this cycle should be slightly less than the total number of epochs, and, in the last part of training, we should allow the learning rate to decrease more than the minimum, by several orders of magnitude.

In the best case this schedule achieves a massive speed-up – what Smith calls *Superconvergence* – as compared to conventional learning rate schedules. Using the 1Cycle policy he needs \~10x fewer training iterations of a ResNet-56 on ImageNet to match the performance of the original paper, for instance). The schedule seems to perform robustly well across common architectures and optimizers.

PyTorch implements both of these methods `torch.optim.lr_scheduler.CyclicLR` and `torch.optim.lr_scheduler.OneCycleLR,` see [the documentation](https://pytorch.org/docs/stable/optim.html).

One drawback of these schedulers is that they introduce a number of additional hyperparameters. [This post](https://towardsdatascience.com/hyper-parameter-tuning-techniques-in-deep-learning-4dad592c63c8) and [this repo](https://github.com/davidtvs/pytorch-lr-finder), offer a nice overview and implementation of how good hyper-parameters can be found including the Learning Rate Finder mentioned above.

Why does this work? It doesn't seem entirely clear but one[ possible explanation](https://arxiv.org/pdf/1506.01186.pdf) might be that regularly increasing the learning rate helps to traverse [saddle points in the loss landscape ](https://papers.nips.cc/paper/2015/file/430c3626b879b4005d41b8a46172e0c0-Paper.pdf)more quickly.

## 2. Use multiple workers and pinned memory in DataLoader

When using [torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader), set `num_workers > 0`, rather than the default value of 0, and `pin_memory=True`, rather than the default value of False. Details of this are [explained here](https://pytorch.org/docs/stable/data.html).

[Szymon Micacz](https://nvlabs.github.io/eccv2020-mixed-precision-tutorial/files/szymon_migacz-pytorch-performance-tuning-guide.pdf) achieves a 2x speed-up for a single training epoch by using four workers and pinned memory.

A rule of thumb that [people are using ](https://discuss.pytorch.org/t/guidelines-for-assigning-num-workers-to-dataloader/813/5)to choose the number of workers is to set it to four times the number of available GPUs with both a larger and smaller number of workers leading to a slow down.

Note that increasing num\_workerswill increase your CPU memory consumption.

## 3. Max out the batch size

This is a somewhat contentious point. Generally, however, it seems like using the largest batch size your GPU memory permits will accelerate your training (see [NVIDIA's Szymon Migacz](https://nvlabs.github.io/eccv2020-mixed-precision-tutorial/files/szymon_migacz-pytorch-performance-tuning-guide.pdf), for instance). Note that you will also have to adjust other hyperparameters, such as the learning rate, if you modify the batch size. A rule of thumb here is to double the learning rate as you double the batch size.

[OpenAI has a nice empirical paper](https://arxiv.org/pdf/1812.06162.pdf) on the number of convergence steps needed for different batch sizes. [Daniel Huynh](https://towardsdatascience.com/implementing-a-batch-size-finder-in-fastai-how-to-get-a-4x-speedup-with-better-generalization-813d686f6bdf) runs some experiments with different batch sizes (also using the 1Cycle policy discussed above) where he achieves a 4x speed-up by going from batch size 64 to 512.

[One of the downsides](https://arxiv.org/pdf/1609.04836.pdf) of using large batch sizes, however, is that they might lead to solutions that generalize worse than those trained with smaller batches.

## 4. Use Automatic Mixed Precision (AMP)

The release of PyTorch 1.6 included a native implementation of Automatic Mixed Precision training to PyTorch. The main idea here is that certain operations can be run faster and without a loss of accuracy at semi-precision (FP16) rather than in the single-precision (FP32) used elsewhere. AMP, then, automatically decide which operation should be executed in which format. This allows both for faster training and a smaller memory footprint.

In the best case, the usage of AMP would look something like this:

    import torch
    # Creates once at the beginning of training
    scaler = torch.cuda.amp.GradScaler()
    
    for data, label in data_iter:
       optimizer.zero_grad()
       # Casts operations to mixed precision
       with torch.cuda.amp.autocast():
          loss = model(data)
    
       # Scales the loss, and calls backward()
       # to create scaled gradients
       scaler.scale(loss).backward()
    
       # Unscales gradients and calls
       # or skips optimizer.step()
       scaler.step(optimizer)
    
       # Updates the scale for next iteration
       scaler.update()

Benchmarking a number of common language and vision models on NVIDIA V100 GPUs, [Huang and colleagues find](https://pytorch.org/blog/accelerating-training-on-nvidia-gpus-with-pytorch-automatic-mixed-precision/) that using AMP over regular FP32 training yields roughly 2x – but upto 5.5x – training speed-ups.

Currently, only CUDA ops can be autocast in this way. See the [documentation](https://pytorch.org/docs/stable/amp.html#op-eligibility) here for more details on this and other limitations.

u/SVPERBlA points out that you can squeeze out some additional performance (\~ 20%) from AMP on NVIDIA Tensor Core GPUs if you convert your tensors to the [Channels Last memory format](https://pytorch.org/tutorials/intermediate/memory_format_tutorial.html). Refer to [this section](https://docs.nvidia.com/deeplearning/performance/dl-performance-convolutional/index.html#tensor-layout) in the NVIDIA docs for an explanation of the speedup and more about NCHW versus NHWC tensor formats.

## 5. Consider using another optimizer

AdamW is Adam with weight decay (rather than L2-regularization) which was popularized by fast.ai and is now available natively in PyTorch as `torch.optim.AdamW`. AdamW seems to consistently outperform Adam in terms of both the error achieved and the training time. See [this excellent blog](https://www.fast.ai/2018/07/02/adam-weight-decay/) post on why using weight decay instead of L2-regularization makes a difference for Adam.

Both Adam and AdamW work well with the 1Cycle policy described above.

There are also a few not-yet-native optimizers that have received a lot of attention recently, most notably LARS ([pip installable implementation](https://github.com/kakaobrain/torchlars)) and [LAMB](https://github.com/cybertronai/pytorch-lamb).

NVIDA's APEX implements fused versions of a number of common optimizers such as [Adam](https://nvidia.github.io/apex/optimizers.html). This implementation avoid a number of passes to and from GPU memory as compared to the PyTorch implementation of Adam, yielding speed-ups in the range of 5%.

## 6. Turn on cudNN benchmarking

If your model architecture remains fixed and your input size stays constant, setting `torch.backends.cudnn.benchmark = True` might be beneficial ([docs](https://pytorch.org/docs/stable/backends.html#torch-backends-cudnn)). This enables the cudNN autotuner which will benchmark a number of different ways of computing convolutions in cudNN and then use the fastest method from then on.

For a rough reference on the type of speed-up you can expect from this, [Szymon Migacz](https://nvlabs.github.io/eccv2020-mixed-precision-tutorial/files/szymon_migacz-pytorch-performance-tuning-guide.pdf) achieves a speed-up of 70% on a forward pass for a convolution and a 27% speed-up for a forward + backward pass of the same convolution.

One caveat here is that this autotuning might become very slow if you max out the batch size as mentioned above.

## 7. Beware of frequently transferring data between CPUs and GPUs

Beware of frequently transferring tensors from a GPU to a CPU using `tensor.cpu()` and vice versa using `tensor.cuda()` as these are relatively expensive. The same applies for `.item()` and `.numpy()` – use `.detach()` instead.

If you are creating a new tensor, you can also directly assign it to your GPU using the keyword argument `device=torch.device('cuda:0')`.

If you do need to transfer data, using `.to(non_blocking=True)`, might be useful [as long as you don't have any synchronization points](https://discuss.pytorch.org/t/should-we-set-non-blocking-to-true/38234/4) after the transfer.

If you really have to, you might want to give Santosh Gupta's [SpeedTorch](https://github.com/Santosh-Gupta/SpeedTorch) a try, although it doesn't seem entirely clear when this actually does/doesn't provide speed-ups.

## 8. Use gradient/activation checkpointing

Quoting directly from the [documentation](https://pytorch.org/docs/stable/checkpoint.html):

>Checkpointing works by trading compute for memory. Rather than storing all intermediate activations of the entire computation graph for computing backward, the checkpointed part does **not** save intermediate activations, and instead recomputes them in backward pass. It can be applied on any part of a model.  
>  
>Specifically, in the forward pass, function will run in [torch.no\_grad()](https://pytorch.org/docs/stable/generated/torch.no_grad.html#torch.no_grad) manner, i.e., not storing the intermediate activations. Instead, the forward pass saves the inputs tuple and the functionparameter. In the backwards pass, the saved inputs and function is retrieved, and the forward pass is computed on function again, now tracking the intermediate activations, and then the gradients are calculated using these activation values.

So while this will might slightly increase your run time for a given batch size, you'll significantly reduce your memory footprint. This in turn will allow you to further increase the batch size you're using allowing for better GPU utilization.

While checkpointing is implemented natively as `torch.utils.checkpoint`([docs](https://pytorch.org/docs/stable/checkpoint.html)), it does seem to take some thought and effort to implement properly. Priya Goyal [has a good tutorial ](https://github.com/prigoyal/pytorch_memonger/blob/master/tutorial/Checkpointing_for_PyTorch_models.ipynb)demonstrating some of the key aspects of checkpointing.

## 9. Use gradient accumulation

Another approach to increasing the batch size is to accumulate gradients across multiple `.backward()` passes before calling optimizer.step().

Following [a post](https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255) by Hugging Face's Thomas Wolf, gradient accumulation can be implemented as follows:

    model.zero_grad()                                   # Reset gradients tensors
    for i, (inputs, labels) in enumerate(training_set):
        predictions = model(inputs)                     # Forward pass
        loss = loss_function(predictions, labels)       # Compute loss function
        loss = loss / accumulation_steps                # Normalize our loss (if averaged)
        loss.backward()                                 # Backward pass
        if (i+1) % accumulation_steps == 0:             # Wait for several backward steps
            optimizer.step()                            # Now we can do an optimizer step
            model.zero_grad()                           # Reset gradients tensors
            if (i+1) % evaluation_steps == 0:           # Evaluate the model when we...
                evaluate_model()                        # ...have no gradients accumulate

This method was developed mainly to circumvent GPU memory limitations and I'm not entirely clear on the trade-off between having additional `.backward()` loops. [This discussion](https://forums.fast.ai/t/accumulating-gradients/33219/28) on the fastai forum seems to suggest that it can in fact accelerate training, so it's probably worth a try.

## 10. Use Distributed Data Parallel for multi-GPU training

Methods to accelerate distributed training probably warrant their own post but one simple one is to use `torch.nn.DistributedDataParallel` rather than `torch.nn.DataParallel`. By doing so, each GPU will be driven by a dedicated CPU core avoiding the GIL issues of DataParallel.

In general, I can strongly recommend reading the [documentation on distributed training.](https://pytorch.org/tutorials/beginner/dist_overview.html)

## 11. Set gradients to None rather than 0

Use `.zero_grad(set_to_none=True)` rather than `.zero_grad()`.

Doing so will let the memory allocator handle the gradients rather than actively setting them to 0. This will lead to yield a *modest* speed-up as they say in the [documentation](https://pytorch.org/docs/stable/optim.html), so don't expect any miracles.

Watch out, doing this is not side-effect free! Check the docs for the details on this.

## 12. Use .as_tensor() rather than .tensor()

`torch.tensor()` always copies data. If you have a numpy array that you want to convert, use `torch.as_tensor()` or `torch.from_numpy()` to avoid copying the data.

## 13. Turn on debugging tools only when actually needed

PyTorch offers a number of useful debugging tools like the [autograd.profiler](https://pytorch.org/docs/stable/autograd.html#profiler), [autograd.grad\_check](https://pytorch.org/docs/stable/autograd.html#numerical-gradient-checking), and [autograd.anomaly\_detection](https://pytorch.org/docs/stable/autograd.html#anomaly-detection). Make sure to use them to better understand when needed but to also turn them off when you don't need them as they will slow down your training.

## 14. Use gradient clipping

Originally used to avoid exploding gradients in RNNs, there is both some [empirical evidence as well as some theoretical support](https://openreview.net/forum?id=BJgnXpVYwS) that clipping gradients (roughly speaking: `gradient = min(gradient, threshold)`) accelerates convergence.

Hugging Face's [Transformer implementation](https://github.com/huggingface/transformers/blob/7729ef738161a0a182b172fcb7c351f6d2b9c50d/examples/run_squad.py#L156) is a really clean example of how to use gradient clipping as well as some of the other methods such as AMP mentioned in this post.

In PyTorch this can be done using `torch.nn.utils.clip_grad_norm_`([documentation](https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html#torch.nn.utils.clip_grad_norm_)).

It's not entirely clear to me which models benefit how much from gradient clipping but it seems to be robustly useful for RNNs, Transformer-based and ResNets architectures and a range of different optimizers.

## 15. Turn off bias before BatchNorm

This is a very simple one: turn off the bias of layers before BatchNormalization layers. For a 2-D convolutional layer, this can be done by setting the bias keyword to False: `torch.nn.Conv2d(..., bias=False, ...)`.  (Here's a r[eminder why this makes sense](https://stackoverflow.com/questions/46256747/can-not-use-both-bias-and-batch-normalization-in-convolution-layers).)

You will save some parameters, I would however expect the speed-up of this to be relatively small as compared to some of the other methods mentioned here.

## 16. Turn off gradient computation during validation

This one is straightforward: set `torch.no_grad()` during validation.

## 17. Use input and batch normalization

You're probably already doing this but you might want to double-check:

* Are you [normalizing](https://pytorch.org/docs/stable/torchvision/transforms.html) your input?
* Are you using [batch-normalization](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html)?

And [here's](https://stats.stackexchange.com/questions/437840/in-machine-learning-how-does-normalization-help-in-convergence-of-gradient-desc) a reminder of why you probably should.

### Bonus tip from the comments: Use JIT to fuse point-wise operations.

If you have adjacent point-wise operations you can use [PyTorch JIT](https://pytorch.org/docs/stable/jit.html#creating-torchscript-code) to combine them into one FusionGroup which can then be launched on a single kernel rather than multiple kernels as would have been done per default. You'll also save some memory reads and writes.

[Szymon Migacz shows](https://nvlabs.github.io/eccv2020-mixed-precision-tutorial/files/szymon_migacz-pytorch-performance-tuning-guide.pdf) how you can use the `@torch.jit.script` decorator to fuse the operations in a GELU, for instance:

    @torch.jit.script
    def fused_gelu(x):
        return x * 0.5 * (1.0 + torch.erf(x / 1.41421))

In this case, fusing the operations leads to a 5x speed-up for the execution of `fused_gelu`  
as compared to the unfused version.

See also [this post](https://pytorch.org/blog/optimizing-cuda-rnn-with-torchscript/) for an example of how Torchscript can be used to accelerate an RNN.

Hat tip to u/Patient_Atmosphere45 for the suggestion.

## Sources and additional resources

Many of the tips listed above come from Szymon Migacz' [talk](https://www.youtube.com/watch?v=9mS1fIYj1So) and post in the [PyTorch docs](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html).

PyTorch Lightning's William Falcon has [two](https://towardsdatascience.com/9-tips-for-training-lightning-fast-neural-networks-in-pytorch-8e63a502f565) [interesting](https://towardsdatascience.com/7-tips-for-squeezing-maximum-performance-from-pytorch-ca4a40951259) posts with tips to speed-up training. [PyTorch Lightning](https://github.com/PyTorchLightning/pytorch-lightning) does already take care of some of the points above per-default.

Thomas Wolf at Hugging Face has a [number](https://medium.com/@Thomwolf) of interesting articles on accelerating deep learning – with a particular focus on language models.

The same goes for [Sylvain Gugger](https://sgugger.github.io/category/basics.html) and [Jeremy Howard](https://www.youtube.com/watch?v=LqGTFqPEXWs): they have many interesting posts in particular on [learning](https://sgugger.github.io/the-1cycle-policy.html) [rates](https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html) and [AdamW](https://www.fast.ai/2018/07/02/adam-weight-decay/).

*Thanks to Ben Hahn, Kevin Klein and Robin Vaaler for their feedback on a draft of this post!*

**I've also put all of the above into this** [**blog post**](https://efficientdl.com/faster-deep-learning-in-pytorch-a-guide/)**.**",764,38,d here are  ways of making pytorch training faster  what did i miss,-0.1531
[D] Is anyone frankly getting a little tired of seeing these covid19 diagnosis models on their linkedin?,"I am a little concerned by the sheer number of posts just like this, claiming to achieve 100%/near 100% accuracy on small datasets using a pre-trained resnet50. The traction and accolades they get is astounding. Any way to effectively call people out on these? Am I being salty? I get we all want to help, but these are muddying the waters of actual research, which is far more complicated and more worthwhile.

Edit: not to even mention the gall of using the ongoing pandemic for likes and branding because it 'sells'",764,119,d is anyone frankly getting a little tired of seeing these covid diagnosis models on their linkedin,-0.3832
[D] Misuse of Deep Learning in Nature Journal’s Earthquake Aftershock Paper,"*Recently, I saw a [post](https://towardsdatascience.com/stand-up-for-best-practices-8a8433d3e0e8) by [Rajiv Shah](https://twitter.com/rajcs4), Chicago-based data-scientist, regarding an article published in Nature last year called [Deep learning of aftershock patterns following large earthquakes](https://www.nature.com/articles/s41586-018-0438-y), written by scientists at Harvard in collaboration with Google. Below is the article:*

**Stand Up for Best Practices:
Misuse of Deep Learning in Nature’s Earthquake Aftershock Paper**

**The Dangers of Machine Learning Hype**

Practitioners of AI, machine learning, predictive modeling, and data science have grown enormously over the last few years. What was once a niche field defined by its blend of knowledge is becoming a rapidly growing profession. As the excitement around AI continues to grow, the new wave of ML augmentation, automation, and GUI tools will lead to even more growth in the number of people trying to build predictive models.

But here’s the rub: While it becomes easier to use the tools of predictive modeling, predictive modeling knowledge is not yet a widespread commodity. Errors can be counterintuitive and subtle, and they can easily lead you to the wrong conclusions if you’re not careful.

I’m a data scientist who works with dozens of expert data science teams for a living. In my day job, I see these teams striving to build high-quality models. The best teams work together to review their models to detect problems. There are many hard-to-detect-ways that lead to problematic models (say, by allowing target leakage into their training data).

Identifying issues is not fun. This requires admitting that exciting results are “too good to be true” or that their methods were not the right approach. In other words, *it’s less about the sexy data science hype that gets headlines and more about a rigorous scientific discipline.*

**Bad Methods Create Bad Results**

Almost a year ago, I read an [article](https://www.nature.com/articles/s41586-018-0438-y) in Nature that claimed unprecedented accuracy in predicting earthquake aftershocks by using deep learning. Reading the article, my internal radar became deeply suspicious of their results. *Their methods simply didn’t carry many of the hallmarks of careful predicting modeling.*

I started to dig deeper. In the meantime, this article blew up and became [widely recognized](https://blog.google/technology/ai/forecasting-earthquake-aftershock-locations-ai-assisted-science/)! It was even included in the [release notes](https://medium.com/tensorflow/whats-coming-in-tensorflow-2-0-d3663832e9b8) for Tensorflow as an example of what deep learning could do. However, in my digging, I found major flaws in the paper. Namely, data leakage which leads to unrealistic accuracy scores and a lack of attention to model selection (you don’t build a 6 layer neural network when a simpler model provides the same level of accuracy).

To my earlier point: these are subtle, but *incredibly basic* predictive modeling errors that can invalidate the entire results of an experiment. Data scientists are trained to recognize and avoid these issues in their work. I assumed that this was simply overlooked by the author, so I contacted her and let her know so that she could improve her analysis. Although we had previously communicated, she did not respond to my email over concerns with the paper.

**Falling On Deaf Ears**

So, what was I to do? My coworkers told me to just [tweet](https://twitter.com/rajcs4/status/1143236424738775046) [it](https://twitter.com/DataScienceLA/status/1143245342785228800) and let it go, but I wanted to stand up for good modeling practices. I thought reason and best practices would prevail, so I started a 6-month process of writing up my results and shared them with Nature.
Upon sharing my results, I received a note from Nature in January 2019 that despite serious concerns about data leakage and model selection that invalidate their experiment, they saw no need to correct the errors, because “**Devries et al. are concerned primarily with using machine learning as [a] tool to extract insight into the natural world, and not with details of the algorithm design**.” The authors provided a much [harsher](https://github.com/rajshah4/aftershocks_issues/blob/master/correspondence/Authors_DeVries_Response.pdf) response.

You can read the entire exchange on my [github](https://github.com/rajshah4/aftershocks_issues).

It’s not enough to say that I was disappointed. This was a major paper (it’s **Nature**!) that bought into AI hype and published a paper despite it using flawed methods.

Then, just this week, I ran [across](https://link.springer.com/chapter/10.1007/978-3-030-20521-8_1) [articles](https://arxiv.org/abs/1904.01983) by Arnaud Mignan and Marco Broccardo on shortcomings that they found in the aftershocks article. Here are two more data scientists with expertise in earthquake analysis who also noticed flaws in the paper. I also have placed my analysis and reproducible code on [github](https://github.com/rajshah4/aftershocks_issues).

**Standing Up For Predictive Modeling Methods**

I want to make it clear: my goal is not to villainize the authors of the aftershocks paper. I don’t believe that they were malicious, and I think that they would argue their goal was to just show how machine learning could be applied to aftershocks. Devries is an accomplished earthquake scientist who wanted to use the latest methods for her field of study and found exciting results from it.

But here’s the problem: their insights and results were based on fundamentally flawed methods. It’s not enough to say, “This isn’t a machine learning paper, it’s an earthquake paper.” If you use predictive modeling, then the quality of your results are determined by the quality of your modeling. Your work becomes data science work, and you are on the hook for your scientific rigor.

There is a huge appetite for papers that use the latest technologies and approaches. It becomes very difficult to push back on these papers.

But if we allow papers or projects with fundamental issues to advance, it hurts all of us. It undermines the field of predictive modeling.

Please push back on bad data science. Report bad findings to papers. And if they don’t take action, go to twitter, post about it, share your results and make noise. This type of collective action worked to raise awareness of p-values and combat the epidemic of p-hacking. We need good machine learning practices if we want our field to continue to grow and maintain credibility.

[Link to Rajiv's Article](https://towardsdatascience.com/stand-up-for-best-practices-8a8433d3e0e8)

[Original Nature Publication](https://www.nature.com/articles/s41586-018-0438-y) (note: paywalled)

[GitHub repo contains an attempt to reproduce Nature's paper](https://github.com/rajshah4/aftershocks_issues)

[Confrontational correspondence with authors](https://github.com/rajshah4/aftershocks_issues/blob/master/correspondence/Authors_DeVries_Response.pdf)",774,135,d misuse of deep learning in nature journals earthquake aftershock paper,0.0
[D] I'm so sick of the hype,"Sorry if this is not a constructive post, its more of a rant really. I'm just so sick of the hype in this field, I want to feel like I'm doing engineering work/proper science but I'm constantly met with buzz words and ""business-y"" type language. I was browsing and I saw the announcement for the Tensorflow World conference happening now, and I went on the website and was again met with ""Be part of the ML revolution."" in big bold letters. Like okay, I understand that businesses need to get investors, but for the past 2 years of being in this field I'm really starting to feel like I'm in marketing and not engineering. I'm not saying the products don't deliver or that there's miss-advertising, but there's just too much involvement of ""business type"" folks more so in this field compared to any other field of engineering and science... and I really hate this. It makes me wonder why is this the case? How come there's no towardschemicalengineering.com type of website? Is it because its really easy for anyone to enter this field and gain a superficial understanding of things? 

The issue I have with this is that I feel a constant pressure to frame whatever I'm doing with marketing lingo otherwise you immediately lose people's interest if you don't play along with the hype. 

Anyhow /rant

EDIT: Just wanted to thank everyone who commented as I can't reply to everyone but I read every comment so far and it has helped to make me realize that I need to adjust my perspective. I am excited for the future of ML no doubt.",758,309,d im so sick of the hype,-0.6418
[R] Single biological neuron can compute XOR,"We’ve known for a while that real neurons in the brain are more powerful than artificial neurons in neural networks. It takes a 2-layer ANN to compute XOR, which can apparently be done with a single real neuron, according to recent [paper](https://science.sciencemag.org/content/367/6473/83) published in Science.

[Dendritic action potentials and computation in human layer 2/3 cortical neurons](https://science.sciencemag.org/content/367/6473/83)",763,119,r single biological neuron can compute xor,0.0
[D] Has anyone else lost interest in ML research?,"I am a masters student and I have been doing ML research from a few years. I have a few top tier publications as well. Lately, I seem to have lost interest in research. I feel most of my collaborators (including my advisors) are mostly running after papers and don't seem to have interest in doing interesting off-the-track things. Ultimately, research has just become chasing one deadline after another. Another thing that bugs me is that most of the research (including mine) is not very useful. Even if I get some citations, I feel that it is highly unlikely that the work I am doing will ever be used by the general public. Earlier, I was very excited about PhD, but now I think it will be worthless pursuit. Is what I feel valid? How do I deal with these feelings and rejuvenate my interest in research? Or should I switch to something else - maybe applied ML?",764,156,d has anyone else lost interest in ml research,0.1779
[D] AMA: I left Google AI after 3 years.,"During the 3 years, I developed love-hate relationship of the place. Some of my coworkers and I left eventually for more applied ML job, and all of us felt way happier so far.

EDIT1 (6/13/2022, 4pm): I need to go to Cupertino now. I will keep replying this evening or tomorrow.

EDIT2 (6/16/2022 8am): Thanks everyone's support. Feel free to keep asking questions. I will reply during my free time on Reddit.",753,447,d ama i left google ai after  years,0.0
[D] Totally Open Alternatives to ChatGPT,"I have migrated this to GitHub for easy contribution: https://github.com/nichtdax/awesome-totally-open-chatgpt

By alternative, I mean projects feature different language model for chat system.
I do **not** count alternative **frontend** projects because they just call the API from OpenAI. 
I do **not** consider alternative **transformer decoder** to GPT 3.5 either because the training data of them are (mostly) not for chat system.

Tags:

-   B: bare (no data, no model's weight, no chat system)
-   F: full (yes data, yes model's weight, yes chat system including TUI and GUI)

| Project                                                                               | Description                                                                                                                                                                                                                                                                                                                                                                               | Tags |
| ------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---- |
| [lucidrains/PaLM-rlhf-pytorch](https://github.com/lucidrains/PaLM-rlhf-pytorch)       | Implementation of RLHF (Reinforcement Learning with Human Feedback) on top of the PaLM architecture. Basically ChatGPT but with PaLM                                                                                                                                                                                                                                                      | B    |
| [togethercomputer/OpenChatKit](https://github.com/togethercomputer/OpenChatKit)       | OpenChatKit provides a powerful, open-source base to create both specialized and general purpose chatbots for various applications. [Demo](https://huggingface.co/spaces/togethercomputer/OpenChatKit)                                                                                                                                                                                    | F    |
| [oobabooga/text-generation-webui](https://github.com/oobabooga/text-generation-webui) | A gradio web UI for running Large Language Models like GPT-J 6B, OPT, GALACTICA, LLaMA, and Pygmalion.                                                                                                                                                                                                                                                                                    | F    |
| [KoboldAI/KoboldAI-Client](https://github.com/KoboldAI/KoboldAI-Client)               | This is a browser-based front-end for AI-assisted writing with multiple local & remote AI models. It offers the standard array of tools, including Memory, Author's Note, World Info, Save & Load, adjustable AI settings, formatting options, and the ability to import existing AI Dungeon adventures. You can also turn on Adventure mode and play the game like AI Dungeon Unleashed. | F    |
| [LAION-AI/Open-Assistant/](https://github.com/LAION-AI/Open-Assistant/)               | OpenAssistant is a chat-based assistant that understands tasks, can interact with third-party systems, and retrieve information dynamically to do so.                                                                                                                                                                                                                                     | F    |",746,68,d totally open alternatives to chatgpt,0.0
"[N] Dolly 2.0, an open source, instruction-following LLM for research and commercial use","""Today, we’re releasing Dolly 2.0, the first open source, instruction-following LLM, fine-tuned on a human-generated instruction dataset licensed for research and commercial use"" - Databricks

https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm

Weights: https://huggingface.co/databricks

Model: https://huggingface.co/databricks/dolly-v2-12b

Dataset: https://github.com/databrickslabs/dolly/tree/master/data

Edit: Fixed the link to the right model",739,130,n dolly  an open source instructionfollowing llm for research and commercial use,0.0
[D] How do you find the motivation to keep doing ML?,"I currently work on ML research and am feeling completely demotivated. I want to hear how y'all manage to stay focused and productive. At a high level, here are the main reasons why I find it hard to justify working 8+ hours a day on ML:

1. **The world is burning** (Covid, climate change, social unrest), and I'm constantly wondering what the opportunity cost is for not doing something more immediately impactful and meaningful. I try to be more humble and accept that the world doesn't need me to ""save"" it. But it also feels wrong to just hunker down and tinker with hyperparameters all day.
2. In the deep learning era, the day-to-day ML work feels like **shooting in the dark**. Honestly every time I try to do something principled and grounded in theory, reality slaps me in the face. It just doesn't work. What does work is anticlimactic: training bigger & longer, or arbitrarily tweaking BERT for whatever niche.
3. **The field is so crowded**. The arxiv firehose is overwhelming and (forgive my cynicism) so full of noise. So much gets published everyday, yet so little. There's this crazy race to publish anything, regardless how meaningless that extra layer you added to BERT is. And while I really try to keep my integrity and not write a paper about how I swept the s\*\*\* out of those hyperparameters and increased the average GLUE score by a whooping 0.2, realistically I still need to keep up with this crazy pace if I don't want to get fired.

I feel trapped because I can't find pleasure neither in the process (which has become synonymous with throwing stuff at BERT and seeing what happens), nor the outcome (wasting huge amounts of compute power in a world that is burning, occasionally discovering mildly uninteresting things). At the end of the day, I'm depleted of energy and so can't rely on other areas of my life to fill in the void.

Enlighten me! What's your secret? How do you keep going?

Edit: Thank you all so much for your thoughtful messages / advice and for sharing your experiences. You all gave me a lot of food for thought and hope that it's not all lost.",737,177,d how do you find the motivation to keep doing ml,0.34
[N] The 2nd edition of An Introduction to Statistical Learning (ISLR) has officially been published (with PDF freely available),"The second edition of one of the best books (if not the best) for machine learning beginners has been published and is available for download from here: [https://www.statlearning.com](https://www.statlearning.com).

Summary of the changes:

https://preview.redd.it/6a6t8c6nrjf71.png?width=1708&format=png&auto=webp&s=30fbc427933b938a1cce97ffc2be216fb141082e",734,56,n the nd edition of an introduction to statistical learning islr has officially been published with pdf freely available,0.4404
[News] New NVIDIA EULA prohibits Deep Learning on GeForce GPUs in data centers.,"According to German tech magazine golem.de, the new NVIDIA EULA prohibits Deep Learning applications to be run on GeForce GPUs.

Sources:

https://www.golem.de/news/treiber-eula-nvidia-untersagt-deep-learning-auf-geforces-1712-131848.html

http://www.nvidia.com/content/DriverDownload-March2009/licence.php?lang=us&type=GeForce

The EULA states:

""No Datacenter Deployment. The SOFTWARE is not licensed for datacenter deployment, except that blockchain processing in a datacenter is permitted.""

EDIT: Found an English article: https://wirelesswire.jp/2017/12/62708/



",734,236,news new nvidia eula prohibits deep learning on geforce gpus in data centers,0.0
[P] I created a complete overview of machine learning concepts seen in 27 data science and machine learning interviews,"Hey everyone,

During my last interview cycle, I did 27 machine learning and data science interviews at a bunch of companies (from Google to a \~8-person YC-backed computer vision startup). Afterwards, I wrote an overview of all the concepts that showed up, presented as a series of tutorials along with practice questions at the end of each section.

I hope you find it helpful! [ML Primer](https://www.confetti.ai/assets/ml-primer/ml_primer.pdf)",733,74,p i created a complete overview of machine learning concepts seen in  data science and machine learning interviews,0.25
"[R] 🤖🌟 Unlock the Power of Personal AI: Introducing ChatLLaMA, Your Custom Personal Assistant! 🚀💬","🚀 Introducing ChatLLaMA: Your Personal AI Assistant Powered by LoRA! 🤖

&#x200B;

Hey AI enthusiasts! 🌟 We're excited to announce that you can now create custom personal assistants that run directly on your GPUs!

&#x200B;

ChatLLaMA utilizes LoRA, trained on Anthropic's HH dataset, to model seamless conversations between an AI assistant and users.

&#x200B;

Plus, the RLHF version of LoRA is coming soon! 🔥

&#x200B;

👉 Get it here: [https://cxn.to/@serpai/lora-weights](https://cxn.to/@serpai/lora-weights)

&#x200B;

📚 Know any high-quality dialogue-style datasets? Share them with us, and we'll train ChatLLaMA on them!

&#x200B;

🌐 ChatLLaMA is currently available for 30B and 13B models, and the 7B version.

&#x200B;

🔔 Want to stay in the loop for new ChatLLaMA updates? Grab the FREE \[gumroad link\]([https://cxn.to/@serpai/lora-weights](https://cxn.to/@serpai/lora-weights)) to sign up and access a collection of links, tutorials, and guides on running the model, merging weights, and more.  (Guides on running and training the model coming soon)

&#x200B;

🤔 Have questions or need help setting up ChatLLaMA? Drop a comment or DM us, and we'll be more than happy to help you out! 💬

&#x200B;

Let's revolutionize AI-assisted conversations together! 🌟

&#x200B;

\*Disclaimer: trained for research, no foundation model weights, and the post was ran through gpt4 to make it more coherent.

&#x200B;

👉 Get it here: [https://cxn.to/@serpai/lora-weights](https://cxn.to/@serpai/lora-weights)

&#x200B;

\*Edit: [https://github.com/serp-ai/LLaMA-8bit-LoRA](https://github.com/serp-ai/LLaMA-8bit-LoRA) <- training repo/instructions (If anything is unclear just let us know and we will try to help/fix the issue!)  (Sorry for spamming the link, don't really know how else to remind people lol)",726,247,r  unlock the power of personal ai introducing chatllama your custom personal assistant ,0.0
"[P] I've asked a dozen researchers about their favourite ML books, here are the results","Hey all!

Over the past week or so, I went around Twitter and asked a dozen researchers which books they would recommend.

In the end, I got responses from people like Denny Britz, Chris Albon and Jason Antic, so I hope you like their top picks :)

[https://mentorcruise.com/books/ml/](https://mentorcruise.com/books/ml/)",728,47,p ive asked a dozen researchers about their favourite ml books here are the results,0.0
"[N] Apple Executive Who Left Over Return-to-Office Policy Joins Google AI Unit: Ian Goodfellow, a former director of machine learning at Apple, is joining DeepMind.","According to an article published in [Bloomberg](https://www.bloomberg.com/news/articles/2022-05-17/ian-goodfellow-former-apple-director-of-machine-learning-to-join-deepmind), 

*An Apple Inc. executive who left over the company’s stringent return-to-office policy is joining Alphabet Inc.’s DeepMind unit, according to people with knowledge of the matter.*

*Ian Goodfellow, who oversaw machine learning and artificial intelligence at Apple, left the iPhone maker in recent weeks, citing the lack of flexibility in its work policies. The company had been planning to require corporate employees to work from the office on Mondays, Tuesdays and Thursdays, starting this month. That deadline was put on hold Tuesday, though.*

https://www.bloomberg.com/news/articles/2022-05-17/ian-goodfellow-former-apple-director-of-machine-learning-to-join-deepmind",716,108,n apple executive who left over returntooffice policy joins google ai unit ian goodfellow a former director of machine learning at apple is joining deepmind,0.0
"[N] IBM Watson is dead, sold for parts.","&#x200B;

[Sold to Francisco Partners \(private equity\) for $1B](https://preview.redd.it/bgbt7h38lgf81.png?width=500&format=png&auto=webp&s=7778a00fd4ef4e060baf9fd3fbf334aa840e7e9e)

[IBM Sells Some Watson Health Assets for More Than $1 Billion - Bloomberg](https://www.bloomberg.com/news/articles/2022-01-21/ibm-is-said-to-near-sale-of-watson-health-to-francisco-partners) 

Watson was billed as the future of healthcare, but failed to deliver on its ambitious promises.

""IBM agreed to sell part of its IBM Watson Health business to private equity firm Francisco Partners, scaling back the technology company’s once-lofty ambitions in health care.  

""The value of the assets being sold, which include extensive and wide-ranging data sets and products, and image software offerings, is more than $1 billion, according to people familiar with the plans. IBM confirmed an earlier Bloomberg report on the sale in a statement on Friday, without disclosing the price.""

This is encouraging news for those who have sights set on the healthcare industry. Also a lesson for people to focus on smaller-scale products with limited scope.",716,155,n ibm watson is dead sold for parts,-0.6486
[News] Twitter algorithm now open source,"News just released via [this Tweet](https://twitter.com/TwitterEng/status/1641872259320274944?t=OGxvSuB9SLO2nUmfA-esIA&s=19).

Source code here: https://github.com/twitter/the-algorithm

I just listened to Elon Musk and Twitter Engineering talk about it on [this Twitter space](https://twitter.com/i/spaces/1jMJgLdenVjxL).",709,152,news twitter algorithm now open source,0.0
[D] Stanford's CS229 2018 course is finally on YouTube,"Stanford's legendary [CS229 course from 2008](https://www.youtube.com/playlist?list=PLA89DCFA6ADACE599) just put all of their [2018 lecture videos](https://www.youtube.com/playlist?list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU) on YouTube. Also check out the corresponding [course website](http://cs229.stanford.edu/syllabus-autumn2018.html) with problem sets, syllabus, slides and class notes. Happy learning!

Edit: The problem sets seemed to be locked, but they are easily findable via GitHub. For instance, [this repo](https://github.com/zhixuan-lin/cs229-ps-2018) has all the problem sets for the autumn 2018 session.",708,53,d stanfords cs  course is finally on youtube,0.0
[R] A popular self-driving car dataset is missing labels for hundreds of pedestrians,"**Blog Post:** [https://blog.roboflow.ai/self-driving-car-dataset-missing-pedestrians/](https://blog.roboflow.ai/self-driving-car-dataset-missing-pedestrians/)

**Summary:** The Udacity Self Driving Car dataset (5,100 stars and 1,800 forks) contains thousands of unlabeled vehicles, hundreds of unlabeled pedestrians, and dozens of unlabeled cyclists. Of the 15,000 images, I found (and corrected) issues with 4,986 (33%) of them.

**Commentary:**  
This is really scary. I discovered this because we're working on converting and re-hosting popular datasets in many popular formats for easy use across models... I first noticed that there were a bunch of completely unlabeled images.

Upon digging in, I was appalled to find that fully 1/3 of the images contained errors or omissions! Some are small (eg a part of a car on the edge of the frame or a ways in the distance not being labeled) but some are egregious (like the woman in the crosswalk with a baby stroller).

I think this really calls out the importance of rigorously inspecting any data you plan to use with your models. Garbage in, garbage out... and self-driving cars should be treated seriously.

I went ahead and corrected by hand the missing bounding boxes and fixed a bunch of other errors like phantom annotations and duplicated boxes. There are still quite a few duplicate boxes (especially around traffic lights) that would have been tedious to fix manually, but if there's enough demand I'll go back and clean those as well.

**Corrected Dataset:** [https://public.roboflow.ai/object-detection/self-driving-car](https://public.roboflow.ai/object-detection/self-driving-car)",705,52,r a popular selfdriving car dataset is missing labels for hundreds of pedestrians,0.1531
[D] Advanced Takeaways from fast.ai book,"I recently read the Fast AI deep learning [book](https://www.goodreads.com/book/show/50204643-deep-learning-for-coders-with-fastai-and-pytorch) and wanted to summarise some of the many advanced takeaways & tricks I got from it.  I’m going to leave out the basic things because there’s enough posts about them, i’m just focusing on what I found new or special in the book.

I’ve also put the insights into a [deck](https://saveall.ai/shared/deck/140&4&3K3uXPazkg4&reddit_posts) on save all to help you remember them over the long-term. I would **massively recommend using a spaced repetition app like anki or** [**save all**](https://saveall.ai/landing/reddit_posts) **for the things you learn** otherwise you’ll just forget so much of what is important. Here’s the takeaways:

# Neural Network Training Fundamentals

* Always **start** an ML project by **producing simple baselines**
   * If is binary classification then could even be as simple as predicting the most common class in the training dataset
   * Other baselines: linear regression, random forest, boosting etc…
* Then you can **use your baseline to clean your data** by looking at the datapoints it gets most incorrect and checking to see if they are actually classified correctly in the data
* In general you can also **leverage your baselines** to **help debug** your models
   * e.g. if you make your neural network 1 layer then it should be able to match the performance of a linear regression baseline, if it doesn’t then you have a bug!
   * e.g. if adding a feature improves the performance of linear regression then it should probably also improve the performance of your neural net unless you have a bug!
* Hyperparameter optimisation can help a bit (especially for the learning rate) but in general there are default hyperparameters that can do quite well and so **closely** **optimising the hyperparameters should be one of the last things you try** rather than the first
* **If you know something** about the problem then try to **inject it as an inductive bias into the training process**
   * e.g. if some of your features are related in a sequential way then incorporate them into training separately using an RNN
   * e.g. if you know the output should only be between -3 and 3 then use sigmoid to design the final layer so that it forces the output of the network to be in this range

# Transfer Learning

* Always use transfer learning if you can by finding a model pre-trained for a similar task and then fine-tune that model for your particular task
   * e.g. see [huggingface](http://huggingface.co/) for help with this in NLP
* **Gradual unfreezing** and **discriminative learning rates** work well when fine-tuning a transfer learned model
   * **Gradual unfreezing** = freeze earlier layers and **train the later layers only**, then **gradually unfreeze** the earlier layers one by one
   * **Discriminative learning rates** = having **different learning rates per layer of your network** (usually **earlier** **layers** have **smaller learning rates** than later layers)

# Tricks to Deal with Overfitting

* **Best way** to deal with **overfitting** is by getting **more data**. **Exhaust this first** before you start regularising with other methods
* **Data augmentation** is really powerful and now possible with text as well as images:
   * **Image** data augmentation -  crop, pad, squish and resize images
   * **Text** data augmentation - negate words, replace words with similes, perturb word embeddings (nice github [repo](https://github.com/QData/TextAttack) for this)
* **Mixup regularisation** = create new data by averaging together training datapoints
* **Backwards training (NLP only):** train an additional separate model that is **fed text backwards** and then **average the outputs** of your two models to get your final prediction

# Other Tricks to Improve Performance

* **Test time augmentation** = at test time, use the **average prediction** from many **augmented versions of the input** as your prediction rather than just the prediction from the true input
* **1 cycle training** = when you increase and reduce the learning rate throughout training in a circular fashion (usually makes a **huge difference)**
* **Learning rate finder algorithm** = algorithm that Fast AI provide to help you automatically discover roughly the best learning rate
* **Never use one-hot encodings,** use **embeddings** instead, even in **tabular data**!
* Using **AdamW** instead of **Adam** can help a little bit
* **Lower precision training** can help and on [pytorch lightning](https://github.com/PyTorchLightning/pytorch-lightning) is just a simple flag you can set
* For **regression problems** if you know the **output should be within a range** then its good to use **sigmoid** to force the neural net output to be within this range
   * I.e. make the network output:  min\_value + sigmoid(output) \* (max\_value - min\_value)
* **Clustering** your features can help you **identify which ones are the most redundant** and then removing the can help performance
* **Label smoothing** = use 0.1 and 0.9 instead of 0 and 1 for label targets (can smoothen training)
* **Don’t dichotomise** your data, if your output is continuous then its better to train the network to predict continuous values rather than turning it into a classification problem
* **Progressive resizing** = train model on smaller resolution images first, then increase resolution gradually (can speed up training a lot)
* Strategically using **bottleneck layers** to force the network to form **more compact representations of the data** at different points can be helpful
* Try using **skip connections** as they can help smooth out the loss surface

&#x200B;

Please let me know if you found this helpful and if there are any other training tricks you use that we should also know about?",698,108,d advanced takeaways from fastai book,0.25
[D] Llama-3 may have just killed proprietary AI models,"[Full Blog Post ](https://www.kadoa.com/blog/llama3-killed-proprietary-models)

Meta released Llama-3 only three days  ago, and it already feels like the inflection point when open source models finally closed the gap with proprietary models. The initial benchmarks show that Llama-3 70B comes pretty close to GPT-4 in many tasks:

* The [official Meta page](https://llama.meta.com/llama3/) only shows that Llama-3 outperforms Gemini 1.5 and Claude Sonnet.
* [Artificial Analysis](https://artificialanalysis.ai/models/llama-3-instruct-70b) shows that Llama-3 is in-between Gemini-1.5 and Opus/GPT-4 for quality.
* On [LMSYS Chatbot Arena Leaderboard](https://arena.lmsys.org/), Llama-3 is ranked #5 while current GPT-4 models and Claude Opus are still tied at #1.

The even more powerful Llama-3 400B+ model is still in training and is likely to surpass GPT-4 and Opus once released.

## Meta vs OpenAI

Some speculate that Meta's goal from the start was to target OpenAI with a [""scorched earth""](https://en.wikipedia.org/wiki/Scorched_earth) approach by releasing powerful open models to disrupt the competitive landscape and avoid being left behind in the AI race.

Meta can likely outspend OpenAI on compute and talent:

* OpenAI makes an estimated revenue of $2B and is likely unprofitable. Meta generated a revenue of $134B and profits of $39B in 2023.
* Meta's compute resources likely outrank OpenAI by now.
* Open source likely attracts better talent and researchers.

One possible outcome could be the acquisition of OpenAI by Microsoft to catch up with Meta. Google is also making moves into the open model space and has similar capabilities to Meta. It will be interesting to see where they fit in.

## The Winners: Developers and AI Product Startups

I recently wrote about the [excitement of building an AI startup](https://www.kadoa.com/blog/why-building-an-ai-startup-feels-amazing) right now, as your product automatically improves with each major model advancement. With the release of Llama-3, the opportunities for developers are even greater:

* No more vendor lock-in.
* Instead of just wrapping proprietary API endpoints, developers can now integrate AI deeply into their products in a very cost-effective and performant way. There are already over 800 [llama-3 models variations on Hugging Face](https://huggingface.co/models?sort=modified&search=llama3), and it looks like everyone will be able to fine-tune for their us-cases, languages, or industry.
* Faster, cheaper hardware: Groq can now generate 800 llama-3 tokens per second at a small fraction of the GPT costs. Near-instant LLM responses at low prices are on the horizon.

Open source multimodal models for vision and video still have to catch up, but I expect this to happen very soon.

The release of Llama-3 marks a significant milestone in the democratization of AI, but it's probably too early to declare the death of proprietary models. Who knows, maybe GPT-5 will surprise us all and surpass our imaginations of what transformer models can do.

These are definitely super exciting times to build in the AI space!",690,205,d llama may have just killed proprietary ai models,-0.6705
"[P] Just discovered a new 3Blue1Brown-styled, quality ML Youtube channel.","I'm reading Jax's documentation today and in there was a link to a [""quite accessible videos to get a deeper sense""](https://jax.readthedocs.io/en/latest/jax-101/04-advanced-autodiff.html) of Automatic Differentiation and it's actually very good ([What is Automatic Differentiation](https://www.youtube.com/watch?v=wG_nF1awSSY&t=6s)?)

https://preview.redd.it/9i2tiwv5nn371.png?width=1847&format=png&auto=webp&s=083e62f60b1cfe837c68661b900750f163734140

The video style is 3Blue1Brown-inspired, explains the topic from bottom up, very accessible though not shy away from maths.

I see that the channel is still relatively small but already got some great videos on Normalising Flow and Transformer. If you like those too please go there and subscribe to encourage the authors to create more high-quality contents.",697,30,p just discovered a new bluebrownstyled quality ml youtube channel,0.0
[D] The winner of the NeurIPS 2024 Best Paper Award  sabotaged the other teams,"Presumably, the winner of the NeurIPS 2024 Best Paper Award (a guy from ByteDance, the creators of Tiktok) sabotaged the other teams to derail their research and redirect their resources to his own. Plus he was at meetings debugging his colleagues' code, so he was always one step ahead. There's a call to withdraw his paper.

[https://var-integrity-report.github.io/](https://var-integrity-report.github.io/)

I have not checked the facts themselves, so if you can verify what is asserted and if this is true this would be nice to confirm.",695,98,d the winner of the neurips  best paper award  sabotaged the other teams,0.91
[D] Possible malware found hidden inside images from the ImageNet dataset,"I think I've discovered malware hidden inside at least one image from the bat synset: http://imagenet.stanford.edu/api/text/imagenet.synset.geturls?wnid=n02139199

The following URLs show up in Microsoft's AV tools as containing malware:

> http://www. learnanimals . com/gray-bat/gray-bat.gif

> http://www. pixelbirds .co . uk/webnyct1.jpg

> http://www. pixelbirds .co . uk/webmarot2.jpg

But when I posted my find to this subreddit a few days ago, individuals had trouble reproducing my find. I assumed this meant it was a false positive, but decided to dig into why that might be. I sent Microsoft the files saying they were a false positive, and they responded saying that the files were indeed malicious. The IP addresses for the malicious files point to hosts that have been compromised numerous times in the past according to a quick search.

I believe there are two versions of gray-bat.gif, with one containing the malware and the other is completely clean. Somewhere along the line, a check is performed to determine what file to give the user requesting it and that's why some people end up with a file that doesn't contain malware. I don't know exactly what it checks for, but using wget seems to reliably get the malicious file.

When looking at this URL:

> http://www. learnanimals . com/gray-bat/gray-bat.gif

I find that it has a redirect to this page:

> http://www. learnanimals . com/cgi-sys/suspendedpage.cgi 

This suspendedpage.cgi page has HTML code that contains a redirect to a URL that I suspect contains the malicious file:

https://pastebin.com/HXPxcgTV

It may be related to this: https://blog.malwarebytes.com/threat-analysis/2015/02/deceiving-cpanel-account-suspended-page-serves-exploits/

The URL that's redirected to appears to be associated with malware distribution. VirusTotal & Hybrid-Analysis for the fwdssp domain:
 
https://www.virustotal.com/gui/url/b142b3628c4c53c531a26fdbffa973cd8f500749581384c09eb4c2ea5b198aab/details

https://www.virustotal.com/gui/url/f572077bfe5e53f7be82c2457e98ad45ebbff51c954be6dc0cf228666ddeda70/detection

https://www.hybrid-analysis.com/sample/1f6ea986f545c1099a0cb39db793058a4c18a0a5151ffc62cc541978fa61c482

https://www.joesandbox.com/analysis/280363/0/html

I haven't been able to find out if/how the other two images work and I don't know what the malicious code is doing. I could be completely wrong about this, so keep that in mind. I also don't know if this possible malware is a threat to anyone downloading the ImageNet dataset or who the intended targets are. I also haven't checked every ImageNet image, as I've only been using a few synsets.

Edit:

Google Drive is now suddenly reporting the files as infected with a virus, but most AV tools are still not detecting anything. I also uploaded the files to VirusTotal here: https://www.virustotal.com/gui/file/bf1c1063f889d834a826d8e7c79134c2a674705f2504ce4af6018d4b0d47f980/detection",693,60,d possible malware found hidden inside images from the imagenet dataset,0.0
"[R] Google Replaces BERT Self-Attention with Fourier Transform: 92% Accuracy, 7 Times Faster on GPUs","A research team from Google shows that replacing transformers’ self-attention sublayers with Fourier Transform achieves 92 percent of BERT accuracy on the GLUE benchmark with training times seven times faster on GPUs and twice as fast on TPUs.

Here is a quick read: [Google Replaces BERT Self-Attention with Fourier Transform: 92% Accuracy, 7 Times Faster on GPUs.](https://syncedreview.com/2021/05/14/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-19/)

The paper *FNet: Mixing Tokens with Fourier Transforms* is on [arXiv](https://arxiv.org/abs/2105.03824).",692,97,r google replaces bert selfattention with fourier transform  accuracy  times faster on gpus,0.0
[R] The Modern Mathematics of Deep Learning,"[PDF on ResearchGate](https://www.researchgate.net/publication/351476107_The_Modern_Mathematics_of_Deep_Learning) / [arXiv](https://arxiv.org/abs/2105.04026) (This review paper appears as a book chapter in the book [""Mathematical Aspects of Deep Learning""](https://doi.org/10.1017/9781009025096) by Cambridge University Press)

**Abstract:**  We describe the new field of mathematical analysis of deep learning. This field emerged around a list of research questions that were not answered within the classical framework of learning theory. These questions concern: the outstanding generalization power of overparametrized neural networks, the role of depth in deep architectures, the apparent absence of the curse of dimensionality, the surprisingly successful optimization performance despite the non-convexity of the problem, understanding what features are learned, why deep architectures perform exceptionally well in physical problems, and which fine aspects of an architecture affect the behavior of a learning task in which way. We present an overview of modern approaches that yield partial answers to these questions. For selected approaches, we describe the main ideas in more detail.",692,142,r the modern mathematics of deep learning,0.0
"[N] Hinton, LeCun, Bengio receive ACM Turing Award","According to [NYTimes](https://www.nytimes.com/2019/03/27/technology/turing-award-hinton-lecun-bengio.html) and [ACM website](https://awards.acm.org/about/2018-turing): *Yoshua Bengio, Geoffrey Hinton and Yann LeCun, the fathers of deep learning, receive the ACM Turing Award for conceptual and engineering breakthroughs that have made deep neural networks a critical component of computing today.*",679,159,n hinton lecun bengio receive acm turing award,0.5423
"[D] Can we please stop using ""is all we need"" in titles?","As the title suggests. We need to stop or decrease the usage of ""... is all we need"" in paper titles. It's slowly getting a bit ridiculous. There is most of the time no actual scientific value in it. It has become a bad practice of attention grabbing for attentions' sake.",679,104,d can we please stop using is all we need in titles,0.0258
[D] PyTorch is moving to the Linux Foundation,"https://pytorch.org/blog/PyTorchfoundation/

I wonder if this will lead to a lot of departures at Meta.",679,70,d pytorch is moving to the linux foundation,0.0
"[N] Hugging Face raised $100M at $2B to double down on community, open-source & ethics","👋 Hey there! Britney Muller here from Hugging Face. We've got some big news to share!

* Hugging Face Full Series C Announcement: [https://huggingface.co/blog/series-c](https://huggingface.co/blog/series-c)
* TechCrunch: [https://techcrunch.com/2022/05/09/hugging-face-reaches-2-billion-valuation-to-build-the-github-of-machine-learning/](https://techcrunch.com/2022/05/09/hugging-face-reaches-2-billion-valuation-to-build-the-github-of-machine-learning/)

We want to have a positive impact on the AI field. We think the direction of more responsible AI is through openly sharing models, datasets, training procedures, evaluation metrics and working together to solve issues. We believe open source and open science bring trust, robustness, reproducibility, and continuous innovation. With this in mind, we are leading [**BigScience**](https://bigscience.huggingface.co/), a collaborative workshop around the study and creation of very large language models gathering more than 1,000 researchers of all backgrounds and disciplines. We are now training the [**world's largest open source multilingual language model**](https://twitter.com/BigScienceLLM) 🌸

Over 10,000 companies are now using Hugging Face to build technology with machine learning. Their Machine Learning scientists, Data scientists and Machine Learning engineers have saved countless hours while accelerating their machine learning roadmaps with the help of our [**products**](https://huggingface.co/platform) and [**services**](https://huggingface.co/support).

⚠️ But there’s still a huge amount of work left to do.

At Hugging Face, we know that Machine Learning has some important limitations and challenges that need to be tackled now like biases, privacy, and energy consumption. With openness, transparency & collaboration, we can foster responsible & inclusive progress, understanding & accountability to mitigate these challenges.

Thanks to the new funding, we’ll be doubling down on research, open-source, products and responsible democratization of AI.",681,52,n hugging face raised m at b to double down on community opensource  ethics,0.4215
[D] Some interesting observations about machine learning publication practices from an outsider,"I come from a traditional engineering field, and here is my observation about ML publication practice lately:

I have noticed that there are groups of researchers working on the intersection of ""old"" fields such as optimization, control, signal processing and the like, who will all of a sudden publish a massive amount of paper that purports to solve a certain problem. The problem itself is usually recent and sometimes involves some deep neural network.

However, upon close examination, the only novelty is the problem (usually proposed by other unaffiliated groups) but not the method proposed by the researchers that purports to solve it.

I was puzzled by why a very large amount of seemingly weak papers, literally rehashing (occasionally, well-known) techniques from the 1980s or even 60s are getting accepted, and I noticed the following recipe:

1. **Only ML conferences.** These groups of researchers will only ever publish in machine learning conferences (and not to optimization and control conferences/journals, where the heart of their work might actually lie). For example, on a paper about adversarial machine learning, the entire paper was actually about solving an optimization problem, but the optimization routine is basically a slight variation of other well studied methods. ***Update***: I also noticed that if a paper does not go through NeurIPS or ICLR, they will be directly sent to AAAI and some other smaller name conferences, where they will be accepted. So nothing goes to waste in this field.
2. **Peers don't know what's going on.** Through openreview, I found that the reviewers (not just the researchers) are uninformed about their particular area, and only seem to comment on the correctness of the paper, but not the novelty. In fact, I doubt the reviewers themselves know about the novelty of the method. ***Update***: by novelty I meant how novel it is with respect to the state-of-the-art of a certain technique, especially when it intersects with operations research, optimization, control, signal processing. The state-of-the-art *could be* far ahead than what mainstream ML folks know about.
3. **Poor citation practices.** Usually the researchers will only cite themselves or other ""machine learning people"" (whatever this means) from the last couple of years. Occasionally, there will be 1 citation from hundreds of years ago attributed to Cauchy, Newton, Fourier, Cournot, Turing, Von Neumann and the like, and then a hundred year jump to 2018 or 2019. I see, ""This problem was studied by *some big name* in 1930 and *Random Guy XYZ* in 2018"" a lot.
4. **Wall of math.** Frequently, there will be a massive wall of math, proving some esoteric condition on the eigenvalue, gradient, Jacobian, and other curious things about their problem (under other esoteric assumptions). There will be several theorems, none of which are applicable because the moment they run their highly non-convex deep learning application, all conditions are violated. Hence the only thing obtained from these intricate theorems + math wall are some faint intuition (which are violated immediately). And then nothing is said. 

***Update***: If I could add one more, it would be that certain techniques, after being proposed, and after the authors claim that it beats a lot of benchmarks, will be seemingly be abandoned and never used again. ML researchers seem to like to jump around topics a lot, so that might be a factor. But usually in other fields, once a technique is proposed, it is refined by the same group of researchers over many years, sometimes over the course of a researcher's career.

In some ways, this makes certain area of ML sort of an echo chamber, where researchers are pushing through a large amount of known results rehashed and somewhat disguised by the novelty of their problem and these papers are all getting accepted because no one can detect the lack of novelty (or when they do detect, it is only 1 guy out of 3 reviewers). I just feel like ML conferences are sort of being treated as some sort of automatic paper acceptance cash cow.

Just my two cents coming from outside of ML. My observation does not apply to all fields of ML.",670,171,d some interesting observations about machine learning publication practices from an outsider,0.4019
"[P] 64,000 pictures of cars, labeled by make, model, year, price, horsepower, body style, etc.",[ Removed by Reddit in response to a copyright notice. ],670,46,p  pictures of cars labeled by make model year price horsepower body style etc,0.0
[N] Microsoft buys AI speech tech company Nuance for $19.7 billion,"From [The Verge](https://www.theverge.com/2021/4/12/22379414/microsoft-buys-nuance-ai-speech-tech).

I may be wrong on this, but afaik it has been a while since Microsoft made such a huge acquisition of a company with an arguably heavily-convoluted internal ecosystem. It feels like MS did it for the data acquisition processes more than for the product portfolio, which IMO will be cannibalized. Any thoughts?",671,82,n microsoft buys ai speech tech company nuance for  billion,0.0
[Discussion] Machine Learning is not just about Deep Learning,"I understand how mind blowing the potential of deep learning is, but the truth is, majority of companies in the world dont care about it, or do not need that level of machine learning expertise.

If we want to democratize machine learning we have to acknowledge the fact the most people Learning all the cool generative neural networks will not end up working for Google or Facebook.

What I see is that most youngsters join this bandwagon of machine learning with hopes of working on these mind-blowing ideas, but when they do get a job at a descent company with a good pay, but are asked to produce ""medicore"" models, they feel like losers.
I dont know when, but somewhere in this rush of deep learning, the spirit of it all got lost.

Since when did the people who use Gradient Boosting, Logistic regression, Random Forest became oldies and medicore.

The result is that, most of the guys we interwiew for a role know very little about basics and hardly anything about the underlying maths.
The just know how to use the packages on already prepared data.

Update : Thanks for all the comments, this discussion has really been enlightening for me and an amazing experience, given its my first post in reddit.
Thanks a lot for the Gold Award, it means a lot to me.

Just to respond to some of the popular questions and opinions in the comments.

1. Do we expect people to have to remember all the maths of the machine learning?

No ways, i dont remember 99% of what i studied in college. But thats not the point. When applying these algorithms, one must know the underlying principles of it, and not just which python library they need to import.

2. Do I mean people should not work on Deep Learning or not make a hype of it, as its not the best thing?

Not at all, Deep Learning is the frontier of Machine Learning and its the mind blowing potential of deep learning which brought most of us into the domain.
All i meant was, in this rush to apply deep learning to everything, we must not lose sight of simpler models, which most companies across the world still use and would continue to use due to there interpretability.

3. What do I mean by Democratization of ML.

ML is a revolutionary knowledge, we can all agree on that, and therefore it is essential that such knowledge be made available to all the people, so they can learn about its potential and benifit from the changes it brings to there lives, rather then being intimidated by it. People are always scared of what they don't understand.",666,191,discussion machine learning is not just about deep learning,0.0
[D] How OpenAI Sold its Soul for $1 Billion: The company behind GPT-3 and Codex isn’t as open as it claims.,"An essay by Alberto Romero that traces the history and developments of OpenAI from the time it became a ""capped-for-profit"" entity from a non-profit entity:

Link: https://onezero.medium.com/openai-sold-its-soul-for-1-billion-cf35ff9e8cd4",659,107,d how openai sold its soul for  billion the company behind gpt and codex isnt as open as it claims,0.0
"[N] GPT-4 is coming next week – and it will be multimodal, says Microsoft Germany - heise online","[https://www.heise.de/news/GPT-4-is-coming-next-week-and-it-will-be-multimodal-says-Microsoft-Germany-7540972.html](https://www.heise.de/news/GPT-4-is-coming-next-week-and-it-will-be-multimodal-says-Microsoft-Germany-7540972.html)

>**GPT-4 is coming next week**: at an approximately one-hour hybrid information event entitled ""**AI in Focus - Digital Kickoff"" on 9 March 2023**, four Microsoft Germany employees presented Large Language Models (LLM) like GPT series as a disruptive force for companies and their Azure-OpenAI offering in detail. The kickoff event took place in the German language, news outlet Heise was present. **Rather casually, Andreas Braun, CTO Microsoft Germany** and Lead Data & AI STU, **mentioned** what he said was **the imminent release of GPT-4.** The fact that **Microsoft is fine-tuning multimodality with OpenAI should no longer have been a secret since the release of Kosmos-1 at the beginning of March.**

[ Dr. Andreas Braun, CTO Microsoft Germany and Lead Data  & AI STU at the Microsoft Digital Kickoff: \\""KI im Fokus\\"" \(AI in  Focus, Screenshot\) \(Bild: Microsoft\) ](https://preview.redd.it/rnst03avarma1.jpg?width=1920&format=pjpg&auto=webp&s=c5992e2d6c6daf32e56a0a3ffeeecfe10621f73f)",659,80,n gpt is coming next week  and it will be multimodal says microsoft germany  heise online,0.0
"[P] Probabilistic Machine Learning: An Introduction, Kevin Murphy's 2021 e-textbook is out","Here is the link to the draft of his new textbook, Probabilistic Machine Learning: An Introduction.

https://probml.github.io/pml-book/book1.html

Enjoy!",658,106,p probabilistic machine learning an introduction kevin murphys  etextbook is out,0.0
"[N] Getty Images Claims Stable Diffusion Has Stolen 12 Million Copyrighted Images, Demands $150,000 For Each Image","From [Article](https://www.theinsaneapp.com/2023/02/getty-images-stable-diffusion.html):

Getty Images new lawsuit claims that Stability AI, the company behind Stable Diffusion's AI image generator, stole 12 million Getty images with their captions, metadata, and copyrights ""without permission"" to ""train its Stable Diffusion algorithm.""

The company has asked the court to order Stability AI to remove violating images from its website and pay $150,000 for each. 

However, it would be difficult to prove all the violations. Getty submitted over 7,000 images, metadata, and copyright registration, used by Stable Diffusion.",656,321,n getty images claims stable diffusion has stolen  million copyrighted images demands  for each image,-0.25
"[D] We're the Meta AI research team behind CICERO, the first AI agent to achieve human-level performance in the game Diplomacy. We’ll be answering your questions on December 8th starting at 10am PT. Ask us anything!","**EDIT 11:58am PT:** Thanks for all the great questions, we stayed an almost an hour longer than originally planned to try to get through as many as possible — but we’re signing off now! We had a great time and thanks for all thoughtful questions!

PROOF: [https://i.redd.it/8skvttie6j4a1.png](https://i.redd.it/8skvttie6j4a1.png)

We’re part of the research team behind CICERO, Meta AI’s latest research in cooperative AI. CICERO is the first AI agent to achieve human-level performance in the game Diplomacy. Diplomacy is a complex strategy game involving both cooperation and competition that emphasizes natural language negotiation between seven players.   Over the course of 40 two-hour games with 82 human players, CICERO achieved more than double the average score of other players, ranked in the top 10% of players who played more than one game, and placed 2nd out of 19 participants who played at least 5 games.   Here are some highlights from our recent announcement:

* **NLP x RL/Planning:** CICERO combines techniques in NLP and RL/planning, by coupling a controllable dialogue module with a strategic reasoning engine. 
* **Controlling dialogue via plans:** In addition to being grounded in the game state and dialogue history, CICERO’s dialogue model was trained to be controllable via a set of intents or plans in the game. This allows CICERO to use language intentionally and to move beyond imitation learning by conditioning on plans selected by the strategic reasoning engine.
* **Selecting plans:** CICERO uses a strategic reasoning module to make plans (and select intents) in the game. This module runs a planning algorithm which takes into account the game state, the dialogue, and the strength/likelihood of various actions. Plans are recomputed every time CICERO sends/receives a message.
* **Filtering messages:** We built an ensemble of classifiers to detect low quality messages, like messages contradicting the game state/dialogue history or messages which have low strategic value. We used this ensemble to aggressively filter CICERO’s messages. 
* **Human-like play:** Over the course of 72 hours of play – which involved sending 5,277 messages – CICERO was not detected as an AI agent.

You can check out some of our materials and open-sourced artifacts here: 

* [Research paper](https://www.science.org/doi/10.1126/science.ade9097)
* [Project overview](https://ai.facebook.com/research/cicero/)
* [Diplomacy gameplay page](https://ai.facebook.com/research/cicero/diplomacy/)
* [Github repo](https://github.com/facebookresearch/diplomacy_cicero)
* [Our latest blog post](https://ai.facebook.com/blog/cicero-ai-negotiates-persuades-and-cooperates-with-people/)

Joining us today for the AMA are:

* Andrew Goff (AG), 3x Diplomacy World Champion
* Alexander Miller (AM), Research Engineering Manager
* Noam Brown (NB), Research Scientist [(u/NoamBrown)](https://www.reddit.com/user/NoamBrown/)
* Mike Lewis (ML), Research Scientist [(u/mikelewis0)](https://www.reddit.com/user/mikelewis0/)
* David Wu (DW), Research Engineer [(u/icosaplex)](https://www.reddit.com/user/icosaplex/)
* Emily Dinan (ED), Research Engineer
* Anton Bakhtin (AB), Research Engineer
* Adam Lerer (AL), Research Engineer
* Jonathan Gray (JG), Research Engineer
* Colin Flaherty (CF), Research Engineer [(u/c-flaherty)](https://www.reddit.com/user/c-flaherty)

We’ll be here on December 8, 2022 @ 10:00AM PT - 11:00AM PT.",660,162,d were the meta ai research team behind cicero the first ai agent to achieve humanlevel performance in the game diplomacy well be answering your questions on december th starting at am pt ask us anything,0.0
[P][N] Announcing Connected Papers - A visual tool for researchers to find and explore academic papers," Hi /r/MachineLearning, 

After a long beta, we are really excited to release [Connected Papers](http://connectedpapers.com/) to the public!

Connected papers is a unique, visual tool to help researchers and applied scientists find and explore papers relevant to their field of work.

[https://www.connectedpapers.com/](https://www.connectedpapers.com/)

I'm one of the creators, and in my work as a ML&CV engineer and team lead, almost every project involves a phase of literature review - trying to find the most similar work to the problem my team is trying to solve, or trying to track the relevant state of the art and apply it to our use case.

Connected Papers enables the researcher/engineer to explore paper-space in a much more efficient way. Given one paper that you think is relevant to your problem, it generates a visual graph of related papers in a way that makes it easy to see the most cited / recent / similar papers at a glance (Take a look at this [example graph](http://beta.connectedpapers.com:8050/main/9397e7acd062245d37350f5c05faf56e9cfae0d6/DeepFruits-A-Fruit-Detection-System-Using-Deep-Neural-Networks/graph) for a paper called ""DeepFruits: A Fruit Detection System Using Deep Neural Networks"").

You can read more about us in our launch blog post here:

[https://medium.com/connectedpapers/announcing-connected-papers-a-visual-tool-for-researchers-to-find-and-explore-academic-papers-89146a54c7d4?sk=eb6c686826e03958504008fedeffea18](https://medium.com/connectedpapers/announcing-connected-papers-a-visual-tool-for-researchers-to-find-and-explore-academic-papers-89146a54c7d4?sk=eb6c686826e03958504008fedeffea18)

Discussion and feedback are welcome!

Cheers,  
Eddie",656,80,pn announcing connected papers  a visual tool for researchers to find and explore academic papers,0.0
[D] Is the tech industry still not recovered or I am that bad?,"I am a recent PhD graduate from a top university in Europe, working on some popular topics in ML/CV, I've published 8 - 20 papers, most of which I've first-authored. These papers have accumulated 1000 - 3000 citations. (using a new account and wide range to maintain anonymity)

Despite what I thought I am a fairly strong candidate, I've encountered significant challenges in my recent job search. I have been mainly aiming for Research Scientist positions, hopefully working on open-ended research. I've reached out to numerous senior ML researchers across the EMEA region, and while some have expressed interests, unfortunately, none of the opportunities have materialised due to various reasons, such as limited headcounts or simply no updates from hiring managers.

I've mostly targeted big tech companies as well as some recent popular ML startups. Unfortunately, the majority of my applications were rejected, often without the opportunity for an interview. (I only got interviewed once by one of the big tech companies and then got rejected.) In particular, despite referrals from friends, I've met immediate rejection from Meta for Research Scientist positions (within a couple of days). I am currently simply very confused and upset and not sure what went wrong, did I got blacklisted from these companies? But I couldn't recall I made any enemies. I am hopefully seeking some advise on what I can do next....",661,239,d is the tech industry still not recovered or i am that bad,-0.5423
"[N] School of AI, founded by Siraj Raval, severs ties with Siraj Raval over recents scandals","https://twitter.com/SchoolOfAIOffic/status/1185499979521150976

Wow, just when you thought it wouldn't get any worse for Siraj lol",655,177,n school of ai founded by siraj raval severs ties with siraj raval over recents scandals,-0.4939
"[N] Kaggle Deep Fake detection: 470Gb of videos, $1M prize pool 💰💰💰","[https://www.kaggle.com/c/deepfake-detection-challenge](https://www.kaggle.com/c/deepfake-detection-challenge)

Some people were concerned with the possible flood of deep fakes. Some people were concerned with low prizes on Kaggle. This seems to address those concerns.",649,113,n kaggle deep fake detection gb of videos m prize pool ,0.0516
[Discussion] OpenAI should now change their name to ClosedAI,It's the only way to complete the hype wave.,651,222,discussion openai should now change their name to closedai,0.0
[D] How Facebook got addicted to spreading misinformation,"Behind paywall:

With new machine-learning models coming online daily, the company created a new system to track their impact and maximize user engagement. The process is still the same today. Teams train up a new machine-learning model on FBLearner, whether to change the ranking order of posts or to better catch content that violates Facebook’s community standards (its rules on what is and isn’t allowed on the platform). Then they test the new model on a small subset of Facebook’s users to measure how it changes engagement metrics, such as the number of likes, comments, and shares, says Krishna Gade, who served as the engineering manager for news feed from 2016 to 2018.

If a model reduces engagement too much, it’s discarded. Otherwise, it’s deployed and continually monitored. On Twitter, Gade explained that his engineers would get notifications every few days when metrics such as likes or comments were down. Then they’d decipher what had caused the problem and whether any models needed retraining.

But this approach soon caused issues. The models that maximize engagement also favor controversy, misinformation, and extremism: put simply, people just like outrageous stuff. Sometimes this inflames existing political tensions. The most devastating example to date is the case of Myanmar, where viral fake news and hate speech about the Rohingya Muslim minority escalated the country’s religious conflict into a full-blown genocide. Facebook admitted in 2018, after years of downplaying its role, that it had not done enough “to help prevent our platform from being used to foment division and incite offline violence.”

While Facebook may have been oblivious to these consequences in the beginning, it was studying them by 2016. In an internal presentation from that year, reviewed by the Wall Street Journal, a company researcher, Monica Lee, found that Facebook was not only hosting a large number of extremist groups but also promoting them to its users: “64% of all extremist group joins are due to our recommendation tools,” the presentation said, predominantly thanks to the models behind the “Groups You Should Join” and “Discover” features.

https://www.technologyreview.com/2021/03/11/1020600/facebook-responsible-ai-misinformation/",638,119,d how facebook got addicted to spreading misinformation,-0.3182
"[D] Hi everyone! Founder of Anaconda & Pydata.org here, to ask a favor...","My team and I are working on figuring out the best ways to invest and better support the data science & numerical computing community. We put together a small survey ""Day in the Life of a Data Scientist"", and would really appreciate getting feedback from the reddit data science & ML community.

The survey: https://www.surveymonkey.com/r/PYNPW5D

Also, of course, please feel free to leave comments, thoughts, and questions for me and the team here on this thread.

Thank you!

-Peter",638,64,d hi everyone founder of anaconda  pydataorg here to ask a favor,0.4574
[N] Udacity had an interventional meeting with Siraj Raval on content theft for his AI course,"&#x200B;

According to Udacity insiders Mat Leonard @MatDrinksTea and Michael Wales @walesmd:

&#x200B;

https://preview.redd.it/yr5yg453tjo31.png?width=978&format=png&auto=webp&s=358a16c6f4493eb0d15b57ed29e28ac69721e3e2

[https://twitter.com/MatDrinksTea/status/1175481042448211968](https://twitter.com/MatDrinksTea/status/1175481042448211968)

>Siraj has a habit of stealing content and other people’s work. That he is allegedly scamming these students does not surprise me one bit. I hope people in the ML community stop working with him.

[https://twitter.com/walesmd/status/1176268937098596352](https://twitter.com/walesmd/status/1176268937098596352)

>Oh no, not when working with us. We literally had an intervention meeting, involving multiple Directors, including myself, to explain to you how non-attribution was bad. Even the Director of Video Production was involved, it was so blatant that non-tech pointed it out.  
>  
>If I remember correctly, in the same meeting we also had to explain why Pepe memes were not appropriate in an educational context.  This was right around the time we told you there was absolutely no way your editing was happening and we required our own team to approve.  
>  
>And then we also decided, internally, as soon as the contract ended; @MatDrinksTea would be redoing everything.",643,216,n udacity had an interventional meeting with siraj raval on content theft for his ai course,0.0
[P] The Last Machine & Deep-Learning Compendium You’ll Ever Need,"**TL;DR –** [Go to The Compendium](https://towardsdatascience.com/the-last-machine-deep-learning-compendium-youll-ever-need-dc973643c4e1) – This is a curated ***\~330*** page document, with resources on almost any Data Science and ML topic you can probably imagine.

***Disclaimer:*** This is not my project, but a friend's.

I know medium posts are not exactly projects – but this one should count as one.

It is an incredible resource created over a very long period of time – it has literally hundreds of pages with links and summaries on almost any topic in DS, ML, DL you can think of (using CTRL+F is a huge pleasure). It is still being maintained, by someone that has real life experience in the industry and academic research....also, if you want [you can go directly to the Google Doc itself](https://docs.google.com/document/d/1wvtcwc8LOb3PZI9huQOD7UjqUoY98N5r3aQsWKNAlzk/edit?usp=sharing).

I think this would be a great resource for many people in the community, and this might be a good place to share additional awesome curated resources.",636,40,p the last machine  deeplearning compendium youll ever need,0.0
[N] Free copy of Deep Learning with PyTorch book now available online,"PyTorch just released a [free copy](https://pytorch.org/deep-learning-with-pytorch) of the newly released Deep Learning with PyTorch book, which contains 500 pages of content spanning everything PyTorch. Happy Learning!",631,79,n free copy of deep learning with pytorch book now available online,0.5106
[D] Hugging Face has released an official course,"Link: [https://huggingface.co/course/](https://huggingface.co/course/chapter1)

The incredible team over at hugging face has put out a course covering almost the entirety of their ecosystem:

\- Transformers  
\- Datasets  
\- Tokenizers  
\- Accelerate  
\- Model Hub

They also plan on hosting live office hours and facilitating study groups via their forums. 

&#x200B;

PS: If there's enough interest from APAC regions, I would love to help organise a study group. (I do not work at HF, but I'm excited to dive into this course)",635,59,d hugging face has released an official course,0.4215
[P] Browse State-of-the-Art Papers with Code,"[https://paperswithcode.com/sota](https://paperswithcode.com/sota)

Hi all,

We’ve just released the latest version of Papers With Code. As part of this we’ve extracted 950+ unique ML tasks, 500+ evaluation tables (with state of the art results) and 8500+ papers with code. We’ve also open-sourced the entire dataset.

Everything on the site is editable and versioned. We’ve found the tasks and state-of-the-art data really informative to discover and compare research - and even found some research gems that we didn’t know about before. Feel free to join us in annotating and discussing papers!

Let us know your thoughts.

Thanks!

Robert",632,71,p browse stateoftheart papers with code,0.0
[D] Is ML doomed to end up closed-source?,"So basically, OpenAI is keeping its models a secret, Hugging Face added a new gated feature, and LLaMA is using a non-commercial license. It looks like companies are all moving towards closed-source and monopolizing ML. 

I've always loved Hugging Face, but now they are doing the opposite of what they preach with this new gated feature thing, this is just not open-source and shouldn't be encouraged in the first place.

Open AI [clearly stated](https://openai.com/policies/terms-of-use#:~:text=use%20output%20from%20the%20Services%20to%20develop%20models%20that%20compete%20with%20OpenAI) that you can't ""use output from the Services to develop models that compete with OpenAI""

Google shared its paper Attention Is All You Need transparently which was a breakthrough in NLP and got utilized by OpenAI (with many other papers) to build GPT-4 which is adopted by Bing and now posing risk to Google's business. As a consequence, could companies start to avoid sharing research openly and rather monopolize their work for the sake of their own business safety?

Also, assuming we will witness more of these closed-source models. is it safe to just trust them without understanding what data they got exactly trained on? This doesn't seem to make sense, not sure how this would end up.",627,120,d is ml doomed to end up closedsource,-0.6369
"[D] Folks here have no idea how competitive top PhD program admissions are these days, wow...","I'm a CS PhD student, and I see the profiles of everyone admitted to our school (and similar top schools) these days since I'm right in the center of everything (and have been for years). 

I'm reading the comments on the [other thread](https://www.reddit.com/r/MachineLearning/s/o1CLXtdxh8) and honestly shocked. So many ppl believe the post is fake and I see comments saying things like ""you don't even need top conference papers to get into top PhD programs"" (this is incorrect). I feel like many folks here are not up-to-date with just how competitive admissions are to top PhD programs these days...

In fact I'm not surprised. The top programs look at much more than simply publications. Incredibly strong LOR from famous/respected professors and personal connections to the faculty you want to work with are MUCH more important. Based on what they said (how they worked on the papers by themselves and don't have good recs), they have neither of these two most important things...

FYI most of the PhD admits in my year had 7+ top conference papers (some with best paper awards), hundreds of citations, tons of research exp, masters at top schools like CMU or UW or industry/AI residency experience at top companies like Google or OpenAI, rec letters from famous researchers in the world, personal connections, research awards, talks for top companies or at big events/conferences, etc... These top programs are choosing the **top students to admit from the entire world**.

The folks in the comments have no idea how competitive NLP is (which I assume is the original OP's area since they mentioned EMNLP). Keep in mind this was before the ChatGPT boom too, so things now are probably even more competitive...

Also pasting a comment I wrote on a similar thread months back:

""PhD admissions are incredibly competitive, especially at top schools. Most admits to top ML PhD programs these days have multiple publications, numerous citations, incredibly strong LoR from respected researchers/faculty, personal connections to the faculty they want to work with, other research-related activities and achievements/awards, on top of a good GPA and typically coming from a top school already for undergrad/masters.

Don't want to scare/discourage you but just being completely honest and transparent. It gets worse each year too (competition rises exponentially), and I'm usually encouraging folks who are just getting into ML research (with hopes/goals of pursuing a PhD) with no existing experience and publications to maybe think twice about it or consider other options tbh.

It does vary by subfield though. For example, areas like NLP and vision are incredibly competitive, but machine learning theory is relatively less so.""

Edit1: FYI I don't agree with this either. It's insanely unhealthy and overly competitive. However there's no choice when the entire world is working so hard in this field and there's so many ppl in it... These top programs admit the best people due to limited spots, and they can't just reject better people for others.

Edit2: some folks saying u don't need so many papers/accomplishments to get in. That's true if you have personal connections or incredibly strong letters from folks that know the target faculty well. In most cases this is not the case, so you need more pubs to boost your profile. Honestly these days, you usually need both (connections/strong letters plus papers/accomplishments).

Edit3: for folks asking about quality over quantity, I'd say quantity helps you get through the earlier admission stages (as there are way too many applicants so they have to use ""easy/quantifiable metrics"" to filter like number of papers - unless you have things like connections or strong letters from well-known researchers), but later on it's mainly quality and research fit, as individual faculty will review profiles of students (and even read some of their papers in-depth) and conduct 1-on-1 interviews. So quantity is one thing that helps get you to the later stages, but quality (not just of your papers, but things like rec letters and your actual experience/potential) matters much more for the final admission decision.

Edit4: like I said, this is field/area dependent. CS as a whole is competitive, but ML/AI is another level. Then within ML/AI, areas like NLP and Vision are ridiculous. It also depends what schools and labs/profs you are targeting, research fit, connections, etc. Not a one size fits all. But my overall message is that things are just crazy competitive these days as a whole, although there will be exceptions.

Edit5: not meant to be discouraging as much as honest and transparent so folks know what to expect and won't be as devastated with results, and also apply smarter (e.g. to more schools/labs including lower-ranked ones and to industry positions). Better to keep more options open in such a competitive field during these times...

Edit6: IMO most important things for top ML PhD admissions: connections and research fit with the prof >= rec letters (preferably from top researchers or folks the target faculty know well) > publications (quality) > publications (quantity) >= your overall research experiences and accomplishments > SOP (as long as overall research fit, rec letters, and profile are strong, this is less important imo as long as it's not written poorly) >>> GPA (as long as it's decent and can make the normally generous cutoff you'll be fine) >> GRE/whatever test scores (normally also cutoff based and I think most PhD programs don't require them anymore since Covid)",622,260,d folks here have no idea how competitive top phd program admissions are these days wow,0.6249
[R] Meta AI open sources new SOTA LLM called LLaMA. 65B version (trained on 1.4T tokens) is competitive with Chinchilla and Palm-540B. 13B version outperforms OPT and GPT-3 175B on most benchmarks.,"[https://twitter.com/GuillaumeLample/status/1629151231800115202?t=4cLD6Ko2Ld9Y3EIU72-M2g&s=19](https://twitter.com/GuillaumeLample/status/1629151231800115202?t=4cLD6Ko2Ld9Y3EIU72-M2g&s=19)

Paper here - [https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/](https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/)",629,214,r meta ai open sources new sota llm called llama b version trained on t tokens is competitive with chinchilla and palmb b version outperforms opt and gpt b on most benchmarks,0.1779
[D] How to save my father's voice?,"My father has contracted ALS, a disease where the motor neurons begin to degrade resulting in paralysis and death. There is no effective treatment and people typically live for 3-5 years after diagnosis,  however my father appears to be progressing more rapidly than is typical - going from being able to walk in October to needing a wheelchair now.

Today, to my horror, I've discovered that it's reached the stage where it is beginning to affect his voice. The next stage will be an inability to speak. I'm really scared about forgetting what he sounds like and my intention is to produce a large number of recordings of his voice.

I was wondering if anyone knew of anything out there that use machine learning to capture his voice and generate new recordings. It would be great if it was something I could use in a text-to-speech engine. Not only could I have something to remember him by and share with my future children, but he could potentially use in a speech synthesizer so he can still speak in his own voice.

I have come across one or two companies that claim to do it for the purpose of tweaking interviews, but on contacting them I haven't had much success.

Any help would be much appreciated. If this is the wrong place to post please let me know.",620,70,d how to save my fathers voice,0.4939
WSJ: The AI industry spent 17x more on Nvidia chips than it brought in in revenue [N],"> ...
> In a presentation earlier this month, the venture-capital firm Sequoia estimated that the AI industry spent $50 billion on the Nvidia chips used to train advanced AI models last year, but brought in only $3 billion in revenue. 

Source: [WSJ](https://www.wsj.com/tech/ai/a-peter-thiel-backed-ai-startup-cognition-labs-seeks-2-billion-valuation-998fa39d) (paywalled)",623,140,wsj the ai industry spent x more on nvidia chips than it brought in in revenue n,0.0
"[D] Is there a ML community ""blind eye"" toward the negative impact of FAANG recommendation algorithms on global society?","If anyone has seen the social dilemma, you'll understand the impact FAANG recommender algorithms have on society. Not in a vague, roundabout way either. These algorithms are trained to maximize profit by influencing people's attention, information streams and priority queues. I think its truly a shame that working for Facebook, Google, YouTube, Twitter etc is seen as ""the holy grail"" as an ML engineer/ researcher.  The best paid (and therefore probably some of the most skilled) people in our field are working on thát. Not medicine, not science.. no, they work on recommender algorithms that act as catalysts for the worst in humanity, in turn for more ad revenue. A glaring (but fixed) example is a 13 year old girl watching diet videos will get anorexia videos recommended on YouTube, not because it's good for her, but because it maximizes the time she spends on YouTube to generate more ad revenue. And it works. Because it worked for thousands of other 13 year olds watching diet videos. 

 My apologies for a bit of a rant but I'm genuinely curious how other ML developers think about this. This is one of the biggest (or probably even THE biggest) impact that machine learning has on the world right now, yet I barely hear about it on this sub (I hope I'm wrong on this). 

Do you think people that developed these algorithms bear some responsibility? Do you think they knew the impact of their algorithms? And finally, maybe I'm wrong, but I feel like no one is discussing this here. Why is that?",624,192,d is there a ml community blind eye toward the negative impact of faang recommendation algorithms on global society,-0.7506
[D]Someone copied parts of my code and changed the license,"Hey there,

Let's get straight to the point : yesterday, NVIDIA released an open source[ pytorch implementation of flownet2](https://github.com/NVIDIA/flownet2-pytorch), which released a CUDA version of the correlation layer introduced by the paper [FlowNet](https://arxiv.org/abs/1504.06852). It turns out out that this code is protected by NVIDIA copyright while it heavily reuse parts of a code I wrote myslef 6 months ago : [FlowNet Pytorch](https://github.com/ClementPinard/FlowNetPytorch)

My goal is not to rant or to fulfil my self esteem, but to figure what to do in the most pragmatic manner in order to take the best of both worlds and make the best implementation possible.

That's not the most important part, but as a proof, here are some comparisons you can make :

[mine](https://github.com/ClementPinard/FlowNetPytorch/blob/607f99f46be3eccbd9b07c73848a68bc12156392/multiscaleloss.py#L8) - [theirs](https://github.com/NVIDIA/flownet2-pytorch/blob/master/losses.py#L46)

[mine](https://github.com/ClementPinard/FlowNetPytorch/blob/5381bd5c699b850785ab5dec6fda523b9126c912/models/FlowNetS.py#L32) - [theirs](https://github.com/NVIDIA/flownet2-pytorch/blob/master/networks/FlowNetS.py#L11)

[mine](https://github.com/ClementPinard/FlowNetPytorch/blob/5381bd5c699b850785ab5dec6fda523b9126c912/models/FlowNetS.py#L9) - [theirs](https://github.com/NVIDIA/flownet2-pytorch/blob/master/networks/submodules.py#L7)

Now as a disclaimer, I am very honoured they decided to use my code, and it is very obvious that my code is not rocket science and the main contribution of this project is not these little snippets but rather the custom layers and the pretrained weights for pytorch.

However, the fact that the README is not giving any credit for what I did feels a little uncool, especially with a [License file](https://github.com/NVIDIA/flownet2-pytorch/blob/master/LICENSE) saying that all copyright goes to NVIDIA.

My other concern is that the parts of the code that got copied were actually not very well written, and the implementation in my own repo is to my mind much better now (for example [`MulstiScaleLoss`](https://github.com/NVIDIA/flownet2-pytorch/blob/master/losses.py#L46) module is a nightmare to read and to use while pytorch gives tools for making it [much more readable](https://github.com/ClementPinard/FlowNetPytorch/blob/master/multiscaleloss.py#L15)). I could make several Pull Requests but it's not garanteed to be merged rapidly and I'd prefer to contact the author first to get things straight and make them know that all I want is the best flownet2 implementation, and as this project is already gaining a lot of stars, it would be pointless to do my own fork ^with ^blackjack ^and ^hookers

My huge mistake was maybe to not have put a License in my code in the first place, but apparently, [a default one still holds](https://help.github.com/articles/licensing-a-repository/#choosing-the-right-license).

So what would be the best to do to get to work constructively with the project authors to improve their implementation and maybe also get a little credit for the code on which they built this project ? (also, is my claim reasonable ?)

Thanks in advance for your help !

EDIT thanks for your comments, I'll contact the main committor of the repo and hopefully everything will be alright! I am glad to see that it was indeed a reasonable claim

EDIT2 matter is solved for me, I got in touch with them quickly, thanks everyone for your help !",618,71,dsomeone copied parts of my code and changed the license,0.0
[N] GitHub and OpenAI release Copilot: an AI pair programmer,"Link to copilot: https://copilot.github.com/   

It is currently being made available as a VSCode extension. Relevant description from the website: 

> **What is GitHub Copilot?**
> GitHub Copilot is an AI pair programmer that helps you write code faster and with less work. GitHub Copilot draws context from comments and code, and suggests individual lines and whole functions instantly. GitHub Copilot is powered by OpenAI Codex, a new AI system created by OpenAI. The GitHub Copilot technical preview is available as a Visual Studio Code extension.

> **How good is GitHub Copilot?**
> We recently benchmarked against a set of Python functions that have good test coverage in open source repos. We blanked out the function bodies and asked GitHub Copilot to fill them in. The model got this right 43% of the time on the first try, and 57% of the time when allowed 10 attempts. And it’s getting smarter all the time.

The service is based on OpenAI's Codex model, which has not been released yet but [Greg Brockman (OpenAI CTO) tweeted that it will be made available through their API later this summer](https://twitter.com/gdb/status/1409890354132750336?s=20)",620,80,n github and openai release copilot an ai pair programmer,0.0
"[P] Papers with Code Update: Indexing 3,000+ ML Datasets","Hi all, we’ve launched an index of over 3,000 ML datasets. It’s our first step to make research datasets more discoverable. With the new feature you can:

* browse datasets by task (f.e., [Question Answering](https://paperswithcode.com/datasets?task=question-answering), [Semantic Segmentation](https://paperswithcode.com/datasets?task=semantic-segmentation)), modality (f.e., [Videos](https://paperswithcode.com/datasets?mod=videos), [3D](https://paperswithcode.com/datasets?mod=3d)) or language (f.e., [English](https://paperswithcode.com/datasets?lang=english), [Chinese](https://paperswithcode.com/datasets?lang=chinese), [German](https://paperswithcode.com/datasets?lang=german), [French](https://paperswithcode.com/datasets?lang=french)),
* keep track of the newest datasets in your area of interests (f.e., [Visual Question Answering](https://paperswithcode.com/datasets?o=newest&task=visual-question-answering), [Autonomous Driving](https://paperswithcode.com/datasets?o=newest&task=autonomous-driving)),
* browse benchmarks evaluating on a particular dataset,
* discover similar datasets,
* view usage over time in open-access research papers.

We focus on datasets introduced in ML papers.

This is an open resource so you can edit and add new datasets. We welcome suggestions, comments and feedback.

Explore the catalogue here: [https://paperswithcode.com/datasets](https://paperswithcode.com/datasets).",620,20,p papers with code update indexing  ml datasets,0.0
[P] Illustrated Deep Learning cheatsheets covering Stanford's CS 230 class,"Set of illustrated Deep Learning cheatsheets covering the content of Stanford's CS 230 class:

* Convolutional Neural Networks: [https://stanford.edu/\~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks)
* Recurrent Neural Networks: [https://stanford.edu/\~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks)
* Tips and tricks: [https://stanford.edu/\~shervine/teaching/cs-230/cheatsheet-deep-learning-tips-and-tricks](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-deep-learning-tips-and-tricks)

[Web version](https://preview.redd.it/1qve59a40x021.png?width=2116&format=png&auto=webp&s=f196f1a94d20c762270e88f182fb3c4c8c9d5f04)

&#x200B;

All the above in PDF format: [https://github.com/afshinea/stanford-cs-230-deep-learning](https://github.com/afshinea/stanford-cs-230-deep-learning)

[PDF version](https://preview.redd.it/636lrf1vyw021.png?width=2388&format=png&auto=webp&s=2cac4b4bc148992d7d66c38ac0fc9a71418d05fb)",613,26,p illustrated deep learning cheatsheets covering stanfords cs  class,0.0
"[D] IEEE bans Huawei employees from reviewing or handling papers for IEEE journals, some people resign from IEEE editorial board as a result","This is because US government has placed Huawei on the ""Entity List"".

&#x200B;

The news broke here: [https://twitter.com/qian\_junhui/status/1133595554905124869](https://twitter.com/qian_junhui/status/1133595554905124869)

&#x200B;

Here is Prof. Zhang's (from Peking University) resignation letter from IEEE NANO: [https://twitter.com/qian\_junhui/status/1133657229561802752](https://twitter.com/qian_junhui/status/1133657229561802752)",603,180,d ieee bans huawei employees from reviewing or handling papers for ieee journals some people resign from ieee editorial board as a result,-0.34
"[P] The weights neccessary to construct Vicuna, a fine-tuned LLM with capabilities comparable to GPT3.5, has now been released","Vicuna is a large language model derived from LLaMA, that has been fine-tuned to the point of having 90% ChatGPT quality. The delta-weights, necessary to reconstruct the model from LLaMA weights have now been released, and can be used to build your own Vicuna.

https://vicuna.lmsys.org/",608,82,p the weights neccessary to construct vicuna a finetuned llm with capabilities comparable to gpt has now been released,0.0
"[D] New Reddit API terms effectively bans all use for training AI models, including research use.","Reddit has updated their terms of use for their data API. I know this is a popular tool in the machine learning research community, and the new API unfortunately impacts this sort of usage.

Here are the new terms: [https://www.redditinc.com/policies/data-api-terms](https://www.redditinc.com/policies/data-api-terms) . Section 2.4 now specifically calls out machine learning as an unapproved usage unless you get the permission of each individual user. The previous version of this clause read:

' You will comply with any requirements or restrictions imposed on usage of User Content by their respective owners, which may include ""all rights reserved"" notices, Creative Commons licenses or other terms and conditions that may be agreed upon between you and the owners.'

Which didn't mention machine learning usage, leaving it to fall under existing laws around this in the situation where a specific restriction is not claimed. The new text adds the following:

'Except as expressly permitted by this section, no other rights or licenses are granted or implied, including any right to use User Content for other purposes, such as for training a machine learning or AI model, without the express permission of rightsholders in the applicable User Content.'

which now explicitly requires you to get permissions from the rightsholder for each user. 

I've sent a note to their API support about the implications of this, especially to the research community. You may want to do the same if this concerns you.",594,163,d new reddit api terms effectively bans all use for training ai models including research use,0.4404
[R] Hello Dolly: Democratizing the magic of ChatGPT with open models,"Databricks shows that anyone can take a dated off-the-shelf open source large language model (LLM) and give it magical ChatGPT-like instruction following ability by training it in less than three hours on one machine, using high-quality training data.

They fine tuned GPT-J using the Alpaca dataset.

Blog: [https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html](https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html)  
Github: [https://github.com/databrickslabs/dolly](https://github.com/databrickslabs/dolly)",603,108,r hello dolly democratizing the magic of chatgpt with open models,0.0
[P] Mathematics for Machine Learning - Sharing my solutions,"Just finished studying [Mathematics for Machine Learning (MML)](https://mml-book.github.io/). Amazing resource for anyone teaching themselves ML.

Sharing my exercise solutions in case anyone else finds helpful (I really wish I had them when I started).

[https://github.com/ilmoi/MML-Book](https://github.com/ilmoi/MML-Book)",599,67,p mathematics for machine learning  sharing my solutions,0.5423
"[Project] I've compiled weather/climate date for the confirmed COVID19 infection sites, if anyone wants it","Hello there.

 

I'm not a machine learning guy (perhaps one day!), but it was suggested to me that some of you may want a crack at this data.

Using JHU's time\_series\_19-covid-Confirmed.csv csv format, and going back to 1/1/20, using Dark Sky's API, I went and grabbed the following pieces of data for each day for each site:

* Cloud cover
* Dew point
* Relative humidity
* Ozone
* Precipitation probability
* Air pressure
* Sunrise time
* Sunset time
* Max temperature
* Min temperature
* UV index
* Wind speed

These are all recorded as CSV files in the /csv folder.

If any of you want to use this to take a crack at trying to figure out if any of these factors play into the spread of the virus, by all means, please do so. You can correlate my values with JHU's numbers in terms of rate of spread and all that from their repository that I branched off of. The big caveat here is that I'm just a guy, and none of my data have been audited or validated or anything, but at least it's something, I guess.

&#x200B;

 [Here is my git repository](https://github.com/imantsm/COVID-19)",598,57,project ive compiled weatherclimate date for the confirmed covid infection sites if anyone wants it,0.0
[P] SoulsGym - Beating Dark Souls III Bosses with Deep Reinforcement Learning,"# The project

I've been working on a new gym environment for quite a while, and I think it's finally at a point where I can share it. SoulsGym is an OpenAI gym extension for Dark Souls III. It allows you to train reinforcement learning agents on the bosses in the game. The Souls games are widely known in the video game community for being notoriously hard.

.. Ah, and this is my first post on r/MachineLearning, so please be gentle ;)

# What is included?

**SoulsGym**

There are really two parts to this project. The first one is [SoulsGym](https://github.com/amacati/SoulsGym), an OpenAI gym extension. It is compatible with the newest API changes after gym has transitioned to the Farama foundation. SoulsGym is essentially a game hacking layer that turns Dark Souls III into a gym environment that can be controlled with Python. However, you still need to own the game on Steam and run it before starting the gym. A detailed description on how to set everything up can be found in the package [documentation](https://soulsgym.readthedocs.io/en/latest/?badge=latest).

**Warning: If you want to try this gym, be sure that you have read the documentation and understood everything. If not handled properly, you can get banned from multiplayer.**

Below, you can find a video of an agent training in the game. The game runs on 3x speed to accelerate training. You can also watch the video on [YouTube](https://www.youtube.com/watch?v=7R5Ef69sFPE).

&#x200B;

[RL agent learning to defeat the first boss in Dark Souls III.](https://reddit.com/link/134r0xf/video/o6ctdppeo8xa1/player)

At this point, only the first boss in Dark Souls III is implemented as an environment. Nevertheless, SoulsGym can easily be extended to include other bosses in the game. Due to their similarity, it shouldn't be too hard to even extend the package to Elden Ring as well. If there is any interest in this in the ML/DS community, I'd be happy to give the other ones a shot ;)

**SoulsAI**

The second part is [SoulsAI](https://github.com/amacati/SoulsAI), a distributed deep reinforcement learning framework that I wrote to train on multiple clients simultaneously. You should be able to use it for other gym environments as well, but it was primarily designed for my rather special use case. SoulsAI enables live-monitoring of the current training setup via a webserver, is resilient to client disconnects and crashes, and contains all my training scripts. While this sounds a bit hacky, it's actually quite readable. You can find a complete documentation that goes into how everything works [here](https://soulsai.readthedocs.io/en/latest/).

Being fault tolerant is necessary since the simulator at the heart of SoulsGym is a game that does not expose any APIs and has to be hacked instead. Crashes and other instabilities are rare, but can happen when training over several days. At this moment, SoulsAI implements ApeX style DQN and PPO, but since PPO is synchronous, it is less robust to client crashes etc. Both implementations use Redis as communication backend to send training samples from worker clients to a centralized training server, and to broadcast model updates from the server to all clients. For DQN, SoulsAI is completely asynchronous, so that clients never have to stop playing in order to perform updates or send samples.

&#x200B;

[Live monitoring of an ongoing training process in SoulsAI.](https://preview.redd.it/9m060w00r8xa1.png?width=1800&format=png&auto=webp&s=abb9c15ce38c99cba9753db95ac9dfc7eeec75a5)

Note: I have not implemented more advanced training algorithms such as Rainbow etc., so it's very likely that one can achieve faster convergence with better performance. Furthermore, hyperparameter tuning is extremely challenging since training runs can easily take days across multiple machines.

# Does this actually work?

Yes, it does! It took me some time, but I was able to train an agent with Duelling Double Deep Q-Learning that has a win rate of about 45% within a few days of training. In this video you can see the trained agent playing against Iudex Gundry. You can also watch the video on [YouTube](https://www.youtube.com/watch?v=86NivRglr3Y).

&#x200B;

[RL bot vs Dark Souls III boss.](https://reddit.com/link/134r0xf/video/rkor3hroj8xa1/player)

I'm also working on a visualisation that shows the agent's policy networks reacting to the current game input. You can see a preview without the game simultaneously running here. Credit for the idea of visualisation goes to [Marijn van Vliet](https://github.com/wmvanvliet/scns).

&#x200B;

[Duelling Double Q-Learning networks reacting to changes in the game observations.](https://reddit.com/link/134r0xf/video/b0a4jzczv8xa1/player)

If you really want to dive deep into the hyperparameters that I used or load the trained policies on your machine, you can find the final checkpoints [here](https://drive.google.com/drive/folders/1cAK1TbY4e4HE4cxyAFEHRpj6MOgp5Zxe?usp=sharing). The hyperparameters are contained in the *config.json* file.

# ... But why?

Because it is a ton of fun! Training to defeat a boss in a computer game does not advance the state of the art in RL, sure. So why do it? Well, because we can! And because maybe it excites others about ML/RL/DL.

**Disclaimer: Online multiplayer**

This project is in no way oriented towards creating multiplayer bots. It would take you ages of development and training time to learn a multiplayer AI starting from my package, so just don't even try. I also do not take any precautions against cheat detections, so if you use this package while being online, you'd probably be banned within a few hours.

# Final comments

As you might guess, this project went through many iterations and it took a lot of effort to get it ""right"". I'm kind of proud to have achieved it in the end, and am happy to explain more about how things work if anyone is interested. There is a lot that I haven't covered in this post (it's really just the surface), but you can find more in the docs I linked or by writing me a pm. Also, I really have no idea how many people in ML are also active in the gaming community, but if you are a Souls fan and you want to contribute by adding other Souls games or bosses, feel free to reach out to me.

Edit: Clarified some paragraphs, added note for online multiplayer.

Edit2: Added hyperparameters and network weights.",599,74,p soulsgym  beating dark souls iii bosses with deep reinforcement learning,-0.4588
[D] My Machine Learning Research Job Interview Experience,"Hi guys,

it seems like a lot of people have questions about finding jobs in ML, or what the typical interview process looks like. Since I've gone through all that recently, I thought it might be helpful to share my experiences:

[https://generalizederror.github.io/My-Machine-Learning-Research-Jobhunt/](https://generalizederror.github.io/My-Machine-Learning-Research-Jobhunt/)

Enjoy",599,120,d my machine learning research job interview experience,0.0
[D] Jurgen Schmidhuber really had GANs in 1990,"he did not call it GAN, he called it curiosity, it's actually famous work, many citations in all the papers on intrinsic motivation and exploration, although I bet many GAN people don't know this yet

I learned about it through his [inaugural tweet](https://twitter.com/SchmidhuberAI) on their [miraculous year](http://people.idsia.ch/~juergen/deep-learning-miraculous-year-1990-1991.html). I knew LSTM, but I did not know that he and Sepp Hochreiter did all those other things 30 years ago. 

The blog sums it up in section 5 Artificial Curiosity Through Adversarial Generative Neural Networks (1990)

> The first NN is called the controller C. C (probabilistically) generates outputs that may influence an environment. The second NN is called the world model M. It predicts the environmental reactions to C's outputs. Using gradient descent, M minimises its error, thus becoming a better predictor. But in a zero sum game, C tries to find outputs that maximise the error of M. M's loss is the gain of C.  

> That is, C is motivated to invent novel outputs or experiments that yield data that M still finds surprising, until the data becomes familiar and eventually boring. Compare more recent summaries and extensions of this principle, e.g., [AC09]. 

> GANs are an application of Adversarial Curiosity [AC90] where the environment simply returns whether C's current output is in a given set [AC19].

So I read those referenced papers. [AC19](https://arxiv.org/abs/1906.04493) is kinda modern guide to the old report [AC90](http://people.idsia.ch/~juergen/FKI-126-90ocr.pdf) where the adversarial part first appeared in section: Implementing Dynamic Curiosity and Boredom, and the generative part in section: Explicit Random Actions versus Imported Randomness, which is like GANs versus conditional GANs. [AC09](http://people.idsia.ch/~juergen/multipleways2009.pdf) is a survey from 2009 and sums it up: maximise reward for prediction error.

I know that Ian Goodfellow says he is the inventor of GANs, but he must have been a little boy when Jurgen did this in 1990. Also funny that Yann LeCun described GANs as ""the coolest idea in machine learning in the last twenty years"" although Jurgen had it thirty years ago  

No, it is NOT the same as predictability minimisation, that's yet another adversarial game he invented, in 1991, section 7 of his [explosive blog post which contains additional jaw-droppers](http://people.idsia.ch/~juergen/deep-learning-miraculous-year-1990-1991.html)",599,152,d jurgen schmidhuber really had gans in ,0.0
AlphaGO WINS!,I feel so happy. ,594,266,alphago wins,0.6932
"[P] Japanese genetic algorithm experiment to make a ""pornographic"" image","I don't have anything to do with this project myself, I've just been following it because I found it interesting and figured I'd share.

[This guy](https://twitter.com/miseromisero) made a [project](https://gamingchahan.com/ecchi/) where anyone is welcome to look at two images and choose which one they think is more ""pornographic"" to train the AI. There isn't really a goal, but it started out with the guy saying that the project ""wins"" when Google Adsense deems the image to be pornographic.

The project ""won"" [today](https://twitter.com/miseromisero/status/1359790904513466369) with the 11225th iteration getting Google to limit the Adsense account tied to the project. That being said it's still ongoing.

You can also take a look at all previous iterations of the image [here](https://gamingchahan.com/ecchi/exhi/)

I wouldn't consider the current version to be NSFW myself as it's still pretty abstract but YMMV (Google certainly seems to think differently at least)",593,68,p japanese genetic algorithm experiment to make a pornographic image,0.0
[D] Yet another rant on PhD Applications,"I guess this is kind of a rant about PhD admissions, specifically in ML and theoretical CS.

<rant>  


I recently applied to several top PhD programs, and so far I've been rejected from Berkeley, University of Washington, Columbia, Stanford, and MIT. I am expecting that I'll be rejected from the remaining programs soon. I didn't even get an interview chance, I was just rejected without speaking to anyone.

I'll start with my profile (which I am willing to verify on a zoom call if any mod requests it). I grew up in a poor city in a third world country, to a very poor family. I managed to work hard during high school, ranking 3rd in my country in national exams, and got accepted on a full ride scholarship to a Hong Kong university. I have a GPA of 3.9+. I have a first author NeurIPS paper that was completed without any faculty advisors (Me and another undergraduate wrote the paper independently and it got accepted). I also have a paper in an A\* information theory conference where we settled an open problem that has been open for 8 years. I have two submissions in TCS and IEEE Transactions on information theory (both A\* journals), and one has already received a minor revision (on its way to be accepted). During my undergrad, my mother got breast cancer, and I had to work two part time jobs just to help with paying for the medical bills, while keeping up with my studies and my research. I remember I slept an average of 5 hours per day in the months of treatment. I have seen two of my LORs, both professors mentioned that I am the best undergraduate who has worked with them in their lifetime as Professors.

I feel tired, mentally exhausted, and crushed. I've worked so hard over the last 8 years, just to have all my dreams destroyed. It doesn't help when everyone around me keeps saying I am ""a shoo-in for Stanford"". I just feel like I've been fighting an uphill fight all my life with no guidance, constantly having to work harder just to prove myself, and in the end, it still didn't work. I just don't understand what these top programs are looking for. I heard some programs like UWashington even interviewed the top 20% of applicants, which means I'm not even close.

</rant>

Edit: [This](https://www.reddit.com/r/MachineLearning/comments/lpt9xb/d_re_yet_another_rant_on_phd_applications/)  
",594,241,d yet another rant on phd applications,-0.34
[D] 10 hard-earned lessons from shipping generative AI products over the past 18 months,"Hey all,

I'm the founder of a generative AI consultancy and we build gen AI powered products for other companies. We've been doing this for 18 months now and I thought I share our learnings - it might help others.

&#x200B;

1. It's a never ending battle to keep up with the latest tools and developments.  


2. By the time you ship your product it's already using an outdated tech-stack.  


3. There are no best-practices yet. You need to make a bet on tools/processes and hope that things won't change much by the time you ship (they will, see point 2).  


4. If your generative AI product doesn't have a VC-backed competitor, there will be one soon.  


5. In order to win you need one of the two things: either (1) the best distribution or (2) the generative AI component is hidden in your product so others don't/can't copy you.  


6. AI researchers / data scientists are suboptimal choice for AI engineering. They're expensive, won't be able to solve most of your problems and likely want to focus on more fundamental problems rather than building products.  


7. Software engineers make the best AI engineers. They are able to solve 80% of your problems right away and they are motivated because they can ""work in AI"".  


8. Product designers need to get more technical, AI engineers need to get more product-oriented. The gap currently is too big and this leads to all sorts of problems during product development.  


9. Demo bias is real and it makes it 10x harder to deliver something that's in alignment with your client's expectation. Communicating this effectively is a real and underrated skill.  


10. There's no such thing as off-the-shelf AI generated content yet. Current tools are not reliable enough, they hallucinate, make up stuff and produce inconsistent results (applies to text, voice, image and video).",598,166,d  hardearned lessons from shipping generative ai products over the past  months,0.0
[D] Google is applying BERT to Search,"Understanding searches better than ever before

If there’s one thing I’ve learned over the 15 years working on Google Search, it’s that people’s curiosity is endless. We see billions of searches every day, and 15 percent of those queries are ones we haven’t seen before--so we’ve built ways to return results for queries we can’t anticipate.

When people like you or I come to Search, we aren’t always quite sure about the best way to formulate a query. We might not know the right words to use, or how to spell something, because often times, we come to Search looking to learn--we don’t necessarily have the knowledge to begin with. 

At its core, Search is about understanding language. It’s our job to figure out what you’re searching for and surface helpful information from the web, no matter how you spell or combine the words in your query. While we’ve continued to improve our language understanding capabilities over the years, we sometimes still don’t quite get it right, particularly with complex or conversational queries. In fact, that’s one of the reasons why people often use “keyword-ese,” typing strings of words that they think we’ll understand, but aren’t actually how they’d naturally ask a question. 

With the latest advancements from our research team in the science of language understanding--made possible by machine learning--we’re making a significant improvement to how we understand queries, representing the biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search. 

**Applying BERT models to Search**  
Last year, we [introduced and open-sourced](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html) a neural network-based technique for natural language processing (NLP) pre-training called Bidirectional Encoder Representations from Transformers, or as we call it--[BERT](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html), for short. This technology enables anyone to train their own state-of-the-art question answering system. 

This breakthrough was the result of Google research on [transformers](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html): models that process words in relation to all the other words in a sentence, rather than one-by-one in order. BERT models can therefore consider the full context of a word by looking at the words that come before and after it—particularly useful for understanding the intent behind search queries.

But it’s not just advancements in software that can make this possible: we needed new hardware too. Some of the models we can build with BERT are so complex that they push the limits of what we can do using traditional hardware, so for the first time we’re using the latest [Cloud TPUs ](https://cloud.google.com/blog/products/ai-machine-learning/cloud-tpu-pods-break-ai-training-records)to serve search results and get you more relevant information quickly. 

**Cracking your queries**  
So that’s a lot of technical details, but what does it all mean for you? Well, by applying BERT models to both ranking and featured snippets in Search, we’re able to do a much better job  helping you find useful information. In fact, when it comes to ranking results, BERT will help Search better understand one in 10 searches in the U.S. in English, and we’ll bring this to more languages and locales over time.

Particularly for longer, more conversational queries, or searches where prepositions like “for” and “to” matter a lot to the meaning, Search will be able to understand the context of the words in your query. You can search in a way that feels natural for you.

To launch these improvements, we did a lot of [testing](https://www.google.com/search/howsearchworks/mission/users/) to ensure that the changes actually are more helpful. Here are some of the examples that showed up our evaluation process that demonstrate BERT’s ability to understand the intent behind your search.  


Here’s a search for “2019 brazil traveler to usa need a visa.” The word “to” and its relationship to the other words in the query are particularly important to understanding the meaning. It’s about a Brazilian traveling to the U.S., and not the other way around. Previously, our algorithms wouldn't understand the importance of this connection, and we returned results about U.S. citizens traveling to Brazil. With BERT, Search is able to grasp this nuance and know that the very common word “to” actually matters a lot here, and we can provide a much more relevant result for this query.

Let’s look at another query: “do estheticians stand a lot at work.” Previously, our systems were taking an approach of matching keywords, matching the term “stand-alone” in the result with the word “stand” in the query. But that isn’t the right use of the word “stand” in context. Our BERT models, on the other hand, understand that “stand” is related to the concept of the physical demands of a job, and displays a more useful response.

Here are some other examples where BERT has helped us grasp the subtle nuances of language that computers don’t quite understand the way humans do.

**Improving Search in more languages**  
We’re also applying BERT to make Search better for people across the world. A powerful characteristic of these systems is that they can take learnings from one language and apply them to others. So we can take models that learn from improvements in English (a language where the vast majority of web content exists) and apply them to other languages. This helps us better return relevant results in the many languages that Search is offered in.

For featured snippets, we’re using a BERT model to improve featured snippets in the two dozen countries where this feature is available, and seeing significant improvements in languages like Korean, Hindi and Portuguese.

**Search is not a solved problem**  
No matter what you’re looking for, or what language you speak, we hope you’re able to let go of some of your keyword-ese and search in a way that feels natural for you. But you’ll still stump Google from time to time. Even with BERT, we don’t always get it right. If you search for “what state is south of Nebraska,” BERT’s best guess is a community called “South Nebraska.” (If you've got a feeling it's not in Kansas, you're right.)

Language understanding remains an ongoing challenge, and it keeps us motivated to continue to improve Search. We’re always getting better and working to find the meaning in-- and most helpful information for-- every query you send our way.

[Source](https://blog.google/products/search/search-language-understanding-bert/)",594,55,d google is applying bert to search,0.0
[N] Google now uses BERT on almost every English query,"[Google: BERT now used on almost every English query](https://searchengineland.com/google-bert-used-on-almost-every-english-query-342193) (October 2020)

>BERT powers almost every single English based query done on Google Search, the company said during its virtual Search on 2020 event Thursday. That’s up from just 10% of English queries when Google first announced the use of the BERT algorithm in Search last October.

DeepRank is Google's internal project name for its use of BERT in search. There are other technologies that use the same name.

Google had already been using machine learning in search via [RankBrain](https://searchengineland.com/faq-all-about-the-new-google-rankbrain-algorithm-234440) since at least sometime in 2015.

Related:

[Understanding searches better than ever before](https://blog.google/products/search/search-language-understanding-bert/) (2019)

[BERT, DeepRank and Passage Indexing… the Holy Grail of Search?](https://inspiremelabs.com/bert-deeprank-passage-indexing/) (2020)

>*Here’s my brief take on how DeepRank will match up with Passage Indexing, and thus open up the doors to the holy grail of search finally.*  
>  
>Google will use Deep Learning to understand each sentence and paragraph and the meaning behind these paragraphs and now match up your search query meaning with the paragraph that is giving the best answer after Google understands the meaning of what each paragraph is saying on the web, and then Google will show you just that paragraph with your answer!  
>  
>This will be like a two-way match… the algorithm will have to process every sentence and paragraph and page with the DeepRank (Deep Learning algorithm) to understand its context and store it not just in a simple word-mapped index but in some kind-of database that understands what each sentence is about so it can serve it out to a query that is processed and understood.  
>  
>This kind of processing will require tremendous computing resources but there is no other company set up for this kind of computing power than Google!

[\[D\] Google is applying BERT to Search](https://www.reddit.com/r/MachineLearning/comments/dn6xrr/d_google_is_applying_bert_to_search/) (2019)

[\[D\] Does anyone know how exactly Google incorporated Bert into their search engines?](https://www.reddit.com/r/MachineLearning/comments/f9qgmt/d_does_anyone_know_how_exactly_google/) (2020)

**Update: added link below.**

[Part of video from Google about use of NLP and BERT in search](https://youtu.be/tFq6Q_muwG0?t=2512) (2020). I didn't notice any technical revelations in this part of the video, except perhaps that the use of BERT in search uses a lot of compute.

**Update: added link below.**

[Could Google passage indexing be leveraging BERT?](https://searchengineland.com/could-google-passage-indexing-be-leveraging-bert-342975) (2020). This article is a deep dive with 30 references.

>The “passage indexing” announcement caused some confusion in the SEO community with several interpreting the change initially as an “indexing” one.  
>  
>A natural assumption to make since the name “passage indexing” implies…erm… “passage” and “indexing.”  
>  
>Naturally some SEOs questioned whether individual passages would be added to the index rather than individual pages, but, not so, it seems, since Google have clarified the forthcoming update actually relates to a passage ranking issue, rather than an indexing issue.  
>  
>“We’ve recently made a breakthrough in ranking and are now able to not just index web pages, but individual passages from the pages,” Raghavan explained. “By better understanding the relevancy of specific passages, not just the overall page, we can find that needle-in-a-haystack information you’re looking for.”  
>  
>This change is about ranking, rather than indexing per say.

**Update: added link below.**

[A deep dive into BERT: How BERT launched a rocket into natural language understanding](https://searchengineland.com/a-deep-dive-into-bert-how-bert-launched-a-rocket-into-natural-language-understanding-324522) (2019)",592,61,n google now uses bert on almost every english query,0.0
[D] ICLR 2020 REJECTION RAGE THREAD,"CAPS ONLY

PEOPLE WITH ACCEPTED PAPERS ARE NOT WELCOME",588,92,d iclr  rejection rage thread,-0.8613
"[D] Your salary is determined mainly by geography, not your skill level (conclusions from the salary model built with 24k samples and 300 questions)","I have built a model that predicts the salary of Data Scientists / Machine Learning Engineers based on 23,997 responses and 294 questions from a 2022 Kaggle Machine Learning & Data Science Survey (Source: [https://jobs-in-data.com/salary/data-scientist-salary](https://jobs-in-data.com/salary/data-scientist-salary))

I have studied the feature importances from the LGBM model.

TL;DR: Country of residence is **an order of magnitude more important** than anything else (including your experience, job title or the industry you work in). So - if you want to follow the famous ""work smart not hard"" - the key question seems to be how to optimize the geography aspect of your career above all else.

The model was built for data professions, but IMO it applies also to other professions as well.

&#x200B;

https://preview.redd.it/6b9r67lctfqc1.png?width=1200&format=png&auto=webp&s=73b437e43c754ede0b19e42d95655edd4b5adc95",585,208,d your salary is determined mainly by geography not your skill level conclusions from the salary model built with k samples and  questions,0.34
[R] Do You Even Need Attention? A Stack of Feed-Forward Layers Does Surprisingly Well on ImageNet,"TL;DR: Got scooped by MLP-Mixer, so I'm releasing my writeup/code/models. I hope someone finds them interesting/useful.

Lately I've been trying a couple variants of simple vision transformers to better understand what makes them perform well. About a month ago, I found that you could replace the attention layers with feed-forward layers and get quite good results. Last week I started a short writeup of the experiment (just a few pages, as I didn't see it as a full paper).

Today Google put out a paper (MLP-Mixer) that proposes exactly the same architecture.

When I saw the paper earlier today I considered scrapping what I had done, but now I figure that I might as well just put it out there.

For those who are interested, here's a [GitHub repo](https://github.com/lukemelas/do-you-even-need-attention) with pretrained models, a [W&B log](https://wandb.ai/lukemelas2/deit-experiments/reports/Do-You-Even-Need-Attention---Vmlldzo2NjUxMzI?accessToken=8kebvweue0gd1s6qiav2orco97v85glogsi8i83576j42bb1g39e59px56lkk4zu) of the experiments, and a 3-page [writeup](https://github.com/lukemelas/do-you-even-need-attention/blob/main/Do-You-Even-Need-Attention.pdf).

Also, if anyone has stories about getting scooped, feel free to share -- I'd imagine people have some crazy stories.

Edit: Wow, thank you all for the support! I really didn't expect this. Based on your suggestions, I've also uploaded a version of the report to arXiv: [https://arxiv.org/abs/2105.02723](https://arxiv.org/abs/2105.02723) ",585,60,r do you even need attention a stack of feedforward layers does surprisingly well on imagenet,0.5106
"[D] There's a flaw/bug in Tensorflow that's preventing gradient updates to weights in custom layers of models created using the Keras functional API, leaving those weights basically frozen. Might be worth checking `model.trainable_variables`.","EDIT:

Someone replied to the issue, this is what was said:

>It looks like what's going on is:
The layers currently enter a 'functional api construction' mode only if all of the inputs in the first argument come from other Keras layers. However, you have None included in the inputs in the first positional arg, so it's not triggering functional api construction.

>That causes the layer to get 'inlined' in the outer functional model rather than correctly included. You should be able to work around this by changing the layer api so Nones should not get passed in.

>We have a major cleanup/refactoring of the Functional API mostly done that make the functional api triggering much clearer (if any symbolic values appear in the inputs) & sort out a number of other issues w/ it. But, that will only land in 2.4. It's not immediately obvious if we can squeeze a fix into tf 2.3 as the RC is already out.

If you look at the notebooks, the inputs to some of the lines look like this:

`    P_outputs = P_trans11((inputHiddenVals, None, None, None))[0]`

It looks like the issue is that the  are extra `None`s are causing disappearing variables issue, and a workaround could be just to have 

`    P_outputs = P_trans11(inputHiddenVals)[0]`




----

tl'dr: For anyone who has used the functional api with custom layers, it might be worth running


    for i, var in enumerate(model.trainable_variables):
        print(model.trainable_variables[i].name)
    

so see if all your weights are there. 

----

Using custom layers with the functional API results in missing weights in the `trainable_variables`. Those weights are not in the `non_trainable_variables` either. 

But if those weights aren't in `trainable_variables`they are essential frozen, since it is only those weights that receive gradient updates, as seen in the Keras model training code below:

https://github.com/tensorflow/tensorflow/blob/1fb8f4988d69237879aac4d9e3f268f837dc0221/tensorflow/python/keras/engine/training.py#L2729


      gradients = tape.gradient(loss, trainable_variables)
    
      # Whether to aggregate gradients outside of optimizer. This requires support
      # of the optimizer and doesn't work with ParameterServerStrategy and
      # CentralStroageStrategy.
      aggregate_grads_outside_optimizer = (
          optimizer._HAS_AGGREGATE_GRAD and  # pylint: disable=protected-access
          not isinstance(strategy.extended,
                         parameter_server_strategy.ParameterServerStrategyExtended))
    
      if aggregate_grads_outside_optimizer:
        # We aggregate gradients before unscaling them, in case a subclass of
        # LossScaleOptimizer all-reduces in fp16. All-reducing in fp16 can only be
        # done on scaled gradients, not unscaled gradients, for numeric stability.
        gradients = optimizer._aggregate_gradients(zip(gradients,  # pylint: disable=protected-access
                                                       trainable_variables))
      if isinstance(optimizer, lso.LossScaleOptimizer):
        gradients = optimizer.get_unscaled_gradients(gradients)
      gradients = optimizer._clip_gradients(gradients)  # pylint: disable=protected-access
      if trainable_variables:
        if aggregate_grads_outside_optimizer:
          optimizer.apply_gradients(
              zip(gradients, trainable_variables),
              experimental_aggregate_gradients=False)
        else:
          optimizer.apply_gradients(zip(gradients, trainable_variables))



The bug can be seen in this Colab gist 

https://colab.research.google.com/gist/Santosh-Gupta/40c54e5b76e3f522fa78da6a248b6826/missingtrainablevarsinference_var.ipynb

This gist uses the transformers library to create the models so its easy to see the bug. For an in-depth look, the colab gist below creates all the custom layers from scratch

https://colab.research.google.com/gist/Santosh-Gupta/aa34086a72956600910976e4f7ebe323/model_weight_debug_scratch_public_inference_var.ipynb


As you can see in the notebooks, a workaround is to create models using keras subclassing instead; model subclassing results in all the weights appearing in `trainable_variables`. To be absolutely sure that the functional API and subclasses models are exactly the same, I ran inference on them using the same input at the bottom of each notebook; the outputs for the models were exactly the same. But training using the functional API model would treat many of the weights as frozen (and there's no way to make them unfrozen since those weights aren't registered in the `non_trainable_variables` either). 

I've been looking at this for about a month, as far as I can tell, I don't think there was anything unique about the transformer layer I created; it may be the case that Any Keras model using custom sublayers and the functional API is prone to this. 

I put up a Github issue 24 days ago, but I can't tell if this is something being worked on. 

https://github.com/tensorflow/tensorflow/issues/40638

If anyone else has been using the Keras functional API with custom layer, would love to hear if you're also getting the same issue when you check the trainable variables.",578,96,d theres a flawbug in tensorflow thats preventing gradient updates to weights in custom layers of models created using the keras functional api leaving those weights basically frozen might be worth checking modeltrainablevariables,0.4215
[N] ‘The Godfather of A.I.’ Leaves Google and Warns of Danger Ahead,https://www.nytimes.com/2023/05/01/technology/ai-google-chatbot-engineer-quits-hinton.html,584,318,n the godfather of ai leaves google and warns of danger ahead,-0.5859
[D] Quitting machine learning for good,"Hi everyone,

I'm of those (rare??) persons who does ML for a living but has no interest in doing it. I've built models using classical and deep learning approaches for 7-8 years, and a lot of them had decent impacts in the companies I've worked with. I'm good at what I do and I'm compensated well for it. However, nothing in the field of ML/DL excites me anymore.

I find it more enjoyable to solve problems in my math textbooks . In fact, I want a career in which I can do some form of mathematics but I don't want to do machine learning for the rest of my life. Before I shifted to ML for the money, I worked a lot on satellite systems engineering. I also took a lot of physics and EE courses during my masters degree (optics, quantum mechanics and solid state devices).

I was thinking of a career in quantum information but I don't have a PhD yet. Also, my computer science skills aren't strong enough to switch to cryptography. Any thoughts / ideas on how to get out of machine learning?

&#x200B;

UPDATE: 2nd March, 2022 \[1\]- Thanks a lot everyone for your answers/comments. I'm overwhelmed and humbled by your responses. I'll reply to each one of you during this week or the next, whenever I find some time. I'm caught up in something personal.

\[2\] I came across this course recently - [http://groups.csail.mit.edu/gdpgroup/6838\_spring\_2021.html](http://groups.csail.mit.edu/gdpgroup/6838_spring_2021.html). This one looks super exciting. Here's a course on discrete differential geometry that I found online -  [https://www.cs.cmu.edu/\~kmcrane/Projects/DDG/](https://www.cs.cmu.edu/~kmcrane/Projects/DDG/). I'd love to explore differential geometry applied to ML problems (or vice versa).

\[3\]  I would prefer to work on ML in fields like applied physics or genetics rather than banking, social media analytics or consumer electronics.

\[4\] (This is a short rant)-  I'm sick of technical papers that have titles like ""X is all you need"" or ""Your classifier is secretly an energy based model and you should treat it like one"". I have nothing against anyone here and I'm absolutely certain that the authors are 100000x more knowledgable than I am but I'm very uncomfortable with such pompous paper titles. Correct me if I'm wrong but I haven't seen catchy titles in physics or mathematics. This is one (trivial) reason why I don't want to pursue a PhD in ML. I hate the grandeur and style!!

\[5\] Rant 2- Taking any online course from any platform does NOT make you a data scientist or an ML researcher. I hate the fact that not many of them are not willing to put in the time/effort to learn the fundamental math behind ML algorithms. When I ask someone in an interview to explain what PCA is, they stop with the answer that PCA is a dimensionality reduction technique. No word about eigenvalues/eigenvectors or covariance matrix. :(

&#x200B;

&#x200B;",580,170,d quitting machine learning for good,0.4404
[D] ML community against Putin,"I am a European ML PhD student and the news of a full-on Russian invasion has had a large impact on me. It is hard to do research and go on like you usually do when a war is escalating to unknown magnitudes. It makes me wonder how I can use my competency to help. Considering decentralized activist groups like the Anonymous hacker group, which supposedly has ""declared war on Russia"", are there any ideas for how the ML community may help using our skillset? I don't know much about cyber security or war, but I know there are a bunch of smart people here who might have ideas on how we can use AI or ML to help. I make this thread mainly to start a discussion/brain-storming session for people who, like me, want to make the life harder for that mf Putin.",582,185,d ml community against putin,0.0
[R] Tiny Language Models (below 10m parameters or only one transformer block) can generate paragraphs of coherent text and reason...provided training is limited to stories that only contain words that a typical 3 to 4-year-olds usually understand.,Paper - https://arxiv.org/abs/2305.07759,571,123,r tiny language models below m parameters or only one transformer block can generate paragraphs of coherent text and reasonprovided training is limited to stories that only contain words that a typical  to yearolds usually understand,-0.5859
[D] OpenAI introduces ChatGPT and Whisper APIs (ChatGPT API is 1/10th the cost of GPT-3 API),"https://openai.com/blog/introducing-chatgpt-and-whisper-apis

> It is priced at $0.002 per 1k tokens, which is 10x cheaper than our existing GPT-3.5 models.

This is a massive, massive deal. For context, the reason GPT-3 apps took off over the past few months before ChatGPT went viral is because a) text-davinci-003 was released and was a significant performance increase and b) the cost was cut from $0.06/1k tokens to $0.02/1k tokens, which made consumer applications feasible without a large upfront cost.

A much better model and a 1/10th cost warps the economics completely to the point that it may be better than in-house finetuned LLMs.

I have no idea how OpenAI can make money on this. This has to be a loss-leader to lock out competitors before they even get off the ground.",577,121,d openai introduces chatgpt and whisper apis chatgpt api is th the cost of gpt api,0.0
[N] OpenAI Switches to PyTorch,"""We're standardizing OpenAI's deep learning framework on PyTorch to increase our research productivity at scale on GPUs (and have just released a PyTorch version of Spinning Up in Deep RL)""

https://openai.com/blog/openai-pytorch/",570,119,n openai switches to pytorch,0.0
[R] The Illustrated Stable Diffusion,"Hi r/MachineLearning,

&#x200B;

Here's a visual description of how Stable Diffusion works, with over 30 original images covering diffusion models, latent diffusion models, CLIP and how it's trained, and more.

[https://jalammar.github.io/illustrated-stable-diffusion/](https://jalammar.github.io/illustrated-stable-diffusion/)

I appreciate all corrections and feedback.",569,31,r the illustrated stable diffusion,0.296
[P] I trained every single SOTA from 2021 and accidentally got a silver medal on Kaggle,"![](https://i.ibb.co/gwpJXBm/lb.png)


I trained every single SOTA model from 2021 and accidentally got a silver medal on an image classification competition on Kaggle recently (Pawpularity Contest). 

> [Here](https://www.kaggle.com/yamqwe/the-nuclear-option-train) If you are interested

The idea was to train every SOTA and then **Nuke the leaderboard with 10 Billion parameters** ensemble of ensembles. 
Some ensembles were also supplemented a bit with catboost 2nd stage model just for the ""why not"". 

**Outline of the approach: https://i.ibb.co/McJ39mW/image-nuke.png**

This stunt was done mainly for the purpose me catching up with the current most recent SOTA vision papers. 

I seriously didn't try to compete on the leaderboard and never had the intention of releasing a public notebook that actually gets a silver medal. 
This came as a complete surprise to me! 
Hope the solution will be useful for many others in the future.

If you got any questions or feedback, I'll be more than happy to discuss them!",571,34,p i trained every single sota from  and accidentally got a silver medal on kaggle,0.1779
"[P][R] A big update to Papers with Code: now with 2500+ leaderboards and 20,000+ results.","We made a big update to the Papers with Code database of results from papers, now with 2500+ leaderboards and 20,000+ results.

You can browse the new updated catalogue here:

[https://paperswithcode.com/sota](https://paperswithcode.com/sota)

This update was powered by our new annotation interface and our new ML research paper that allows us to automatically suggests ML results to extract from the paper. You can read more about these here:

[https://medium.com/paperswithcode/a-home-for-results-in-ml-e25681c598dc](https://medium.com/paperswithcode/a-home-for-results-in-ml-e25681c598dc)

and you can access the research here:

[https://arxiv.org/abs/2004.14356](https://arxiv.org/abs/2004.14356)

[https://paperswithcode.com/paper/axcell-automatic-extraction-of-results-from](https://paperswithcode.com/paper/axcell-automatic-extraction-of-results-from)

and see how the new interface looks like here:

[https://paperswithcode.com/paper/self-training-with-noisy-student-improves/review/](https://paperswithcode.com/paper/self-training-with-noisy-student-improves/review/)

The database is open for everyone to contribute.

All suggestions/comments/feedback welcome!",571,28,pr a big update to papers with code now with  leaderboards and  results,0.0
[R] Google DeepMind Diagnostic LLM Exceeds Human Doctor Top-10 Accuracy (59% vs 34%),"Researchers from Google and DeepMind have developed and evaluated an LLM fine-tuned specifically for clinical diagnostic reasoning. In a new study, they rigorously tested the LLM's aptitude for generating differential diagnoses and aiding physicians.

They assessed the LLM on 302 real-world case reports from the New England Journal of Medicine. These case reports are known to be highly complex diagnostic challenges.

The LLM produced differential diagnosis lists that included the final confirmed diagnosis in the top 10 possibilities in 177 out of 302 cases, a top-10 accuracy of 59%. **This significantly exceeded the performance of experienced physicians, who had a top-10 accuracy of just 34% on the same cases when unassisted.**

According to assessments from senior specialists, the LLM's differential diagnoses were also rated to be **substantially more appropriate and comprehensive** than those produced by physicians, when evaluated across all 302 case reports.

This research demonstrates the potential for LLMs to enhance physicians' clinical reasoning abilities for complex cases. However, the authors emphasize that further rigorous real-world testing is essential before clinical deployment. Issues around model safety, fairness, and robustness must also be addressed.

[**Full summary**](https://aimodels.substack.com/p/googles-new-llm-doctor-is-right-way). [**Paper**](https://arxiv.org/abs/2401.05654).",571,144,r google deepmind diagnostic llm exceeds human doctor top accuracy  vs ,0.0
[P] Datasets should behave like Git repositories,"Let's talk about datasets for machine learning that change over time.

In real-life projects, datasets are rarely static. They grow, change, and evolve over time. But this fact is not reflected in how most datasets are maintained. Taking inspiration from software dev, where codebases are managed using Git, we can create living Git repositories for our datasets as well.

This means the dataset becomes easily manageable, and sharing, collaborating, and updating downstream consumers of changes to the data can be done similar to how we manage PIP or NPM packages.

I wrote a blog about such a project, showcasing how to transform a dataset into a *living-dataset,* and use it in a machine learning project.

[https://dagshub.com/blog/datasets-should-behave-like-git-repositories/](https://dagshub.com/blog/datasets-should-behave-like-git-repositories/)

**Example project:**

The living dataset: [https://dagshub.com/Simon/baby-yoda-segmentation-dataset](https://dagshub.com/Simon/baby-yoda-segmentation-dataset)

A project using the living dataset as a dependency: [https://dagshub.com/Simon/baby-yoda-segmentor](https://dagshub.com/Simon/baby-yoda-segmentor)

Would love to hear your thoughts.

&#x200B;

https://preview.redd.it/cvpu2j7ovac61.png?width=588&format=png&auto=webp&s=15d1fe9cfacf282427e4394b3c729082710d2b99",567,108,p datasets should behave like git repositories,0.3612
[Discussion] (Rant) Most of us just pretend to understand Transformers,"I see a lot of people using the concept of Attention without really knowing what's going on inside the architecture and *why* it works rather than the *how*. Others just put up the picture of attention intensity where the word ""dog"" is ""attending"" the most to ""it"". People slap on a BERT in Kaggle competitions because, well, it is easy to do so, thanks to Huggingface without really knowing what even the abbreviation means. Ask a self-proclaimed person on LinkedIn about it and he will say oh it works on attention and masking and refuses to explain further.  I'm saying all this because after searching a while for ELI5-like explanations, all I could get is a trivial description.",560,180,discussion rant most of us just pretend to understand transformers,-0.4215
[N] Apple hires Ian Goodfellow,"*According to CNBC [article](https://www.cnbc.com/2019/04/04/apple-hires-ai-expert-ian-goodfellow-from-google.html):*

One of Google’s top A.I. people just joined Apple

- Ian Goodfellow joined Apple’s Special Projects Group as a director of machine learning last month.

- Prior to Google, he worked at OpenAI, an AI research consortium originally funded by Elon Musk and other tech notables.

- He is the father of an AI approach known as general adversarial networks, or GANs, and his research is widely cited in AI literature.

Ian Goodfellow, one of the top minds in artificial intelligence at Google, has joined Apple in a director role.

The hire comes as Apple increasingly strives to tap AI to boost its software and hardware. Last year Apple hired John Giannandrea, head of AI and search at Google, to supervise AI strategy.


Goodfellow updated his LinkedIn profile on Thursday to acknowledge that he moved from Google to Apple in March. He said he’s a director of machine learning in the Special Projects Group. In addition to developing AI for features like FaceID and Siri, Apple also has been working on autonomous driving technology. Recently the autonomous group had a round of layoffs.

A Google spokesperson confirmed his departure. Apple declined to comment. Goodfellow didn’t respond to a request for comment.

https://www.cnbc.com/2019/04/04/apple-hires-ai-expert-ian-goodfellow-from-google.html",564,168,n apple hires ian goodfellow,0.0
"[N] Hikvision marketed ML surveillance camera that automatically identifies Uyghurs, on its China website","News Article: https://ipvm.com/reports/hikvision-uyghur

h/t [James Vincent](https://twitter.com/jjvincent/status/1193935124582322182) who regularly reports about ML in The Verge.

The [article](https://ipvm.com/reports/hikvision-uyghur) contains a marketing image from Hikvision, the world's largest security camera company, that speaks volumes about the brutal simplicity of the techno-surveillance state.

The product feature is simple: Han ✅, Uyghur ❌

Hikvision is a regular sponsor of top ML conferences such as CVPR and ICCV, and have reportedly recruited research interns for their US-based research lab using [job posting](https://eccv2018.org/jobs/research-internship/) in ECCV. They have recently been added to a US government [blacklist](https://www.bloomberg.com/news/articles/2019-10-07/u-s-blacklists-eight-chinese-companies-including-hikvision-k1gvpq77), among other companies such as Shenzhen-based Dahua, Beijing-based Megvii (Face++) and Hong Kong-based Sensetime over human rights violation.

Should research conferences continue to allow these companies to sponsor booths at the events that can be used for recruiting?

https://ipvm.com/reports/hikvision-uyghur

(N.B. no, I *don't* work at Sensetime :)",556,93,n hikvision marketed ml surveillance camera that automatically identifies uyghurs on its china website,0.0
Theoretical Foundations of Graph Neural Networks [Research],"Hi all,

Recently I gave an invited talk at the University of Cambridge Computer Laboratory (my MA/PhD alma mater) on **Theoretical Foundations of Graph Neural Networks**. The recording is now live (+ slides in the description!): [https://www.youtube.com/watch?v=uF53xsT7mjc](https://www.youtube.com/watch?v=uF53xsT7mjc)

Here I have made efforts to derive GNNs from first principles, motivate their use across the sciences, and explain how they emerged, in parallel, along several research lines. This represents a 'convergence' of the \~4 years I've spent studying GNNs: I taught them in many ways over the years, and I feel like I have finally found, imho, the most 'natural' way to introduce them.

*(For the amazing insights in this direction, I need to give a shout-out to my ongoing collaborators: Joan Bruna, Michael Bronstein and Taco Cohen!)*

The live Zoom session attracted \~500 people, and I received many emails afterwards in support of the talk -- hence I believe it could be both of use to beginners in the area, and offer a new perspective to seasoned GNN practitioners. 

Please let me know if you found it useful, and of **any** and all feedback! :)",559,35,theoretical foundations of graph neural networks research,0.0
"[N] DeepMind acquires MuJoCo, makes it freely available",See the [blog post](https://deepmind.com/blog/announcements/mujoco). Awesome news!,558,35,n deepmind acquires mujoco makes it freely available,0.4404
[N] The email that got Ethical AI researcher Timnit Gebru fired,"Here is the email (according to platformer), I will post the source in a comment:

Hi friends,

I had stopped writing here as you may know, after all the micro and macro aggressions and harassments I received after posting my stories here (and then of course it started being moderated).


Recently however, I was contributing to a document that Katherine and Daphne were writing where they were dismayed by the fact that after all this talk, this org seems to have hired 14% or so women this year. Samy has hired 39% from what I understand but he has zero incentive to do this.


What I want to say is stop writing your documents because it doesn’t make a difference. The DEI OKRs that we don’t know where they come from (and are never met anyways), the random discussions, the “we need more mentorship” rather than “we need to stop the toxic environments that hinder us from progressing” the constant fighting and education at your cost, they don’t matter. Because there is zero accountability. There is no incentive to hire 39% women: your life gets worse when you start advocating for underrepresented people, you start making the other leaders upset when they don’t want to give you good ratings during calibration. There is no way more documents or more conversations will achieve anything. We just had a Black research all hands with such an emotional show of exasperation. Do you know what happened since? Silencing in the most fundamental way possible.


Have you ever heard of someone getting “feedback” on a paper through a privileged and confidential document to HR? Does that sound like a standard procedure to you or does it just happen to people like me who are constantly dehumanized?


Imagine this: You’ve sent a paper for feedback to 30+ researchers, you’re awaiting feedback from PR & Policy who you gave a heads up before you even wrote the work saying “we’re thinking of doing this”, working on a revision plan figuring out how to address different feedback from people, haven’t heard from PR & Policy besides them asking you for updates (in 2 months). A week before you go out on vacation, you see a meeting pop up at 4:30pm PST on your calendar (this popped up at around 2pm). No one would tell you what the meeting was about in advance. Then in that meeting your manager’s manager tells you “it has been decided” that you need to retract this paper by next week, Nov. 27, the week when almost everyone would be out (and a date which has nothing to do with the conference process). You are not worth having any conversations about this, since you are not someone whose humanity (let alone expertise recognized by journalists, governments, scientists, civic organizations such as the electronic frontiers foundation etc) is acknowledged or valued in this company.


Then, you ask for more information. What specific feedback exists? Who is it coming from? Why now? Why not before? Can you go back and forth with anyone? Can you understand what exactly is problematic and what can be changed?


And you are told after a while, that your manager can read you a privileged and confidential document and you’re not supposed to even know who contributed to this document, who wrote this feedback, what process was followed or anything. You write a detailed document discussing whatever pieces of feedback you can find, asking for questions and clarifications, and it is completely ignored. And you’re met with, once again, an order to retract the paper with no engagement whatsoever.


Then you try to engage in a conversation about how this is not acceptable and people start doing the opposite of any sort of self reflection—trying to find scapegoats to blame.


Silencing marginalized voices like this is the opposite of the NAUWU principles which we discussed. And doing this in the context of “responsible AI” adds so much salt to the wounds. I understand that the only things that mean anything at Google are levels, I’ve seen how my expertise has been completely dismissed. But now there’s an additional layer saying any privileged person can decide that they don’t want your paper out with zero conversation. So you’re blocked from adding your voice to the research community—your work which you do on top of the other marginalization you face here.


I’m always amazed at how people can continue to do thing after thing like this and then turn around and ask me for some sort of extra DEI work or input. This happened to me last year. I was in the middle of a potential lawsuit for which Kat Herller and I hired feminist lawyers who threatened to sue Google (which is when they backed off--before that Google lawyers were prepared to throw us under the bus and our leaders were following as instructed) and the next day I get some random “impact award.” Pure gaslighting.


So if you would like to change things, I suggest focusing on leadership accountability and thinking through what types of pressures can also be applied from the outside. For instance, I believe that the Congressional Black Caucus is the entity that started forcing tech companies to report their diversity numbers. Writing more documents and saying things over and over again will tire you out but no one will listen.


Timnit

---------------------------------
Below is Jeff Dean's message sent out to Googlers on Thursday morning


Hi everyone,


I’m sure many of you have seen that Timnit Gebru is no longer working at Google. This is a difficult moment, especially given the important research topics she was involved in, and how deeply we care about responsible AI research as an org and as a company.


Because there’s been a lot of speculation and misunderstanding on social media, I wanted to share more context about how this came to pass, and assure you we’re here to support you as you continue the research you’re all engaged in.


Timnit co-authored a paper with four fellow Googlers as well as some external collaborators that needed to go through our review process (as is the case with all externally submitted papers). We’ve approved dozens of papers that Timnit and/or the other Googlers have authored and then published, but as you know, papers often require changes during the internal review process (or are even deemed unsuitable for submission). Unfortunately, this particular paper was only shared with a day’s notice before its deadline — we require two weeks for this sort of review — and then instead of awaiting reviewer feedback, it was approved for submission and submitted.
A cross functional team then reviewed the paper as part of our regular process and the authors were informed that it didn’t meet our bar for publication and were given feedback about why. It ignored too much relevant research — for example, it talked about the environmental impact of large models, but disregarded subsequent research showing much greater efficiencies.  Similarly, it raised concerns about bias in language models, but didn’t take into account recent research to mitigate these issues. We acknowledge that the authors were extremely disappointed with the decision that Megan and I ultimately made, especially as they’d already submitted the paper. 
Timnit responded with an email requiring that a number of conditions be met in order for her to continue working at Google, including revealing the identities of every person who Megan and I had spoken to and consulted as part of the review of the paper and the exact feedback. Timnit wrote that if we didn’t meet these demands, she would leave Google and work on an end date. We accept and respect her decision to resign from Google.
Given Timnit's role as a respected researcher and a manager in our Ethical AI team, I feel badly that Timnit has gotten to a place where she feels this way about the work we’re doing. I also feel badly that hundreds of you received an email just this week from Timnit telling you to stop work on critical DEI programs. Please don’t. I understand the frustration about the pace of progress, but we have important work ahead and we need to keep at it.


I know we all genuinely share Timnit’s passion to make AI more equitable and inclusive. No doubt, wherever she goes after Google, she’ll do great work and I look forward to reading her papers and seeing what she accomplishes.
Thank you for reading and for all the important work you continue to do. 


-Jeff",554,664,n the email that got ethical ai researcher timnit gebru fired,-0.0772
"[D] Deep Learning has a size problem. We need to focus on state-of-the-art efficiency, not state-of-the-art accuracy.","I'm not sure the recent trend of larger and larger models is going to help make deep learning more useful or applicable. Mulit-billion parameter models might add a few percentage points of accuracy, but they don't make it easier to build DL-powered applications or help other people start using the technology.

At the same time, there are some incredible results out there applying techniques like distillation, pruning, and quantization. I'd love for it to be standard practice to apply these techniques to more projects to see just how small and efficient we can make models.

For anyone interested in the topic, I wrote up a brief primer on the problem and some research into solutions. I'd love to hear of any success or failures people here have had with these techniques in production settings.

[https://heartbeat.fritz.ai/deep-learning-has-a-size-problem-ea601304cd8](https://heartbeat.fritz.ai/deep-learning-has-a-size-problem-ea601304cd8)",556,115,d deep learning has a size problem we need to focus on stateoftheart efficiency not stateoftheart accuracy,-0.0516
[D] PyTorch 2.0 Announcement,"PyTorch 2.0 was just announced at the PyTorch Conference:

[https://pytorch.org/get-started/pytorch-2.0/](https://pytorch.org/get-started/pytorch-2.0/)

See also the accompanying twitter thread: [https://twitter.com/PyTorch/status/1598708792598069249](https://twitter.com/PyTorch/status/1598708792598069249)",552,47,d pytorch  announcement,0.0
[P] GPT-2 + BERT reddit replier. I built a system that generates replies by taking output from GPT-2 and using BERT models to select the most realistic replies. People on r/artificial replied to it as if it were a person.,"I was trying to make a reddit reply bot with GPT-2 to see if it could pass as a human on reddit.  I realized that a decent fraction of the output was looking pretty weird so I wanted to improve on the results.  I came up with this method:

[Method Overview](https://preview.redd.it/l2xenzvlxbf41.png?width=939&format=png&auto=webp&s=dc6df001c76f8c498e3268455ba0bc53fd3923f4)

Since I don't have the kind of compute to train new things from scratch, I just took a pretrained BERT and fine-tuned it to detect real from GPT-2 generated. Then I used the BERT model as a filter (kind of like a GAN but without the feedback between generator and discriminator).  I also aded a BERT model to try to predict which comment would get the most upvotes.

Several people replied to the output replies as if it was a real person so I think it probably passes a light Turing sniff test (maybe they were bots too, who knows?).  Hopefully nobody gets too mad that I tested the model in the wild. I ran it sparingly and made sure it wasn't saying anything inflammatory.

I wrote up a [results overview](https://www.bonkerfield.org/2020/02/combining-gpt-2-and-bert/) and a [tutorial post](https://www.bonkerfield.org/2020/02/reddit-bot-gpt2-bert/) to explain how it works.  And I put all of my code on [github](https://github.com/lots-of-things/gpt2-bert-reddit-bot) and on [Colab](https://drive.google.com/open?id=1by97qt6TBpi_o644uKnYmQE5AJB1ybMK).

The thing I like most about this method is that it mirrors how I actually write replies too.  In my head, I generate a couple of ideas and then pick between them after the fact with my ""inner critic.""

Hope you enjoy it and if you want to play with it, please only use it for good.",549,63,p gpt  bert reddit replier i built a system that generates replies by taking output from gpt and using bert models to select the most realistic replies people on rartificial replied to it as if it were a person,0.0
Illustrated Machine Learning cheatsheets covering Stanford's CS 229 class,"Set of illustrated Machine Learning cheatsheets covering the content of Stanford's CS 229 class:  

* Deep Learning: [https://stanford.edu/\~shervine/teaching/cs-229/cheatsheet-deep-learning.html](https://stanford.edu/~shervine/teaching/cs-229/cheatsheet-deep-learning.html)
* Supervised Learning: [https://stanford.edu/\~shervine/teaching/cs-229/cheatsheet-supervised-learning.html](https://stanford.edu/~shervine/teaching/cs-229/cheatsheet-supervised-learning.html)
* Unsupervised Learning: [https://stanford.edu/\~shervine/teaching/cs-229/cheatsheet-unsupervised-learning.html](https://stanford.edu/~shervine/teaching/cs-229/cheatsheet-unsupervised-learning.html)
* Tips and tricks: [https://stanford.edu/\~shervine/teaching/cs-229/cheatsheet-machine-learning-tips-and-tricks.html](https://stanford.edu/~shervine/teaching/cs-229/cheatsheet-machine-learning-tips-and-tricks.html)

https://preview.redd.it/ub77t5cawah11.jpg?width=2048&format=pjpg&auto=webp&s=1262b50d06aba286d0273035b322ba5c04636691",550,17,illustrated machine learning cheatsheets covering stanfords cs  class,0.0
[D] Calling out the authors of 'Trajformer' paper for claiming they published code but never doing it,"I read a paper from NeurIPS 2020 titled 'Trajformer: Trajectory Prediction with Local Self-Attentive Contexts for Autonomous Driving'. I found it interesting and the authors claim multiple times in the paper that 'we release our code at '[https://github.com/Manojbhat09/Trajformer](https://github.com/Manojbhat09/Trajformer)'. Turns out they never did, fine, I thought perhaps they will in the future and starred the repo to check it out later.

Many others raised issues asking for update on code release and they never replied. Finally, it April they update the readme to say that they will release the code and that's been the last update.

I know this is a common trend in ML papers now, but what sucks is that I emailed the authors (both the grad student and the PI) multiple times asking for an update an they never replied. Their paper is literally based on empirical improvements and without working code to replicate the results it is their word against mine.

I strongly think things have to change, and I believe they only will if we call them out. I waited long enough, and made significant effort to contact the authors with no response. I mean I don't mind them not releasing their code, but at least don't claim that you did in the paper/review phase and then disappear. An undergrad in my lab asked why she should take time to clean up the code and document it before release while others just move on to the next interesting project and I don't have an answer. ",544,89,d calling out the authors of trajformer paper for claiming they published code but never doing it,0.0
[P] I created artificial life simulation using neural networks and genetic algorithm.,"&#x200B;

https://preview.redd.it/s9132dyqll441.png?width=1280&format=png&auto=webp&s=b8012705b448f3519b05d42aab2c78ae12622a33

Those are my creatures, each have its own neural network, they eat and reproduce. New generations mutate and behave differently.  Entire map is 5000x5000px and starts with 160 creatures and 300 food.

[https://www.youtube.com/watch?v=VwoHyswI7S0](https://www.youtube.com/watch?v=VwoHyswI7S0&t=9s)",546,77,p i created artificial life simulation using neural networks and genetic algorithm,0.25
[N][D] YOLO Creator Joseph Redmon Stopped CV Research Due to Ethical Concerns,"Joseph Redmon, creator of the popular object detection algorithm YOLO (You Only Look Once), tweeted last week that he had ceased his computer vision research to avoid enabling potential misuse of the tech — citing in particular “military applications and privacy concerns.”

Read more: [YOLO Creator Joseph Redmon Stopped CV Research Due to Ethical Concerns](https://medium.com/syncedreview/yolo-creator-says-he-stopped-cv-research-due-to-ethical-concerns-b55a291ebb29)",549,183,nd yolo creator joseph redmon stopped cv research due to ethical concerns,0.6408
[R] Sparks of Artificial General Intelligence: Early experiments with GPT-4,"[New paper](https://arxiv.org/abs/2303.12712) by MSR researchers analyzing an early (and less constrained) version of GPT-4. Spicy quote from the abstract:

""Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system.""

What are everyone's thoughts?",549,356,r sparks of artificial general intelligence early experiments with gpt,0.4767
[P] Thinc: A refreshing functional take on deep learning,"Introducing the new Thinc, a refreshing functional take on deep learning!

- 🔮 Static type checking
- 🔥 Mix PyTorch, TensorFlow, ApacheMXNet
- ⛓️ Integrated config system
- 🧮 Extensible backends incl. JAX (experimental)
- 🧬 Variable-length sequences & more

https://thinc.ai/",550,48,p thinc a refreshing functional take on deep learning,0.0
"[R] You can't train GPT-3 on a single GPU, but you *can* tune its hyperparameters on one","> You can't train GPT-3 on a single GPU, much less tune its hyperparameters (HPs).  
>  
>  
But what if I tell you…  
>  
>  
…you \*can\* tune its HPs on a single GPU thanks to new theoretical advances?

Hi Reddit,

I'm excited to share with you our latest work, [\[2203.03466\] Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer (arxiv.org)](https://arxiv.org/abs/2203.03466).

Code: [https://github.com/microsoft/mup](https://t.co/5S0YAghCYx)

  


https://preview.redd.it/nnb2usdjlkm81.png?width=1195&format=png&auto=webp&s=ca9e6d5cddfbea5675cf00854806d5189c3e40bb

(Disclaimer: this post is shamelessly converted from my twitter thread)

The idea is actually really simple: in a special parametrization introduced in [our previous work](https://arxiv.org/abs/2011.14522) ([reddit thread](https://www.reddit.com/r/MachineLearning/comments/k8h01q/r_wide_neural_networks_are_feature_learners_not/)) called µP, narrow and wide neural networks share the same set of optimal hyperparameters. This works even as width -> ∞.

&#x200B;

https://preview.redd.it/dqna8guklkm81.png?width=1838&format=png&auto=webp&s=2f7ba582a1cc949461dac8601a896034eaf0ff84

The hyperparameters can include learning rate, learning rate schedule, initialization, parameter multipliers, and more, even individually for each parameter tensor. We empirically verified this on Transformers up to width 4096.

&#x200B;

https://preview.redd.it/rwdsb6snlkm81.jpg?width=2560&format=pjpg&auto=webp&s=c3152f2746132d92dc3788a42aa6926a61d7c46f

Using this insight, we can just tune a tiny version of GPT-3 on a single GPU --- if the hyperparameters we get on the small model is near optimal, then they should also be near optimal on the large model! We call this way of tuning \*µTransfer\*.

&#x200B;

https://preview.redd.it/mi7ibyyolkm81.png?width=1195&format=png&auto=webp&s=144af103b2aaf3ffeb2ccf19aad7565527dbd003

We µTransferred hyperparameters from a small 40 million parameter version of GPT-3 — small enough to fit on a single GPU — to the 6.7 billion version. With some asterisks, we get a performance comparable to the original GPT-3 model with twice the parameter count!

&#x200B;

https://preview.redd.it/rrq2yfwplkm81.png?width=3232&format=png&auto=webp&s=6cf4cc9652db48e12b48a85b2f837e55e64bd09c

The total tuning cost is only 7% of the whole pretrain compute cost! Since the direct tuning of the small model costs roughly the same even as the large model increases in size, tuning the 175B GPT-3 this way would probably cost at most 0.3% of the total pretrain compute.

You: ""wait can I shrink the model only in width?""

Bad news: there's not much theoretical guarantee for non-width stuff

good news: we empirically tested transfer across depth, batch size, sequence length, & timestep work within reasonable ranges on preLN transformers.

&#x200B;

https://preview.redd.it/x7fo95yqlkm81.jpg?width=2560&format=pjpg&auto=webp&s=1935bf10f1524f9da3df0cc2e95ae1ec9b805f37

We applied this to tune BERT-base and BERT-large simultaneously by shrinking them to the same small model in both width and depth, where we did the direct tuning. We got a really nice improvement over the already well-tuned megatron BERT baseline, especially for BERT-large!

&#x200B;

https://preview.redd.it/db5eausrlkm81.png?width=1687&format=png&auto=webp&s=4453ec387477d20cbcdab266ccbc8e36032c87fd

In general, it seems that the larger a model is, the less well tuned it is --- which totally makes sense --- and thus the more to gain from µTransfer. We didn't have compute to retrain the GPT-3 175B model, but I'll leave your mouth watering with that thought.

OK, so what actually is µP and how do you implement it?

It's encapsulated by the following table for how to scale your initialization and learning rate with fan-in or fan-out. The purple text is µP and the gray text in parenthesis is pytorch default, for reference, and the black text is shared by both.

&#x200B;

https://preview.redd.it/4475drzvlkm81.png?width=1507&format=png&auto=webp&s=b65f56ef2d8c24f077a24a8df40eb7f98c80f7e2

But just like you don't typically want to implement autograd by hand even though autograd is just chain rule, we recommend using our package [https://github.com/microsoft/mup](https://t.co/5S0YAg026Z) to implement µP in your models.

The really curious ones of you: ""OK what is the theoretical motivation behind all this?""

Unfortunately, this is already getting long, so feel free to check out the [reddit thread](https://www.reddit.com/r/MachineLearning/comments/k8h01q/r_wide_neural_networks_are_feature_learners_not/) on [our previous theoretical paper](https://arxiv.org/abs/2011.14522), and people let me know if this is something you want to hear for another time!

But I have to say that this is a rare occasion in deep learning where very serious mathematics has concretely delivered a result previously unthinkable, and I'm elated with how things turned out! In contrast to [this reddit thread a few days ago](https://www.reddit.com/r/MachineLearning/comments/t8fn7m/d_are_we_at_the_end_of_an_era_where_ml_could_be/), I think there are plenty of room for new, fundamental mathematics to change the direction of deep learning and artificial intelligence in general --- why chase the coattail of empirical research trying to ""explain"" them all when you can lead the field with deep theoretical insights?

Let me know what you guys think in the comments, or feel free to email me (gregyang at microsoft dot com)!",546,39,r you cant train gpt on a single gpu but you can tune its hyperparameters on one,0.0
[Discussion] Why are Einstein Sum Notations not popular in ML? They changed my life.,"I recently discovered \`torch.einsum\` and now I am mad at every friend, mentor, acquaintance for not telling me about it. 

They are just way more intuitive and can handle most operations that I would want to do with tensors so elegantly. No more of having to remember which way is axis=0, No more of having to remember which way is dim=1 and no more of remembering so many numpy and torch functions only to misuse np.unsqueeze and torch.expand\_dims. 

It takes only 30 mins or so to learn the notation and become somewhat proficient but then you are sorted for life. 

What are the arguments for and against using einstein notations for everything? Will I be writing code which others find difficult to understand? Kindly pitch in your thoughts and theories on why are they so seldom used when they are one-size-fit-all.",544,114,discussion why are einstein sum notations not popular in ml they changed my life,-0.3252
[D] Are you using PyTorch or TensorFlow going into 2022?,"PyTorch, TensorFlow, and both of their ecosystems have been developing so quickly that I thought it was time to take another look at how they stack up against one another. I've been doing some analysis of how the frameworks compare and found some pretty interesting results.

For now, PyTorch is still the ""research"" framework and TensorFlow is still the ""industry"" framework.

The majority of *all* papers on Papers with Code use PyTorch

https://preview.redd.it/p62rqqidzi581.png?width=747&format=png&auto=webp&s=9c3b19ecc9c1386f6706f5b03e905280610ee81e

While more job listings seek users of TensorFlow

https://preview.redd.it/lcvzxrwmik581.png?width=747&format=png&auto=webp&s=e669f33897491225e0e793ae452b7ff64da17dee

**I did a more thorough analysis of the relevant differences between the two frameworks,** [**which you can read here**](https://www.assemblyai.com/blog/pytorch-vs-tensorflow-in-2022/) **if you're interested.**

Which framework are you using going into 2022? How do you think JAX/Haiku will compete with PyTorch and TensorFlow in the coming years? I'd love to hear your thoughts!",545,364,d are you using pytorch or tensorflow going into ,0.0
[R] DeepMind Open Sources AlphaFold Code,"""Last year we presented #AlphaFold v2 which predicts 3D structures of proteins down to atomic accuracy. Today we’re proud to share the methods in @Nature w/open source code. Excited to see the research this enables. More very soon!""

https://twitter.com/demishassabis/status/1415736975395631111

I did not see this one coming, I got to admit it.",543,56,r deepmind open sources alphafold code,0.0
[N] The register did a full exposé on Siraj Raval. Testimonials from his former students and people he stole code from.,"https://www.theregister.co.uk/2019/09/27/youtube_ai_star/

I found this comment on the article hilarious

> Why aren't you writing these articles slamming universities?
> I am currently a software engineer in a data science team producing software that yields millions of dollars in revenue for our company. I did my undergraduate in physics and my professors encouraged us to view MIT Open Courseware lectures alongside their subpar teaching. I learned more from those online lectures than I ever could in those expensive classes. I paid tens of thousands of dollars for that education. I decided that it was better bang for my buck to learn data science than in would every be to continue on in the weak education system we have globally. I paid 30 dollars month, for a year, to pick up the skills to get into data science. I landed a great job, paying a great salary because I took advantage of these types of opportunities. If you hate on this guy for collecting code that is open to the public and creating huge value from it, then you can go get your masters degree for $50-100k and work for someone who took advantage of these types of offerings. Anyone who hates on this is part of an old school, suppressive system that will continue to hold talented people down. Buck the system and keep learning!

Edit:

Btw, the Journalist, Katyanna Quach,  is looking for people who have had direct experiences with Siraj. If you have, you can contact directly her directly here

https://www.theregister.co.uk/Author/Email/Katyanna-Quach

here

https://twitter.com/katyanna_q

or send tips here

corrections@theregister.co.uk",541,174,n the register did a full expos on siraj raval testimonials from his former students and people he stole code from,0.0
Synopsis of top Go professional's analysis of Google's Deepmind's Go AI,"Hi there. Earlier this month I had [a discussion](https://www.reddit.com/r/hearthstone/comments/3zdibn/intelligent_agents_for_hearthstone/cylnbf2) over on /r/hearthstone with /u/yetipirate about Computer Go. Then the news hit this week of the first Go AI to beat a human professional.

We had some more discussion then, and I made a synopsis of [this video](https://www.youtube.com/watch?v=NHRHUHW6HQE), where the US Go Association has Myungwan Kim, 9-Dan Pro, analyse the games between the AlphaGo AI and human professional Fan Hui, 2-Dan Pro. (FTR: Professional go ranks start at 1-Dan and go up to 9-Dan, but rather than the absolute top 9-Dan is more like the beginning of grandmastery. The best players in the world are like 9-Dan+++++. Lee Sedol, which AlphaGo will challenge next this March, is at this latter level.)

/u/yetipirate suggested this synopsis might interest some people here as well, since it digests the salient points of a two hour video with lots of Go jargon into a more manageable post. So hence I'm posting it here, I hope you all enjoy it. Feel free to ask me any questions about Go, but I'm not that strong myself so ymmv. Anyway without further ado:

**In General:**

The match has been big news in East-Asia as well. The thing which most shocked all the professionals was that AlphaGo played so much like a human player. Their first impressions were that it's as if this was a human playing, not a computer.

Since how a human plays is, obviously, pretty well known, they decided that they'll focus commentary mostly on those cases where AlphaGo doesn't play like a human.

The first thing that Myungwan Kim noted was that AlphaGo has a Japanese playstyle (this is especially interesting because among the three traditional Go powerhouses, China, Korea, and Japan, the Japanese have been the weakest in international competitions for the past several decades). The commentators don't know, but they suspect it is that the original human data set was biased towards Japanese playstyles.

Myungwan Kim also makes a comment about one of the lines continually repeated in the coverage of Computer Go. The line that ""if you ask a top Go player why they like a certain move, they'll often say 'it felt right'"". Myungwan Kim wanted to add that just because it's based on intuition, doesn't mean there's no logic behind it at all. Top Go players aren't just guessing what are good moves, they have a real and complicated rational understanding about what specific moves are doing. Even if the final decision might come down to which move feels the best, it's not as simple as top pro's just doing a random move and saying 'I felt like it'.

**The Games:**

In the **first game** both sides played very passively in the opening. Leisurely and gentle they say.

Myungwan Kim finds that AlphaGo has a weakness here, it doesn't seem to understand the value of taking and holding initiative. Complicated to explain, but at its core it's about doing moves which force your opponent to use their turn to react to your move over doing moves which might be equally valuable to you, but leave your opponent free to do whatever they want on their turn.

Important, Myungwan Kim says because of this that the first game Fan Hui was winning in the opening. He says this was the only game Fan Hui was winning after the opening. He estimates Fan Hui was about 10 points ahead, and can't see white getting back even 5 points coming out of that opening. Myungwan Kim offers some alternate moves for AlphaGo which would still have Fan Hui in the lead, but would've given AlphaGo better opportunities to comeback.

Conclusion from the opening: AlphaGo lost because it didn't understand the value of initiative.

Myungwan Kim later points to one huge mistake by Fan Hui in the midgame that lost him the game. I can't go into detail here because, as characteristic of top-level Go, it's the difference of placing one stone one space higher. But Myungwan Kim says that while Fan Hui made other small mistakes, this one move is the big one which let AlphaGo come back from losing the opening.

Final conclusion from game one: Aside from not understanding initiative. Myungwan Kim says AlphaGo betrays itself as a computer in that it sometimes it goes too far in mimicking standard professional play and does the most common move instead of the most optimal move. In other words, it's extremely book smart, but at times fails to notice when it should be ignoring the books because the specific situation in the game makes the less standard move the most optimal one instead. (A bit cliche imo, but Myungwan Kim says ""AlphaGo is not creative"".) They think that might really hurt AlphaGo in the game against Lee Sedol.

**Game 2**, they note Fan Hui really played too aggressively, as he noted in his own post-match interview. Myungwan Kim says he can really see Fan Hui wasn't playing his best game, but was trying to test AlphaGo to see if it could be tricked into making exploitable mistakes.

Myungwan Kim says Fan Hui actually put up a really good fight. After the opening it should've been over for Fan Hui, but AlphaGo almost allowed Fan Hui to get back in the game.

**Game 3** is similar to the fifth game, though Fan Hui played better in the beginning here. Myungwan Kim notes several moves by AlphaGo which are top professional moves. He notes some moves by Fan Hui which he thinks hints that Fan Hui might be a bit out of practice when it comes to playing professional level games (he says it's the kind of move you do if too used to playing teaching games against amateurs). Fan Hui lost because he played over-aggressive and left too many holes in his defence as a result.

On the **fifth game**, Myungwan Kim says AlphaGo was winning from the beginning here. They marvel at some of AlphaGo's moves here, but they're not sure whether AlphaGo really knew what it was doing or if it just got 'lucky' somehow.

Myungwan Kim points out AlphaGo made a huge mistake early in this game, but was saved because not long after Fan Hui made an equally huge mistake. But this is an example where he thinks a real grandmaster like Lee Sedol would not have allowed AlphaGo to get away with the kind of mistake it made there.

**AlphaGo's Strengths and Weaknesses:**

Myungwan Kim lists AlphaGo's strengths:

 * It's not afraid of 'Ko'. 'Ko' is too complex a concept to explain succinctly, for an attempt [see my post here](https://www.reddit.com/r/MachineLearning/comments/43fl90/synopsis_of_top_go_professionals_analysis_of/czi7swh). They marvel at some of AlphaGo's moves surrounding a 'Ko' situation, but aren't sure if AlphaGo really knew what it was doing or just got lucky that it worked out.

 * Reading might be AlphaGo's strength. As in, cases where it comes down to very straightforward fights and moves it's very strong at choosing the right moves.

Myungwan Kim lists AlphaGo's weaknesses:

 * Doesn't understand initiative, as explained earlier.

 * At times too obsessed with following common patterns, when the specific situation might require creative deviation from those patterns. Also explained earlier.

 * It doesn't understand 'Aji'. 'Aji' is difficult to explain, but it refers to the amount of uncertainty remaining in a specific grouping of white and black stones. (Usually, it's about the chance that a group of stones which is 'death' might become alive and vice versa as a result of things happening elsewhere on the board.) You can also put this differently as: AlphaGo lacks proper long-term thinking.

 * Myungwan Kim thinks AlphaGo has difficulty, or even doesn't at all, evaluating the value of specific stones. It's good at making moves which directly gain territory for itself, but tends to miss moves which reduce the value of the opponent's stones.

 * It can make really high level moves at times, but it doesn't understand those moves. Which it displays by making the right moves at the wrong time.

More generally Myungwan Kim thinks a weakness of AlphaGo is its insularity. He really stresses that human pro's become much stronger when they discuss and analyse their games with other pro's. And because AlphaGo primarily plays against itself the quality of the feedback it gets on its play is too one-note, which leaves holes in its plays whereas human pro's getting feedback from many other human pro's end up with more robust and stronger playstyles. He really thinks to progress past its current level AlphaGo needs to play more with top human pro's rather than just itself. Right now, Myungwan Kim en most pro's he knows don't feel threatened by AlphaGo. They also talk about how AlphaGo can be useful for human pro's to study and become stronger, which can make AlphaGo stronger in turn. (This last paragraph is imo all just Myungwan Kim musing based on his understanding of how AlphaGo was designed more than evaluating its plays themselves, so that's why I didn't list it as a bullet point.)

In general, I get the sense from Myungwan Kim's explanations that he thinks AlphaGo is stronger at the more concrete parts of Go play, such as territory and life-or-death, and weaker at the more vague concepts, such as influence and uncertainty.

**[word limit hit, final part below]**",546,130,synopsis of top go professionals analysis of googles deepminds go ai,0.2023
"[N] Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs","> Introducing MPT-7B, the latest entry in our MosaicML Foundation Series. MPT-7B is a transformer trained from scratch on 1T tokens of text and code. It is open source, available for commercial use, and matches the quality of LLaMA-7B. MPT-7B was trained on the MosaicML platform in 9.5 days with zero human intervention at a cost of ~$200k. Starting today, you can train, finetune, and deploy your own private MPT models, either starting from one of our checkpoints or training from scratch. For inspiration, we are also releasing three finetuned models in addition to the base MPT-7B: MPT-7B-Instruct, MPT-7B-Chat, and MPT-7B-StoryWriter-65k+, the last of which uses a context length of 65k tokens!

https://www.mosaicml.com/blog/mpt-7b",540,119,n introducing mptb a new standard for opensource commercially usable llms,0.0
[D] Go champion Lee Se-dol beaten by DeepMind retires after declaring AI invincible,"[https://en.yna.co.kr/view/AEN20191127004800315](https://en.yna.co.kr/view/AEN20191127004800315)

Announced today in South Korea, and it’s made me think on the sort of impact that these things will have on people in the coming days. There’s definitely a great deal of good that can be achieved, with innovation/growth and so many opportunities in general for the companies and people involved in this work.

But at the same time, it is kind of sad to see some of the human element get left behind. I’m sure Lee Se-dol could have played for many more years if he wanted to, continuing to contribute greatly to the professional Go scene as a player.

This is something that I wonder then, if people working at companies like Google / DeepMind should be thinking about. I’m sure the growing profit margins and money that’s flowing in from all our work is more than satisfactory for the company leadership / investors to not have any issues. As the engineers responsible for actually building everything though, is there any kind of ethical consideration on our part that we need to recognize? I don’t know. I am curious as to what you all think here in [r/machinelearning](https://www.reddit.com/r/machinelearning/) though.",547,148,d go champion lee sedol beaten by deepmind retires after declaring ai invincible,0.6486
"[P] ""Mathematics for Machine Learning"": drafts for all chapters now available","[Site](https://mml-book.github.io/)

[Discussion from 4 months ago](https://www.reddit.com/r/MachineLearning/comments/8kifb0/n_mathematics_for_machine_learning/)

Since the beginning of the year, new chapters became available one by one, and it seems like all draft chapters have become available since a few weeks ago. Personally, as a ""math deficient"" person, I've been using this as a resource to prepare myself (yet again) for another attempt at Bishop's PRML.",543,52,p mathematics for machine learning drafts for all chapters now available,0.0
[R] Telling GPT-4 you're scared or under pressure improves performance,"In a recent paper, researchers have discovered that LLMs show enhanced performance when provided with prompts infused with emotional context, which they call ""EmotionPrompts.""

These prompts incorporate sentiments of urgency or importance, such as ""It's crucial that I get this right for my thesis defense,"" as opposed to neutral prompts like ""Please provide feedback.""

The study's empirical evidence suggests substantial gains. This indicates a **significant sensitivity of LLMs to the implied emotional stakes** in a prompt:

* Deterministic tasks saw an 8% performance boost
* Generative tasks experienced a 115% improvement when benchmarked using BIG-Bench.
* Human evaluators further validated these findings, observing a 10.9% increase in the perceived quality of responses when EmotionPrompts were used.

This enhancement is attributed to the models' capacity to detect and prioritize the heightened language patterns that imply a need for precision and care in the response.

The research delineates the potential of EmotionPrompts to refine the effectiveness of AI in applications where understanding the user's intent and urgency is paramount, even though the AI does not genuinely comprehend or feel emotions.

**TLDR: Research shows LLMs deliver better results when prompts signal emotional urgency. This insight can be leveraged to improve AI applications by integrating EmotionPrompts into the design of user interactions.**

[Full summary is here](https://aimodels.substack.com/p/telling-gpt-4-youre-scared-or-under). Paper [here](https://arxiv.org/pdf/2307.11760.pdf).",536,119,r telling gpt youre scared or under pressure improves performance,-0.3182
[D] What is the tool stack of ML teams at startups? + intel from 41 companies,"We were wondering what are the tools, frameworks, libraries, and methodologies that **ML teams at startups actually use.**

...and so we asked a bunch of teams and got 41 of them to answer.

We got way more insights than we could handle but after grouping it into a few clusters of most-prevalent answers we got something like this:

* Software development setup
   * For IDE there are two camps: Jupyter Lab + NB extensions with occasional Deepnote, and Colab on one side and Pycharm or VSCode on the other ( R studio was a clear winner for R users)
   * Github for version control
   * Python (most) R (some)
* Machine Learning frameworks
   * Pandas + Matplotlib + Plotly for exploration and visualization
   * Sklearn + XGBoost for classical algos
   * Tensorflow+Keras or Pytorch (sometimes both at the same company) for deep learning. Pretty even split I'd say
* MLOps
   * Kubeflow, Airflow, Amazon Sagemaker, Azure for orchestration
   * Kubeflow, MLflow, Amazon Sagemaker, for model packaging/serving
   * pytest-benchmark, MLperf for profiling and optimization when moving models from training to inference
   * MLflow, Comet, Neptune for experiment management
* Unexpected 🙂
   * Wetware – ""the hardware and software combination that sits between your ears – is the most important, most useful, most powerful machine learning tool you have.""

This is of course TLDR but you can [check out the full article](https://neptune.ai/blog/tools-libraries-frameworks-methodologies-ml-startups-roundup?utm_source=reddit&utm_medium=post&utm_campaign=blog-tools-libraries-frameworks-methodologies-ml-startups-roundup) if you want.

How about you? **What is your team using that we missed?**",540,164,d what is the tool stack of ml teams at startups  intel from  companies,0.0
[News] TensorFlow 2.0 is out!,"The day has finally come, go grab it here:

[https://github.com/tensorflow/tensorflow/releases/tag/v2.0.0](https://github.com/tensorflow/tensorflow/releases/tag/v2.0.0)

I've been using it since it was in alpha stage and I'm very satisfied with the improvements and new additions.",539,145,news tensorflow  is out,0.0
[D] Anyone having trouble reading a particular paper? Post it here and we'll help figure out any parts you are stuck on.,"UPDATE 2: This round has wrapped up. To keep track of the next round of this, you can check https://www.reddit.com/r/MLPapersQandA/ 

UPDATE: Most questions have been answered, and those who I wasn't able to answer, started a discussion which would hopefully lead to an answer. 

I am not able to answer any new questions on this thread, but will continue any discussions already ongoing, and will answer those questions on the next round.  

I made a new help thread btw, this time I am helping people looking for papers, check it out

https://www.reddit.com/r/MachineLearning/comments/8bwuyg/d_anyone_having_trouble_finding_papers_on_a/

If you have a paper you need help on, please post it in the next round of this, tentatively scheduled for April 24th. 

For more information, please see the subreddit I make to track and catalog these discussions. 

https://www.reddit.com/r/MLPapersQandA/comments/8bwvmg/this_subreddit_is_for_cataloging_all_the_papers/


----------------------------------------------------------------------------


I was surprised to hear that even Andrew Ng has trouble reading certain papers at times and he reaches out to other experts to get help, so I guess that it's something most of us will probably always have to deal with to some extent or another. 

If you're having trouble with a particular paper, post it with the parts you are having trouble with, and hopefully me or someone else may help out. It'll be like a mini study group to extract as much valuable info from each paper. 

Even if it's a paper that you're not per say totally stuck on, but it's just that it'll take a while to completely figure out, post it anyway in case you find some value in shaving off some precious time in pursuing the total comprehension of that paper, so that you can more quickly move onto other papers. 

Edit:

Okay we got some papers. I'm going through them one by one. Please have specific questions on where exactly you are stuck, even if it's a big picture issue. Just say something like 'what's the big picture'. 

Edit 2:

Gotta to do some irl stuff but will continue helping out tomorrow. Some of the papers are outside my proficiency so hopefully some other people on the subreddit can help out. 

Edit 3:

Okay this really blew up. Some papers it's taking a really long time to figure out. 

Another request I have in addition to specific question, type out any additional info/brief summary that can help cut down on the time it will take for someone to answer the question. For example, if there's an equation whose components are explained through out the paper, make a mini glossary of said equation. Try to aim so that perhaps the reader doesn't even need to read the paper (likely not possible but aiming for this will make for excellent summary info) and they can answer your question. 

What attempts have you made so far to figure out the question. 

Finally, what is your best guess to what you think the answer might be, and why. 

Edit 4:

More people should participate in the papers, not just people who can answer the questions. If any of the papers listed are of interest to you, can you read them, and reply to the comment with your own questions about the paper, so that someone can answer both your questions. It might turn out that he person who posted the paper knows the question, and it even might be the case that you stumbled upon the answers to the original questions. 

Think of each paper as an invite to an open study group for that paper, not just a queue for an expert to come along and answer it. 

Edit 5:

It looks like people want this to be a weekly feature here. I'm going to figure out the best format from the comments here and make a proposal to the mods. 

Edit 6: 

I'm still going through the papers and giving answers. Even if I can't answer the question I'll reply with something, but it'll take a while. But please provide as much summary info as I described in the last edits to help me navigate through the papers and quickly collect as much background info I need to answer the question. ",537,133,d anyone having trouble reading a particular paper post it here and well help figure out any parts you are stuck on,-0.25
[D] COVID-19/Coronavirus challenge - Help scientists design antiviral proteins by playing a puzzle on Fold.It,"There is a challenge in Fold.It to help design antiviral proteins against [coronavirus](https://imgur.com/gallery/adAeNEv). 

The puzzle is here [https://fold.it/portal/node/2008926](https://fold.it/portal/node/2008926).

First thing that came to mind was AlphaFold, but I'm not aware of the particulars to see if it could be useful here in this scenario. 

I'm probably being unrealistic, but I was wondering about your thoughts on this challenge and if there is anything we (as a community) could do to help in this task.",537,29,d covidcoronavirus challenge  help scientists design antiviral proteins by playing a puzzle on foldit,0.5859
[D] Irresponsible anthropomorphism is killing AI journalism,"Basically the title.  The current state of media coverage of AI is fixated on constructing a compelling narrative to readers, and often personifies models well beyond their capabilities.  This is to the extent that articles almost always end up reading like every classifier is some form of limited AGI.

Take [""Meet Norman the Psychopathic AI""](https://www.bbc.com/news/technology-44040008), an article by the BBC, whom I generally consider quite capable journalists.  While the research methodology and some of the implications are discussed in the article, the majority of laypeople who encounter the article will likely erroneously conclude that Norman possesses beliefs, a worldview, and some dark outlook on humanity.  Some readers will think ""Norman"" is violent or dangerous, with a mind of his own.  A headline and an image go a long way in communication, especially online.

And this article is by far not the worst offender. Many news outlets perform much worse, publishing misleading, fearmongering, or sensationalist stories about ""some new AI"", borrowing from pop sci-fi tropes, with the star AI inevitably represented by lacklustre CG avatars bought off stock photo websites.

I remember having several discussions in the wake of the Facebook experiment where researchers had AIs communicate, and saw they developed a communication standard unreadable by humans.  Based on the articles that circulated afterwards, a significant number of people concluded ""they had to turn it off because they were on the verge of SKYNET"".

In the interests of doing more than just ranting: how do we deal with this as a community?  Should we be reaching out to journalists about these issues?  Is it our responsibility in interviews to communicate the limitations of the models we develop?

Personifying the projects we work on, and giving them human qualities, is certainly entertaining and helps market our research.  That said, it seems like a sizeable portion of the public has been misinformed about the state of machine learning research as a result.
",541,62,d irresponsible anthropomorphism is killing ai journalism,-0.8074
[D] What is OpenAI? I don't know anymore.,"*Some [commentary](https://threadreaderapp.com/thread/1153364705777311745.html) from [Smerity](https://twitter.com/Smerity/status/1153364705777311745) about yesterday's [cash infusion](https://openai.com/blog/microsoft/) from MS into OpenAI:*

What is OpenAI? I don't know anymore.
A non-profit that leveraged good will whilst silently giving out equity for [years](https://twitter.com/gdb/status/1105137541970243584) prepping a shift to for-profit that is now seeking to license closed tech through a third party by segmenting tech under a banner of [pre](https://twitter.com/tsimonite/status/1153340994986766336)/post ""AGI"" technology?

The non-profit/for-profit/investor [partnership](https://openai.com/blog/openai-lp/) is held together by a set of legal documents that are entirely novel (=bad term in legal docs), are [non-public](https://twitter.com/gdb/status/1153305526026956800) + unclear, have no case precedence, yet promise to wed operation to a vague (and already re-interpreted) [OpenAI Charter](https://openai.com/charter/).

The claim is that [AGI](https://twitter.com/woj_zaremba/status/1105149945118519296) needs to be carefully and collaboratively guided into existence yet the output of almost [every](https://github.com/facebookresearch) [other](https://github.com/google-research/google-research) [existing](https://github.com/salesforce) [commercial](https://github.com/NVlabs) lab is more open. OpenAI runs a closed ecosystem where they primarily don't or won't trust outside of a small bubble.

I say this knowing many of the people there and with past and present love in my heart—I don't collaborate with OpenAI as I have no freaking clue what they're doing. Their primary form of communication is high entropy blog posts that'd be shock pivots for any normal start-up.

Many of their [blog posts](https://openai.com/blog/cooperation-on-safety/) and [spoken](https://www.youtube.com/watch?v=BJi6N4tDupk) [positions](https://www.youtube.com/watch?v=9EN_HoEk3KY) end up [influencing government policy](https://twitter.com/jackclarkSF/status/986568940028616705) and public opinion on the future of AI through amplified pseudo-credibility due to *Open*, *Musk founded*, repeatedly hyped statements, and a sheen from their now distant non-profit good will era.

I have mentioned this to friends there and say all of this with positive sum intentions: I understand they have lofty aims, I understand they need cash to shovel into the forever unfurling GPU forge, but if they want any community trust long term they need a better strategy.

The implicit OpenAI message heard over the years:
“Think of how transformative and dangerous AGI may be. Terrifying. Trust us. Whether it's black-boxing technology, legal risk, policy initiatives, investor risk, ...—trust us with everything. We're good. No questions, sorry.”

*We'll clarify our position in an upcoming blog post.*",537,144,d what is openai i dont know anymore,0.0
[P] Dataset: 60k+ labeled Polandball characters,"I scraped all comics (as per 2 months ago) on /r/polandball, segmented them, and semi-manually labeled them based on their flags (generally representative of country/region) for an upcoming paper.

The result is over 60,000 images of Polandball characters (countryballs) that can be used for various computer vision and machine learning tasks. I intend to expand this dataset in the future to include any characters which are missing (mainly non-ball characters such as Israel, Kazakhstan, or Singapore).

Link to the dataset: https://www.kaggle.com/zimonitrome/polandball-characters",539,38,p dataset k labeled polandball characters,0.0
[P] I trained an AI model on 120M+ songs from iTunes,"Hey ML Reddit!

I just shipped a project I’ve been working on called Maroofy: [https://maroofy.com](https://maroofy.com/)

You can search for any song, and it’ll use the ***song’s audio*** to find other ***similar-sounding*** music.

**Demo:** [https://twitter.com/subby\_tech/status/1621293770779287554](https://twitter.com/subby_tech/status/1621293770779287554)

**How does it work?**

I’ve indexed \~120M+ songs from the iTunes catalog with a custom AI audio model that I built for understanding music.

My model analyzes raw music audio as input and produces embedding vectors as output.

I then store the embedding vectors for all songs into a vector database, and use semantic search to find similar music!

**Here are some examples you can try:**

Fetish (Selena Gomez feat. Gucci Mane) — [https://maroofy.com/songs/1563859943](https://maroofy.com/songs/1563859943)  The Medallion Calls (Pirates of the Caribbean) — [https://maroofy.com/songs/1440649752](https://maroofy.com/songs/1440649752)

Hope you like it!

This is an early work in progress, so would love to hear any questions/feedback/comments! :D",540,121,p i trained an ai model on m songs from itunes,0.0
"[D] Israeli MIT Professor Regina Barzilay Wins $1M Prize For AI Work In Cancer Diagnostics, Drug Development","and this is the [link](https://nocamels.com/2020/09/israeli-mit-professor-barzilay-1m-prize-ai/)

>An Israeli scientist and professor at the Massachusetts Institute of Technology (MIT) will be awarded a $1 million prize for her work using Machine Learning algorithm models to develop [antibiotics](https://news.mit.edu/2020/artificial-intelligence-identifies-new-antibiotic-0220) and other pharmaceuticals and [to detect and diagnose breast cancer earlier than existing clinical approaches.](https://news.mit.edu/2019/using-ai-predict-breast-cancer-and-personalize-care-0507)  
>  
>Professor Regina Barzilay of MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) was named this year’s recipient of an inaugural AI award by the world’s largest AI society, the Palto Alto-based Association for the Advancement of Artificial Intelligence (AAAI). The organization promotes awareness and research in AI, and honors individuals whose work in the field has a transformative impact on society.  
>  
>She’s the [recipient of the 2017 MacArthur Fellowship](https://news.mit.edu/2017/mit-computer-scientist-regina-barzilay-wins-macarthur-genius-grant-1011), often referred to as a “genius grant,” the National Science Foundation Career Award [in 2015](https://www.nsf.gov/awardsearch/showAward?AWD_ID=0448168), a Microsoft Faculty Fellowship, multiple “best paper” awards in her field, and MIT’s [Jamieson Award](https://www.eecs.mit.edu/news-events/announcements/student-faculty-and-staff-award-winners-honored-eecs-celebrates) for excellence in teaching.  
>  
>Her latest award, the Squirrel AI Award for Artificial Intelligence to Benefit Humanity, comes with an associated prize of $1 million provided by the online education company [Squirrel AI](https://squirrelai.com/).",536,59,d israeli mit professor regina barzilay wins m prize for ai work in cancer diagnostics drug development,0.3818
What is the best way to learn about Reinforcement Learning?,"The best way to learn is with the online [Reinforcement Learning](https://www.ualberta.ca/admissions-programs/online-courses/reinforcement-learning/index.html) specialization from Coursera and the University of Alberta. The two instructors, Martha and Adam White, are good colleagues of mine and did an excellent job creating this series of short courses last year. Also working to these course's advantage is that they are based on the second edition of Andy Barto's and my textbook *Reinforcement Learning: An Introduction*. 

You can earn credit for the course or you can audit it for free (use the little audit link at the bottom of the Coursera form that invites you to ""Start free trial""). Try signing up directly with [coursera.org](https://coursera.org), then go here: [https://www.coursera.org/specializations/reinforcement-learning](https://www.coursera.org/specializations/reinforcement-learning)

The RL textbook is available for free at [http://www.incompleteideas.net/book/the-book.html](http://www.incompleteideas.net/book/the-book.html).

If you want to gain a deeper understanding of machine learning and its role in artificial intelligence, then a good grasp of the fundamentals of reinforcement learning is essential. The first course of the reinforcement learning specialization begins today, June 14, so it is a great day to start learning about reinforcement learning!",535,82,what is the best way to learn about reinforcement learning,0.6369
[D] LPT: Machine Learning University Midterms and Finals solutions are an amazing way to deepen your knowledge of basic Machine Learning Principles.,"Some of these professors write brilliant exam questions that really question your understanding of the fundamentals. I mean, wow, I had no idea how many blindspots I had when it came to stuff I had down. 

A lot of short answer/question so even if you have a spare 10 minutes it's enough to look at, then maybe think about when you do the dishes. 

A good source of these exams are Stanford

https://cs.stanford.edu/academics/courses

They seem pretty friendly about opening up their materials to society. 

Hinton's and Andrew NG's coursera courses are another good source. 

Unfortunately it seems most other universities don't put of their exam solutions. If you know any other great sources, please post em. 
",528,42,d lpt machine learning university midterms and finals solutions are an amazing way to deepen your knowledge of basic machine learning principles,0.6705
[P] I 3D-Printed some Eigenfaces!,Faces are derived from a cropped version of Labeled Faces in the Wild.,533,53,p i dprinted some eigenfaces,0.0
Regarding beginner's guides,"Hi all,


/r/machinelearning is growing rampantly, with over a thousand new subscribers *every day*. As our community grows, it is important to have fertile ground for newcomers to learn the ropes. Since there is already an active subreddit for aiding in the development of machine learning skills, we feel that this is the right time to demarcate the content between these two subs.


As a new rule, all beginner-level content should be posted to our sister sub, /r/learnmachinelearning.  This will free up “real estate” on our page for more in-depth, expert discussions and provide a more focused learning space for beginners.  That’s not to say that all tutorials are outright banned — in particular, explanations of recent or niche papers are still welcome.

We were all beginners once and newcomers to ML are bringing great things to this sub and the general community. Please do continue to engage with and learn from the community here. But we recommend /r/learnmachinelearning if you do want to start getting your hands dirty. 

We hope that this specialization will be beneficial to everyone in the long run.


Best regards, the moderator team",530,54,regarding beginners guides,0.0
[P] My co-founder and I quit our engineering jobs at AWS to build “Tensor Search”. Here is why.,"My co-founder and I,  a senior Amazon research scientist and AWS SDE respectively, launched Marqo a little over a week ago - a ""tensor search"" engine [https://github.com/marqo-ai/marqo](https://github.com/marqo-ai/marqo)

**Another project doing semantic search/dense retrieval. Why??**

Semantic search using vectors does an amazing job when we look at sentences, or short paragraphs. Vectors also do well as an implementation for image search. Unfortunately, vector representations for video, long documents and other more complex data types perform poorly.

The reason isn't really to do with embeddings themselves not being good enough. If you asked a human to find the most relevant document to some search query given a list of long documents, an important question comes to mind - do we want the document that on average is most relevant to your query or the document that has a specific sentence that is very relevant to your search query?

Furthermore, what if the document has multiple components to it? Should we match based on the title of the document? Is that important? Or is the content more important?

These questions arn't things that we can expect an AI algorithm to solve for us, they need to be encoded into each specific search experience and use case.

**Introducing Tensor Search**

We believe that it is possible to tackle this problem by changing the way we think about semantic search - specifically, through *tensor search*.

By deconstructing documents and other data types into configurable chunks which are then vectorised we give users control over the way their documents are searched and represented. We can have any combination the user desires - should we do an average? A maximum? Weight certain components of the document more or less? Do we want to be more specific and target a specific sentence or less specific and look at the whole document?

Further, explainability is vastly improved - we can return as a ""highlight"" the exact content that matched the search query. Therefore, the user can see exactly where the query matched, even if they are dealing with long and complex data types like videos or long documents.

We dig in a bit more into the ML specifics next.

**The trouble with BERT on long documents - quadratic attention**

When we come to text, the vast majority of semantic search applications are using attention based algos like SBERT. Attention tapers off quadratically with sequence length, so subdividing sequences into multiple vectors means that we can significantly improve relevance.

**The disk space, relevance tradeoff**

Tensors allow you to trade disk space for search accuracy. You could retrain an SBERT model and increase the number of values in the embeddings and hence make the embeddings more descriptive, but this is quite costly (particularly if you want to leverage existing ML models). A better solution is instead to chunk the document into smaller components and vectorise those, increasing accuracy at the cost of disk space (which is relatively cheap).

**Tensor search for the general case**

We wanted to build a search engine for semantic search similar to something like Solr or Elasticsearch, where no matter what you throw at it, it can process it and make it searchable. With Marqo, it will use vectors were it can or expand to tensors where necessary - it also allows you the flexibility to specify specific chunking strategies to build out the tensors. Finally, Marqo is still a work in progress, but is at least something of an end-to-end solution - it has a number of features such as:

\- a query DSL language for pre-filtering results (includes efficient keyword, range and boolean queries)  
\- efficient approximate knn search powered by HNSW  
\- onnx support, multi-gpu support  
\- support for reranking

I love to hear feedback from the community! Don't hesitate to reach out on our slack channel (there is a link within the Marqo repo), or directly via linkedin: [https://www.linkedin.com/in/tom-hamer-%F0%9F%A6%9B-04a6369b/](https://www.linkedin.com/in/tom-hamer-%F0%9F%A6%9B-04a6369b/)",531,63,p my cofounder and i quit our engineering jobs at aws to build tensor search here is why,0.0
